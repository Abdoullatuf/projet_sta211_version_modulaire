





## DÃ©finition des mÃ©tadonnÃ©es du projet

from datetime import datetime
from pathlib import Path

# MÃ©tadonnÃ©es du projet
PROJECT_NAME = "Projet STA 211: Internet Advertisements Classification"
DATASET_NAME = "Internet Advertisements Dataset"
AUTHOR = "Abdoullatuf"
DATE = datetime.now().strftime("%Y-%m-%d")  # Date dynamique
VERSION = "1.0"

# VÃ©rification des mÃ©tadonnÃ©es
metadata = {
    "Projet": PROJECT_NAME,
    "Dataset": DATASET_NAME,
    "Auteur": AUTHOR,
    "Date": DATE,
    "Version": VERSION
}

# Affichage des informations
print("ğŸ“‹ MÃ©tadonnÃ©es du projet")
print("="*60)
for key, value in metadata.items():
    if not isinstance(value, str):
        raise TypeError(f"La mÃ©tadonnÃ©e '{key}' doit Ãªtre une chaÃ®ne de caractÃ¨res, reÃ§u {type(value)}")
    print(f"{key}: {value}")

# Sauvegarde des mÃ©tadonnÃ©es dans un fichier (optionnel)
METADATA_DIR = Path("metadata")
METADATA_DIR.mkdir(exist_ok=True)
metadata_file = METADATA_DIR / f"metadata_v{VERSION}.txt"
with open(metadata_file, "w", encoding="utf-8") as f:
    for key, value in metadata.items():
        f.write(f"{key}: {value}\n")
print(f"âœ… MÃ©tadonnÃ©es sauvegardÃ©es dans : {metadata_file}")








## 2.1 Configuration de l'environnement <a id="configuration-environnement"></a>

# Installation des packages (dÃ©commenter si nÃ©cessaire)
# !pip install -q scikit-learn xgboost lightgbm imbalanced-learn umap-learn prince
!pip install -q scikit-learn imbalanced-learn umap-learn prince

import sys
import os
from pathlib import Path
from IPython.display import Markdown, display
import warnings

# Configuration des warnings et de pandas
warnings.filterwarnings('ignore')

import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', '{:.4f}'.format)

# ğŸ” DÃ©tection de l'environnement
def detect_environment():
    try:
        import google.colab
        return "colab"
    except ImportError:
        return "local"

ENV = detect_environment()
print(f"ğŸ”§ Environnement dÃ©tectÃ© : {ENV}")

# ğŸš— Montage de Google Drive si nÃ©cessaire
if ENV == "colab":
    from google.colab import drive
    drive.mount('/content/drive', force_remount=True)
    print("âœ… Google Drive montÃ© avec succÃ¨s")

# ğŸ“ DÃ©finir les chemins selon lâ€™environnement
if ENV == "colab":
    module_path = Path("/content/drive/MyDrive/projet_sta211/modules")
else:
    module_path = Path("G:/Mon Drive/projet_sta211/modules")

if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))
    print(f"âœ… Module path ajoutÃ© : {module_path}")

# ğŸ“¦ Chargement des chemins
try:
    if ENV == "colab":
        from config.paths_config import setup_project_paths
    else:
        from modules.config.paths_config import setup_project_paths

    paths = setup_project_paths()
    print("âœ… Configuration des chemins rÃ©ussie")
except ImportError as e:
    print(f"âŒ Erreur : impossible d'importer setup_project_paths")
    raise

# ğŸ”§ Ajout manuel OUTPUTS_DIR si absent
if "OUTPUTS_DIR" not in paths:
    paths["OUTPUTS_DIR"] = paths["ROOT_DIR"] / "outputs"

# ğŸ“‹ Affichage Markdown des chemins
def display_paths():
    status_icons = {
        key: "âœ…" if Path(path).exists() else "âŒ"
        for key, path in paths.items()
    }
    paths_str = {k: str(v) for k, v in paths.items()}

    md = f"""
### ğŸ“‚ **Chemins configurÃ©s pour le projet**

| Status | Nom du dossier      | Chemin                                        |
|--------|---------------------|-----------------------------------------------|
| {status_icons.get("ROOT_DIR", "?")} | `ROOT_DIR`          | `{paths_str.get("ROOT_DIR", "")}`             |
| {status_icons.get("MODULE_DIR", "?")} | `MODULE_DIR`        | `{paths_str.get("MODULE_DIR", "")}`           |
| {status_icons.get("RAW_DATA_DIR", "?")} | `RAW_DATA_DIR`      | `{paths_str.get("RAW_DATA_DIR", "")}`         |
| {status_icons.get("DATA_PROCESSED", "?")} | `DATA_PROCESSED`    | `{paths_str.get("DATA_PROCESSED", "")}`       |
| {status_icons.get("MODELS_DIR", "?")} | `MODELS_DIR`        | `{paths_str.get("MODELS_DIR", "")}`           |
| {status_icons.get("FIGURES_DIR", "?")} | `FIGURES_DIR`       | `{paths_str.get("FIGURES_DIR", "")}`          |
| {status_icons.get("OUTPUTS_DIR", "?")} | `OUTPUTS_DIR`       | `{paths_str.get("OUTPUTS_DIR", "")}`          |

**LÃ©gende :** âœ… = Existe | âŒ = N'existe pas
"""
    display(Markdown(md))

display_paths()

# ğŸ”§ Infos systÃ¨me
print(f"\nğŸ Python version : {sys.version.split()[0]}")
print(f"ğŸ“ Working directory : {os.getcwd()}")

# ğŸ“Œ Variables globales
ROOT_DIR = paths["ROOT_DIR"]
MODULE_DIR = paths["MODULE_DIR"]
RAW_DATA_DIR = paths["RAW_DATA_DIR"]
DATA_PROCESSED = paths["DATA_PROCESSED"]
MODELS_DIR = paths["MODELS_DIR"]
FIGURES_DIR = paths["FIGURES_DIR"]
OUTPUTS_DIR = paths["OUTPUTS_DIR"]







## 2.2 Import des bibliothÃ¨ques <a id="import-des-bibliotheques"></a>

# ğŸ“¦ Modules standards
import sys
import os
import warnings
from pathlib import Path

# ğŸ§® Manipulation des donnÃ©es
import numpy as np
import pandas as pd

# ğŸ“Š Visualisation
import matplotlib.pyplot as plt
import seaborn as sns

# âš™ï¸ PrÃ©traitement & ModÃ¨les
from sklearn.experimental import enable_iterative_imputer  # â¬…ï¸ NÃ©cessaire pour IterativeImputer
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, calinski_harabasz_score, mean_squared_error
from sklearn.model_selection import KFold

from typing import List, Dict, Optional



# ğŸ“ˆ Statistiques
from scipy.stats import pointbiserialr

# ğŸ” Optionnel : UMAP
try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("âš ï¸ UMAP non disponible. Certaines visualisations seront dÃ©sactivÃ©es.")

# ğŸ² Configuration globale
warnings.filterwarnings("ignore")
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 100)
pd.set_option("display.float_format", '{:.4f}'.format)
sns.set_style("whitegrid")
sns.set_palette("husl")

# DÃ©finition de RANDOM_STATE
if "RANDOM_STATE" not in globals():
    RANDOM_STATE = 42
    np.random.seed(RANDOM_STATE)

# ğŸ” VÃ©rification des modules critiques
print("\nğŸ” VÃ©rification des modules critiques :")
import sklearn  # Ajout explicite

required_modules = {
    "pandas": pd,
    "numpy": np,
    "scikit-learn": sklearn,
    "matplotlib": plt,
    "seaborn": sns
}

for name, alias in required_modules.items():
    try:
        _ = eval(alias) if isinstance(alias, str) else alias
        print(f"  âœ… {name}")
    except Exception:
        print(f"  âŒ {name} - problÃ¨me lors de l'import")

# âš™ï¸ RÃ©sumÃ© de configuration
print(f"\nâš™ï¸ Configuration :")
print(f"  - Random State : {RANDOM_STATE}")
print(f"  - Style matplotlib : seaborn-whitegrid")
print(f"  - Palette seaborn : husl")
print(f"  - UMAP disponible : {UMAP_AVAILABLE}")
print(f"  - Python : {sys.version.split()[0]}")






## 2.3 Configuration des paramÃ¨tres du projet <a id="configuration-parametres-projet"></a>

# ğŸ“¦ Import de la classe ProjectConfig et de la fonction de crÃ©ation
try:
    from config.project_config import ProjectConfig, create_config
except ImportError as e:
    print("âŒ Erreur : impossible d'importer depuis 'config.project_config'")
    raise

# ğŸ› ï¸ CrÃ©ation de la configuration avec les mÃ©tadonnÃ©es + chemins
config = create_config(
    project_name=PROJECT_NAME,
    version=VERSION,
    author=AUTHOR,
    paths=paths
)

# ğŸ¯ Ciblage de la mÃ©trique F1 pour le challenge
config.update("PROJECT_CONFIG.SCORING", "f1")
config.update("PROJECT_CONFIG.SCORING_METRICS", ["f1", "roc_auc", "precision", "recall"])
config.update("PROJECT_CONFIG.PRIMARY_METRIC", "f1")

# ğŸ‘ï¸ Affichage de la configuration
config.display_config()

# ğŸ“Œ Exemples d'accÃ¨s Ã  des valeurs clÃ©s
print("\nğŸ“Œ Exemples d'accÃ¨s Ã  la configuration :")
print(f"  - Test size : {config.get('PROJECT_CONFIG.TEST_SIZE')}")
print(f"  - MÃ©thode d'imputation X4 : {config.get('PROJECT_CONFIG.IMPUTATION_METHODS.X4')}")
print(f"  - Taille des figures : {config.get('VIZ_CONFIG.figure_size')}")

# ğŸ’¾ Sauvegarde de la configuration dans le dossier 'config'
config_file = Path(paths["ROOT_DIR"]) / "config" / f"project_config_v{VERSION}.json"
config.save_config(config_file)

# ğŸŒ Rendre la configuration disponible globalement
PROJECT_CONFIG = config.PROJECT_CONFIG
COLUMN_CONFIG = config.COLUMN_CONFIG
VIZ_CONFIG = config.VIZ_CONFIG
MODEL_CONFIG = config.MODEL_CONFIG
PIPELINE_CONFIG = config.PIPELINE_CONFIG
SAVE_PATHS = config.SAVE_PATHS
F1_OPTIMIZATION = config.F1_OPTIMIZATION

print("\nâœ… Configuration chargÃ©e et disponible globalement")






## 3.1 Chargement des jeux de donnÃ©es bruts <a id="chargement-des-jeux-de-donnees-bruts"></a>

from preprocessing.data_loader import load_data
#from modules.preprocessing.data_loader import load_data  # en local si jamais ne fonctionne pas, dÃ©coche ce si.


from pathlib import Path

# ğŸ“ VÃ©rification de lâ€™existence du dossier RAW_DATA_DIR
if 'RAW_DATA_DIR' not in globals():
    raise NameError("âŒ RAW_DATA_DIR nâ€™est pas dÃ©fini. VÃ©rifiez la configuration dans la section 2.1.")

raw_data_dir = Path(RAW_DATA_DIR)
if not raw_data_dir.exists():
    raise FileNotFoundError(f"âŒ Dossier RAW_DATA_DIR introuvable : {raw_data_dir}")

# ğŸ“‚ Chargement des fichiers CSV
print("ğŸ“‚ Chargement des jeux de donnÃ©es...")

df_study = load_data(
    file_path="data_train.csv",
    require_outcome=True,
    display_info=True,
    raw_data_dir=RAW_DATA_DIR,
    encode_target=True
)

df_eval = load_data(
    file_path="data_test.csv",
    require_outcome=False,
    display_info=True,
    raw_data_dir=RAW_DATA_DIR,
    encode_target=False
)

# ğŸ·ï¸ Renommer 'outcome' en 'y' si nÃ©cessaire
if 'outcome' in df_study.columns:
    df_study = df_study.rename(columns={'outcome': 'y'})
    print("âœ… Colonne 'outcome' renommÃ©e en 'y'")
elif 'y' not in df_study.columns:
    raise ValueError("âŒ Colonne 'y' ou 'outcome' manquante dans df_study")

# ğŸ”¢ VÃ©rification des dimensions attendues
expected_train_shape = (2459, 1559)
expected_test_shape = (820, 1558)

if df_study.shape != expected_train_shape:
    print(f"âš ï¸ Dimensions inattendues pour df_study : {df_study.shape} (attendu : {expected_train_shape})")
if df_eval.shape != expected_test_shape:
    print(f"âš ï¸ Dimensions inattendues pour df_eval : {df_eval.shape} (attendu : {expected_test_shape})")

# ğŸ” VÃ©rification de la variable cible
print("\nğŸ” Valeurs uniques de y :", df_study['y'].unique())
print("ğŸ” Type de y :", df_study['y'].dtype)

# ğŸ“Š RÃ©sumÃ©
print(f"\nğŸ“Š RÃ©sumÃ© :")
print(f"  - Fichier dâ€™Ã©tude     : {df_study.shape}")
print(f"  - Fichier dâ€™Ã©valuation : {df_eval.shape}")
print("\nâœ… Chargement terminÃ© avec succÃ¨s !")






## 3.2 Inspection des colonnes et types <a id="inspection-des-colonnes-et-types"></a>

print("ğŸ” Inspection des types de donnÃ©es")
print("=" * 60)

# ğŸ” RÃ©sumÃ© des types dans df_study
print("\nğŸ“Š Types de donnÃ©es dans df_study :")
type_counts = df_study.dtypes.value_counts()
for dtype, count in type_counts.items():
    print(f"  - {dtype}: {count} colonnes")

# ğŸ“Œ Identification des colonnes par type
continuous_cols = df_study.select_dtypes(include=['float64']).columns.tolist()
int_cols = df_study.select_dtypes(include=['int64']).columns.tolist()
categorical_cols = df_study.select_dtypes(include=['object']).columns.tolist()

# ğŸ” VÃ©rification binaire rÃ©elle parmi les colonnes int
binary_cols = []
non_binary_cols = []

for col in int_cols:
    unique_vals = df_study[col].dropna().unique()
    if len(unique_vals) == 2 and set(unique_vals).issubset({0, 1}):
        binary_cols.append(col)
    else:
        non_binary_cols.append(col)

print(f"\nğŸ“ˆ Colonnes continues : {len(continuous_cols)}")
print(f"ğŸ”¢ Colonnes binaires : {len(binary_cols)}")
print(f"ğŸ“¦ Colonnes catÃ©gorielles : {len(categorical_cols)}")

if non_binary_cols:
    print(f"âš ï¸ Colonnes int64 non binaires dÃ©tectÃ©es (extrait) : {non_binary_cols[:5]}")

# ğŸ¯ Variable cible 'y'
if 'y' in df_study.columns:
    print("\nğŸ¯ Variable cible 'y' :")
    print(f"  - Type : {df_study['y'].dtype}")
    print(f"  - Valeurs uniques : {sorted(df_study['y'].dropna().unique())}")
    print(f"  - Distribution :\n{df_study['y'].value_counts().sort_index()}")
else:
    print("âŒ La colonne cible 'y' est manquante")

# ğŸ”„ Comparaison avec df_eval
print("\nğŸ”„ Comparaison avec df_eval :")
eval_type_counts = df_eval.dtypes.value_counts()
for dtype, count in eval_type_counts.items():
    print(f"  - {dtype}: {count} colonnes")

# âš–ï¸ VÃ©rification des types entre df_study et df_eval
print("\nğŸ” VÃ©rification de la cohÃ©rence des types entre fichiers :")
type_mismatches = [
    (col, df_study[col].dtype, df_eval[col].dtype)
    for col in df_eval.columns if col in df_study.columns and df_study[col].dtype != df_eval[col].dtype
]

if type_mismatches:
    print("âš ï¸ IncohÃ©rences de type dÃ©tectÃ©es :")
    for col, t1, t2 in type_mismatches:
        print(f"  - {col}: {t1} (Ã©tude) vs {t2} (Ã©val)")
else:
    print("âœ… Types cohÃ©rents entre df_study et df_eval")

# ğŸ’¾ Mise Ã  jour de la configuration
print("\nğŸ’¾ Mise Ã  jour de la configuration...")
config.update("COLUMN_CONFIG.CONTINUOUS_COLS", continuous_cols)
config.update("COLUMN_CONFIG.BINARY_COLS", binary_cols)
config.update("COLUMN_CONFIG.CATEGORICAL_COLS", categorical_cols)

# ğŸ“‹ RÃ©sumÃ© final
print("\nğŸ“Š RÃ©sumÃ© des colonnes (hors 'y') :")
print(f"  - Colonnes continues   : {len(continuous_cols)}")
print(f"  - Colonnes binaires    : {len(binary_cols)}")
print(f"  - Colonnes catÃ©gorielles : {len(categorical_cols)}")
print(f"  - Total features (hors cible) : {df_study.shape[1] - 1}")






## 3.3 Distribution de la variable cible <a id="distribution-variable-cible"></a>

print("\nğŸ¯ Analyse de la distribution de la variable cible")
print("=" * 60)

# VÃ©rification de la prÃ©sence de 'y'
if 'y' not in df_study.columns:
    raise ValueError("âŒ Colonne cible 'y' introuvable dans df_study")

# Distribution brute et en %
target_counts = df_study['y'].value_counts().sort_index()
target_pct = df_study['y'].value_counts(normalize=True).sort_index() * 100

# Affichage des proportions
print("\nğŸ“Š Distribution de la variable cible (y) :")
for label in target_counts.index:
    label_str = "Classe 1 (ad.)" if label == 1 else "Classe 0 (noad.)"
    print(f"  - {label_str:<20}: {target_counts[label]:,} ({target_pct[label]:.1f}%)")

# Ratio de dÃ©sÃ©quilibre
imbalance_ratio = target_counts[0] / target_counts[1]
print(f"\nğŸ“ˆ Ratio de dÃ©sÃ©quilibre : {imbalance_ratio:.2f}:1")
print(f"   â†’ Pour chaque publicitÃ©, il y a {imbalance_ratio:.1f} non-publicitÃ©s")

# Visualisation
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# Bar plot
target_counts.plot(kind='bar', ax=ax1, color=['#3498db', '#e74c3c'])
ax1.set_title('Distribution des classes', fontsize=14)
ax1.set_xlabel('Classe')
ax1.set_ylabel('Nombre d\'Ã©chantillons')
ax1.set_xticklabels(['Non-publicitÃ© (0)', 'PublicitÃ© (1)'], rotation=0)

# Pie chart
target_pct.plot(kind='pie', ax=ax2, colors=['#3498db', '#e74c3c'],
                autopct='%1.1f%%', startangle=90)
ax2.set_title('Proportion des classes', fontsize=14)
ax2.set_ylabel('')

plt.tight_layout()

# Sauvegarde sÃ©curisÃ©e dans un sous-dossier
eda_dir = FIGURES_DIR / 'eda'
eda_dir.mkdir(parents=True, exist_ok=True)
plt.savefig(eda_dir / 'target_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# Impact pour la modÃ©lisation
print("\nğŸ’¡ Implications pour la modÃ©lisation :")
print(f"  - Dataset fortement dÃ©sÃ©quilibrÃ© ({imbalance_ratio:.1f}:1)")
print("  - StratÃ©gies recommandÃ©es :")
print("    â€¢ Utiliser stratify=True lors du train/test split")
print("    â€¢ Appliquer SMOTE ou class_weight='balanced'")
print("    â€¢ Optimiser pour F1-score (mÃ©trique du challenge)")
print("    â€¢ Envisager un stacking ou un modÃ¨le robuste aux dÃ©sÃ©quilibres")

# Calcul du F1-score baseline
p = target_pct[1] / 100  # PrÃ©cision et recall identiques si on prÃ©dit toujours 1
baseline_f1 = 2 * p / (1 + p)
print(f"\nğŸ“Š F1-score baseline (prÃ©dire toujours 'ad.') : {baseline_f1:.3f}")
print("   â†’ Les modÃ¨les devront dÃ©passer ce seuil pour Ãªtre utiles")






# 4. Analyse exploratoire <a id="analyse-exploratoire"></a>
## 4.1 Analyse des valeurs manquantes <a id="analyse-des-valeurs-manquantes"></a>

print("ğŸ” Analyse des valeurs manquantes")
print("="*60)

# Utilisation de la fonction du module
from preprocessing.missing_values import (
    analyze_missing_values,
    handle_missing_values,
    find_optimal_k
)



print("\nğŸ“Š Analyse globale des valeurs manquantes :")
missing_stats = analyze_missing_values(df_study)

# Analyse dÃ©taillÃ©e pour les colonnes continues
print("\nğŸ“ˆ DÃ©tail des valeurs manquantes pour les variables continues :")
for col in continuous_cols:
    missing_count = df_study[col].isnull().sum()
    missing_pct = (missing_count / len(df_study)) * 100
    print(f"  - {col}: {missing_count} ({missing_pct:.2f}%)")

# Visualisation des patterns de valeurs manquantes
if missing_stats['total_missing'] > 0:
    # Heatmap des valeurs manquantes pour les colonnes avec des NaN
    cols_with_missing = [col for col in df_study.columns if df_study[col].isnull().sum() > 0]

    if len(cols_with_missing) > 0:
        plt.figure(figsize=(10, 5))

        # CrÃ©er une matrice binaire des valeurs manquantes
        missing_matrix = df_study[cols_with_missing].isnull().astype(int)

        # Heatmap
        sns.heatmap(missing_matrix.T, cmap='RdYlBu', cbar_kws={'label': 'Manquant (1) / PrÃ©sent (0)'})
        plt.title('Pattern des valeurs manquantes', fontsize=14)
        plt.xlabel('Ã‰chantillons')
        plt.ylabel('Variables')
        plt.tight_layout()
        plt.savefig(FIGURES_DIR / 'eda' / 'missing_values_pattern.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Analyse du pattern MAR vs MCAR
        print("\nğŸ” Analyse du type de valeurs manquantes (MAR vs MCAR) :")

        # CorrÃ©lation entre les valeurs manquantes et la cible
        for col in cols_with_missing:
            missing_indicator = df_study[col].isnull().astype(int)
            correlation_with_target = missing_indicator.corr(df_study['y'])
            print(f"  - {col}: corrÃ©lation avec y = {correlation_with_target:.3f}")

            if abs(correlation_with_target) > 0.1:
                print(f"    â†’ Potentiellement MAR (Missing At Random)")
            else:
                print(f"    â†’ Potentiellement MCAR (Missing Completely At Random)")
else:
    print("\nâœ… Aucune valeur manquante dÃ©tectÃ©e dans le dataset !")

# Analyse pour le fichier d'Ã©valuation aussi
print("\nğŸ“Š Analyse des valeurs manquantes dans le fichier d'Ã©valuation :")
missing_stats_eval = analyze_missing_values(df_eval)

# Comparaison des patterns
if missing_stats['total_missing'] > 0 or missing_stats_eval['total_missing'] > 0:
    print("\nğŸ”„ Comparaison des patterns de valeurs manquantes :")
    print(f"  - Fichier d'Ã©tude : {missing_stats['percent_missing']:.2f}% manquant")
    print(f"  - Fichier d'Ã©valuation : {missing_stats_eval['percent_missing']:.2f}% manquant")

    # StratÃ©gie d'imputation recommandÃ©e
    print("\nğŸ’¡ StratÃ©gie d'imputation recommandÃ©e :")
    if 'X4' in missing_stats['cols_missing']:
        x4_missing_pct = missing_stats['percent_per_col'].get('X4', 0)
        if x4_missing_pct < 5:
            print(f"  - X4 ({x4_missing_pct:.1f}% manquant) : Imputation par la mÃ©diane")

    mar_cols = ['X1', 'X2', 'X3']
    mar_missing = any(col in missing_stats['cols_missing'] for col in mar_cols)
    if mar_missing:
        print(f"  - X1, X2, X3 (variables continues) : KNN ou MICE (imputation multivariÃ©e)")


# Correction du type et imputation de X4
print("\nğŸ”§ Correction du type de X4...")
print(f"Valeurs uniques de X4 (avant correction) : {sorted(df_study['X4'].dropna().unique())}")
print(f"Type actuel : {df_study['X4'].dtype}")

# VÃ©rifier que X4 ne contient que 0 et 1
unique_values = df_study['X4'].dropna().unique()
if set(unique_values).issubset({0.0, 1.0}):
    # Imputer d'abord les valeurs manquantes par la mÃ©diane
    X4_median = df_study['X4'].median()
    df_study['X4'] = df_study['X4'].fillna(X4_median)
    df_eval['X4'] = df_eval['X4'].fillna(X4_median)

    # Convertir en int
    df_study['X4'] = df_study['X4'].astype(int)
    df_eval['X4'] = df_eval['X4'].astype(int)

    print(f"âœ… X4 converti en int64 aprÃ¨s imputation par la mÃ©diane ({X4_median})")
    print(f"Nouveau type : {df_study['X4'].dtype}")

    # Mettre Ã  jour la configuration
    config.update("COLUMN_CONFIG.CONTINUOUS_COLS", ['X1', 'X2', 'X3'])
    config.update("COLUMN_CONFIG.BINARY_COLS", ['X4'] + binary_cols)
    continuous_cols = ['X1', 'X2', 'X3']  # Mise Ã  jour locale
else:
    print("âš ï¸ X4 contient des valeurs autres que 0 et 1, conservation en float64")

# RÃ©sumÃ© final
print("\nğŸ“Š RÃ©sumÃ© des valeurs manquantes aprÃ¨s traitement de X4 :")
print(f"  - X1, X2, X3 : ~27% manquant â†’ Ã€ traiter avec KNN/MICE")
print(f"  - X4 : ImputÃ© et converti en binaire")
print(f"  - Pattern MAR dÃ©tectÃ© pour X1, X2, X3 (corrÃ©lation avec y â‰ˆ -0.10)")
print(f"  - Les patterns sont cohÃ©rents entre fichiers d'Ã©tude et d'Ã©valuation")






## 4.2 Analyse statistique des variables quantitatives <a id="analyse-statistique-des-variables-quantitatives"></a>

print("ğŸ“Š Analyse statistique des variables quantitatives")
print("="*60)

from exploration.statistics import analyze_continuous_variables
#from modules.exploration.statistics import analyze_continuous_variables


# Lancement de lâ€™analyse complÃ¨te
results_stats = analyze_continuous_variables(
    df=df_study,
    continuous_cols=continuous_cols,
    target_col='y',
    save_figures_path=str(FIGURES_DIR / "eda")  # Assure-toi que ce dossier existe
)







## 4.2.1 Visualisation des distributions et des boxplots <a id="distributions-et-boxplots"></a>

print("ğŸ“Š Visualisation des distributions et des boxplots")
print("="*60)

from exploration.visualization import visualize_distributions_and_boxplots
# Appel de la fonction
visualize_distributions_and_boxplots(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / "eda"
)








## 4.3 Distribution des variables binaires <a id="distribution-des-variables-binaires"></a>

print("ğŸ”¢ Analyse de la distribution des variables binaires")
print("="*60)

from exploration.visualization import save_fig

# Variables binaires (exclut les variables continues)
binary_cols = [col for col in df_study.columns if col.startswith('X') and col not in continuous_cols]
print(f"\nğŸ“Š Nombre total de variables binaires : {len(binary_cols)}")

# Taux de prÃ©sence (valeurs Ã  1)
presence_rates = {
    col: (df_study[col] == 1).sum() / len(df_study) * 100 for col in binary_cols
}
presence_series = pd.Series(presence_rates)

# Statistiques globales
print(f"\nğŸ“Š Statistiques des taux de prÃ©sence :")
print(f"  - Moyenne : {presence_series.mean():.2f}%")
print(f"  - MÃ©diane : {presence_series.median():.2f}%")
print(f"  - Min : {presence_series.min():.2f}%")
print(f"  - Max : {presence_series.max():.2f}%")

# SparsitÃ© globale
total_values = len(df_study) * len(binary_cols)
total_ones = df_study[binary_cols].sum().sum()
sparsity = (1 - total_ones / total_values) * 100
print(f"\nğŸ“Š SparsitÃ© globale : {sparsity:.2f}% de zÃ©ros")

# Visualisation
plt.figure(figsize=(8, 4))
presence_series.hist(bins=50, color='skyblue', edgecolor='black')
plt.axvline(presence_series.mean(), color='red', linestyle='--', label=f'Moyenne: {presence_series.mean():.1f}%')
plt.xlabel('Taux de prÃ©sence (%)')
plt.ylabel('Nombre de variables')
plt.title('Distribution des taux de prÃ©sence des variables binaires')
plt.legend()
plt.tight_layout()

save_fig("binary_presence_distribution.png", directory=FIGURES_DIR / "eda", dpi=300, show=True)

print("\nâœ… Analyse des variables binaires terminÃ©e")
print("   â†’ Dataset trÃ¨s sparse, adaptÃ© pour des mÃ©thodes de sÃ©lection de features")






## 4.4 Analyse des corrÃ©lations combinÃ©es <a id="analyse-correlations-combinees"></a>

print("ğŸ”— Lancement de l'analyse combinÃ©e des corrÃ©lations (features â†” cible, features â†” features)...")
print("=" * 80)

from exploration.eda_analysis import full_correlation_analysis

# Appel avec paramÃ¨tres personnalisÃ©s
full_correlation_analysis(
    df_study=df_study,
    continuous_cols=continuous_cols,
    presence_rates=presence_rates,
    FIGURES_DIR=FIGURES_DIR,
    ROOT_DIR=ROOT_DIR,
    figsize_corr_matrix=(7, 5),
    figsize_binary=(8, 4)
)








## 4.6 Visualisations globales de l'EDA <a id="visualisation-globale"></a>

print("ğŸ“Š Visualisations exploratoires")
print("=" * 60)

# Imports des fonctions refactorisÃ©es
from exploration.visualization import (
    compare_visualization_methods,
    plot_continuous_by_class,
    plot_binary_sparsity,
    plot_continuous_target_corr,
    plot_eda_summary,
    save_fig
)
from exploration.statistics import optimized_feature_importance  # âœ… Nouvelle fonction importÃ©e

# 1. Distribution des variables continues par classe
print("\nğŸ“ˆ Distribution des variables continues par classe...")
plot_continuous_by_class(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / 'eda'
)

# 2. Visualisation de la sparsitÃ©
print("\nğŸ“‰ Visualisation de la sparsitÃ© des donnÃ©es binaires...")
plot_binary_sparsity(
    df=df_study,
    binary_cols=binary_cols,
    output_dir=FIGURES_DIR / 'eda'
)




# 3. CorrÃ©lations des variables continues avec la cible
print("\nğŸ”— CorrÃ©lations des variables continues avec la cible...")
plot_continuous_target_corr(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / 'eda'
)

# 4. RÃ©duction de dimension avec UMAP / t-SNE / PCA
print("\nğŸ“Š Visualisation multidimensionnelle (PCA / t-SNE / UMAP)...")

df_study_viz = df_study.copy()
df_study_viz['outcome'] = df_study_viz['y'].map({0: 'noad.', 1: 'ad.'})  # âœ… temporaire

# ğŸ” Recalcul des corrÃ©lations si besoin
target_corr = df_study[continuous_cols + ['y']].corr()['y'].drop('y')

important_features = continuous_cols + list(target_corr.abs().nlargest(30).index)
df_sample = df_study_viz[important_features + ['outcome']].dropna()





# 5. Importance des variables
print("\nğŸŒ² Analyse de lâ€™importance des features...")
try:
    df_importance = df_sample.copy()  # contient outcome dÃ©jÃ  transformÃ©e
    importance_results = optimized_feature_importance(
        df=df_importance,
        target_col='outcome',
        method='all',
        top_n=10,
        figsize=(8, 4),
        save_path=FIGURES_DIR / 'eda' / 'feature_importance.png',
        show=True
    )
    if not importance_results.empty:
        print("\nTop 10 features les plus importantes :")
        print(importance_results[['feature', 'Combined_Score']].head(10))
except Exception as e:
    print(f"âš ï¸ Erreur lors de lâ€™analyse dâ€™importance des features : {e}")

# 6. RÃ©sumÃ© visuel global
print("\nğŸ“Š CrÃ©ation du rÃ©sumÃ© visuel de lâ€™EDA...")
plot_eda_summary(
    df=df_study,
    continuous_cols=continuous_cols,
    binary_cols=binary_cols,
    target_corr=target_corr,
    sparsity=sparsity,
    imbalance_ratio=imbalance_ratio,
    output_dir=FIGURES_DIR / 'eda',
    presence_series=presence_series
)

print("\nâœ… Visualisations exploratoires terminÃ©es avec succÃ¨s !")








from preprocessing.final_preprocessing import apply_yeojohnson

# Appliquer Yeo-Johnson aux variables continues
df_study = apply_yeojohnson(
    df=df_study,
    columns=["X1", "X2", "X3"],
    standardize=False,
    save_model=True,
    model_path=MODELS_DIR / "yeojohnson_transformer.pkl"
)

# VÃ©rification visuelle rapide des variables transformÃ©es
df_study[["X1_trans", "X2_trans", "X3_trans"]].describe()





import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# âœ… Liste des variables transformÃ©es
transformed_vars = ["X1_trans", "X2_trans", "X3_trans"]

# ğŸ“ Dossier de sauvegarde
output_dir = FIGURES_DIR / 'preprocessing'
output_dir.mkdir(parents=True, exist_ok=True)

# ğŸ” GÃ©nÃ©ration et sauvegarde des figures
for col in transformed_vars:
    fig, ax = plt.subplots(1, 2, figsize=(6, 2))

    # Histogramme + KDE
    sns.histplot(df_study[col], bins=30, kde=True, ax=ax[0], color="mediumseagreen")
    ax[0].set_title(f"{col} - Histogramme")
    ax[0].set_xlabel(col)

    # Boxplot
    sns.boxplot(x=df_study[col], ax=ax[1], color="salmon")
    ax[1].set_title(f"{col} - Boxplot")

    plt.tight_layout()

    # ğŸ’¾ Sauvegarde
    fig_path = output_dir / f"{col}_distribution_boxplot.png"
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"âœ… Figure sauvegardÃ©e : {fig_path}")









## 5.2 DÃ©tection et suppression des outliers <a id="detection-et-suppression-des-outliers"></a>

print("ğŸ” DÃ©tection et suppression des outliers (mÃ©thode IQR)")
print("=" * 60)

from preprocessing.outliers import detect_and_remove_outliers

# âœ… Variables Ã  traiter (transformÃ©es)
transformed_cols = ["X1_trans", "X2_trans", "X3_trans"]

# âœ… Sauvegarde de la version avant suppression
df_with_outliers = df_study.copy()

# âœ… Chemin de sauvegarde aprÃ¨s nettoyage
output_path = OUTPUTS_DIR / "data" / "df_after_outliers.csv"

# âœ… Suppression des outliers avec export CSV
df_study = detect_and_remove_outliers(
    df=df_study,
    columns=transformed_cols,
    method='iqr',
    iqr_multiplier=1.5,
    verbose=True,
    save_path=output_path
)

# âœ… AperÃ§u statistique post-nettoyage
print("\nğŸ“Š Statistiques descriptives aprÃ¨s suppression des outliers :")
display(df_study[transformed_cols].describe())







## ğŸ“Š Visualisation comparative avant/aprÃ¨s suppression des outliers

from exploration.visualization import plot_outlier_comparison

# âœ… Comparaison visuelle avant / aprÃ¨s (X1_trans, X2_trans, X3_trans)
plot_outlier_comparison(
    df_before=df_with_outliers,
    df_after=df_study,
    cols=transformed_cols,
    output_dir=FIGURES_DIR / "eda",
    show=True
)


















## 5.3.2 PrÃ©paration pour l'imputation multivariÃ©e <a id="preparation-imputation-multivariee"></a>

print("ğŸ”§ PrÃ©paration Ã  l'imputation multiple (KNN / MICE)")
print("=" * 60)

from preprocessing.missing_values import analyze_missing_values

cols_to_check = ["X1_trans", "X2_trans", "X3_trans"]

# ğŸ“ Analyse sur les donnÃ©es AVEC outliers
print("\nğŸ“Š Analyse (donnÃ©es avec outliers)")
analyze_missing_values(df=df_with_outliers, columns=cols_to_check, plot=True)

# âœ… Colonnes Ã  imputer (si moins de 30 % de valeurs manquantes)
cols_impute_with_outliers = [col for col in cols_to_check
                             if df_with_outliers[col].isna().mean() < 0.30]

print("\nğŸ“Œ Colonnes retenues (avec outliers) :")
print(cols_impute_with_outliers)


# ğŸ“ Analyse sur les donnÃ©es SANS outliers
print("\nğŸ“Š Analyse (donnÃ©es sans outliers)")
analyze_missing_values(df=df_study, columns=cols_to_check, plot=True)

cols_impute_no_outliers = [col for col in cols_to_check
                           if df_study[col].isna().mean() < 0.30]

print("\nğŸ“Œ Colonnes retenues (sans outliers) :")
print(cols_impute_no_outliers)





























