    "# 📊 STA211 - EDA & Prétraitement des Données"
    "Ce notebook réalise l’analyse exploratoire des données (EDA) et le prétraitement du dataset **Internet Advertisements** dans le cadre du module **STA211**. L’objectif est de prédire si une image est une publicité (`ad.`) ou non (`noad.`), en optimisant le **score F1** sur un jeu de test.\n",
    "## 🔍 Objectifs :\n",
    "- **Explorer les caractéristiques du dataset**\n",
    "  - Dimensions des données (2459 lignes d’entraînement, 820 lignes de test, 1558 variables explicatives).\n",
    "  - Types de variables : 3 quantitatives (géométrie des images) et 1555 binaires (mots-clés, URL, etc.).\n",
    "  - Distribution des classes : analyse du déséquilibre (13.99% `ad.` vs 86.01% `noad.`).\n",
    "- **Analyser la qualité des données**\n",
    "  - Détecter et traiter les valeurs aberrantes (outliers).\n",
    "  - Vérifier la cohérence des données (types, valeurs inattendues).\n",
    "  - Distributions univariées (histogrammes, box-plots, QQ-plots).\n",
    "  - Corrélations bivariées et multivariées (ACP, AFM, cartes de Kohonen).\n",
    "  - Identification des mots-clés les plus discriminants pour la classe `ad.`.\n",
    "- **Prétraiter les données pour la modélisation**\n",
    "  - Encodage de la variable cible (`ad.`/`noad.` → 1/0).\n",
    "  - Transformation des variables quantitatives (Yeo-Johnson, discrétisation).\n",
    "  - Gestion du déséquilibre via SMOTE ou pondération des classes.\n",
    "  - Séparation train/test stratifiée (80-20) pour validation.\n",
    "# Définition des métadonnées du projet\n",
    "**Objectif** : Définir les métadonnées du projet pour assurer la traçabilité et la reproductibilité du notebook. Ces informations identifient le projet, l’auteur, la version, et la période de réalisation.\n",
    "**Contexte** : Dans le cadre du challenge *Internet Advertisements*, ces métadonnées servent à documenter le travail réalisé pour la classification binaire (`ad.` vs `noad.`) et à faciliter l’évaluation pédagogique.\n",
    "**Méthodologie** : Les métadonnées sont stockées dans des variables globales et affichées pour confirmation. Une date dynamique est utilisée pour refléter le moment de l’exécution.\n",
    "**Prochaines étapes** : Configurer l’environnement et charger les données (section suivante)."
   "execution_count": null,
   "outputs": [],
    "## Définition des métadonnées du projet\n",
    "# Métadonnées du projet\n",
    "# Vérification des métadonnées\n",
    "print(\"📋 Métadonnées du projet\")\n",
    "        raise TypeError(f\"La métadonnée '{key}' doit être une chaîne de caractères, reçu {type(value)}\")\n",
    "# Sauvegarde des métadonnées dans un fichier (optionnel)\n",
    "print(f\"✅ Métadonnées sauvegardées dans : {metadata_file}\")"
    "# Table des matières\n",
    "    - 2.2 [Import des bibliothèques](#import-des-bibliotheques)\n",
    "    - 2.3 [Configuration des paramètres du projet](#configuration-parametres-projet)\n",
    "3. [Chargement et aperçu des données](#chargement-et-apercu-des-donnees)\n",
    "    - 3.1 [Chargement des jeux de données bruts](#chargement-des-jeux-de-donnees-bruts)\n",
    "    - 4.4 [Analyse des corrélations](#analyse-des-correlations)\n",
    "5. [Prétraitement avancé](#pretraitement-avance)\n",
    "    - 5.2 [Détection et suppression des outliers](#detection-et-suppression-des-outliers)\n",
    "        - 5.3.1 [Imputation de X4 par la médiane](#imputation-x4-mediane)\n",
    "        - 5.3.2 [Préparation pour l'imputation multivariée](#preparation-imputation-multivariee)\n",
    "    - 5.4 [Détection et traitement des variables collinéaires](#detection-et-traitement-des-variables-collineaires)\n",
    "    - 6.1 [Application du pipeline de prétraitement (KNN)](#pipeline-knn)\n",
    "    - 6.2 [Application du pipeline de prétraitement (MICE)](#pipeline-mice)\n",
    "    - 6.3 [Comparaison des méthodes d'imputation](#comparaison-methodes)\n",
    "    - 6.4 [Génération des fichiers pour la modélisation](#generation-des-fichiers-pour-la-modelisation)\n",
    "7. [Validation du prétraitement](#validation-pretraitement)\n",
    "    - 7.1 [Vérification de la qualité des données](#verification-qualite)\n",
    "9. [Annexes / Visualisations complémentaires](#annexes)"
   "execution_count": null,
   "outputs": [],
    "# Installation des packages (décommenter si nécessaire)\n",
    "# 🔍 Détection de l'environnement\n",
    "print(f\"🔧 Environnement détecté : {ENV}\")\n",
    "# 🚗 Montage de Google Drive si nécessaire\n",
    "    print(\"✅ Google Drive monté avec succès\")\n",
    "# 📁 Définir les chemins selon l’environnement\n",
    "    print(f\"✅ Module path ajouté : {module_path}\")\n",
    "# 📦 Chargement des chemins\n",
    "    print(\"✅ Configuration des chemins réussie\")\n",
    "    print(f\"❌ Erreur : impossible d'importer setup_project_paths\")\n",
    "# 🔧 Ajout manuel OUTPUTS_DIR si absent\n",
    "# 📋 Affichage Markdown des chemins\n",
    "        key: \"✅\" if Path(path).exists() else \"❌\"\n",
    "### 📂 **Chemins configurés pour le projet**\n",
    "**Légende :** ✅ = Existe | ❌ = N'existe pas\n",
    "# 🔧 Infos système\n",
    "print(f\"\\n🐍 Python version : {sys.version.split()[0]}\")\n",
    "print(f\"📍 Working directory : {os.getcwd()}\")\n",
    "# 📌 Variables globales\n",
    "# 📌 Fichiers finaux filtrés (MICE)\n",
    "# (optionnel) Fichiers finaux filtrés (KNN)\n",
    "## 2.2 Import des bibliothèques"
   "execution_count": null,
   "outputs": [],
    "## 2.2 Import des bibliothèques <a id=\"import-des-bibliotheques\"></a>\n",
    "# 📦 Modules standards\n",
    "# 🧮 Manipulation des données\n",
    "# 📊 Visualisation\n",
    "# ⚙️ Prétraitement & Modèles\n",
    "from sklearn.experimental import enable_iterative_imputer  # ⬅️ Nécessaire pour IterativeImputer\n",
    "# 📈 Statistiques\n",
    "# 🔁 Optionnel : UMAP\n",
    "    print(\"⚠️ UMAP non disponible. Certaines visualisations seront désactivées.\")\n",
    "# 🎲 Configuration globale\n",
    "# Définition de RANDOM_STATE\n",
    "# 🔍 Vérification des modules critiques\n",
    "print(\"\\n🔍 Vérification des modules critiques :\")\n",
    "        print(f\"  ✅ {name}\")\n",
    "        print(f\"  ❌ {name} - problème lors de l'import\")\n",
    "# ⚙️ Résumé de configuration\n",
    "print(f\"\\n⚙️ Configuration :\")\n",
    "## 2.3 Configuration des paramètres du projet"
   "execution_count": null,
   "outputs": [],
    "## 2.3 Configuration des paramètres du projet <a id=\"configuration-parametres-projet\"></a>\n",
    "# 📦 Import de la classe ProjectConfig et de la fonction de création\n",
    "    print(\"❌ Erreur : impossible d'importer depuis 'config.project_config'\")\n",
    "# 🛠️ Création de la configuration avec les métadonnées + chemins\n",
    "# 🎯 Ciblage de la métrique F1 pour le challenge\n",
    "# 👁️ Affichage de la configuration\n",
    "# 📌 Exemples d'accès à des valeurs clés\n",
    "print(\"\\n📌 Exemples d'accès à la configuration :\")\n",
    "print(f\"  - Méthode d'imputation X4 : {config.get('PROJECT_CONFIG.IMPUTATION_METHODS.X4')}\")\n",
    "# 💾 Sauvegarde de la configuration dans le dossier 'config'\n",
    "# 🌍 Rendre la configuration disponible globalement\n",
    "print(\"\\n✅ Configuration chargée et disponible globalement\")\n"
    "# 3. Chargement et aperçu des données <a id=\"chargement-et-apercu-des-donnees\"></a>\n",
    "## 3.1 Chargement des jeux de données bruts <a id=\"chargement-des-jeux-de-donnees-bruts\"></a>\n",
    "**Objectif** : Charger les datasets d’entraînement (`data_train.csv`) et de test (`data_test.csv`), vérifier leur structure, et préparer la variable cible pour l’analyse exploratoire.\n",
    "**Théorie** : Un chargement correct des données est essentiel pour garantir la reproductibilité et la validité des analyses. La vérification des dimensions et des types de données permet de détecter les erreurs tôt dans le processus.\n",
    "**Méthodologie** : Nous utilisons la fonction `load_data` pour charger les fichiers CSV, nettoyons les données (suppression des guillemets, gestion des doublons), encodons la variable cible (`ad.` → 1, `noad.` → 0), et affichons un résumé des dimensions et types.\n",
    "**Prochaines étapes** : Inspecter les colonnes et types (section 3.2) et analyser la distribution de la variable cible (section 3.3)."
   "execution_count": null,
   "outputs": [],
    "## 3.1 Chargement des jeux de données bruts <a id=\"chargement-des-jeux-de-donnees-bruts\"></a>\n",
    "#from modules.preprocessing.data_loader import load_data  # en local si jamais ne fonctionne pas, décoche ce si.\n",
    "# 📁 Vérification de l’existence du dossier RAW_DATA_DIR\n",
    "    raise NameError(\"❌ RAW_DATA_DIR n’est pas défini. Vérifiez la configuration dans la section 2.1.\")\n",
    "    raise FileNotFoundError(f\"❌ Dossier RAW_DATA_DIR introuvable : {raw_data_dir}\")\n",
    "# 📂 Chargement des fichiers CSV\n",
    "print(\"📂 Chargement des jeux de données...\")\n",
    "# 🏷️ Renommer 'outcome' en 'y' si nécessaire\n",
    "    print(\"✅ Colonne 'outcome' renommée en 'y'\")\n",
    "    raise ValueError(\"❌ Colonne 'y' ou 'outcome' manquante dans df_study\")\n",
    "# 🔢 Vérification des dimensions attendues\n",
    "    print(f\"⚠️ Dimensions inattendues pour df_study : {df_study.shape} (attendu : {expected_train_shape})\")\n",
    "    print(f\"⚠️ Dimensions inattendues pour df_eval : {df_eval.shape} (attendu : {expected_test_shape})\")\n",
    "# 🔍 Vérification de la variable cible\n",
    "print(\"\\n🔎 Valeurs uniques de y :\", df_study['y'].unique())\n",
    "print(\"🔎 Type de y :\", df_study['y'].dtype)\n",
    "# 📊 Résumé\n",
    "print(f\"\\n📊 Résumé :\")\n",
    "print(f\"  - Fichier d’étude     : {df_study.shape}\")\n",
    "print(f\"  - Fichier d’évaluation : {df_eval.shape}\")\n",
    "print(\"\\n✅ Chargement terminé avec succès !\")\n"
   "execution_count": null,
   "outputs": [],
    "print(\"🔍 Inspection des types de données\")\n",
    "# 🔎 Résumé des types dans df_study\n",
    "print(\"\\n📊 Types de données dans df_study :\")\n",
    "# 📌 Identification des colonnes par type\n",
    "# 🔁 Vérification binaire réelle parmi les colonnes int\n",
    "print(f\"\\n📈 Colonnes continues : {len(continuous_cols)}\")\n",
    "print(f\"🔢 Colonnes binaires : {len(binary_cols)}\")\n",
    "print(f\"📦 Colonnes catégorielles : {len(categorical_cols)}\")\n",
    "    print(f\"⚠️ Colonnes int64 non binaires détectées (extrait) : {non_binary_cols[:5]}\")\n",
    "# 🎯 Variable cible 'y'\n",
    "    print(\"\\n🎯 Variable cible 'y' :\")\n",
    "    print(\"❌ La colonne cible 'y' est manquante\")\n",
    "# 🔄 Comparaison avec df_eval\n",
    "print(\"\\n🔄 Comparaison avec df_eval :\")\n",
    "# ⚖️ Vérification des types entre df_study et df_eval\n",
    "print(\"\\n🔍 Vérification de la cohérence des types entre fichiers :\")\n",
    "    print(\"⚠️ Incohérences de type détectées :\")\n",
    "        print(f\"  - {col}: {t1} (étude) vs {t2} (éval)\")\n",
    "    print(\"✅ Types cohérents entre df_study et df_eval\")\n",
    "# 💾 Mise à jour de la configuration\n",
    "print(\"\\n💾 Mise à jour de la configuration...\")\n",
    "# 📋 Résumé final\n",
    "print(\"\\n📊 Résumé des colonnes (hors 'y') :\")\n",
    "print(f\"  - Colonnes catégorielles : {len(categorical_cols)}\")\n",
   "execution_count": null,
   "outputs": [],
    "print(\"\\n🎯 Analyse de la distribution de la variable cible\")\n",
    "# Vérification de la présence de 'y'\n",
    "    raise ValueError(\"❌ Colonne cible 'y' introuvable dans df_study\")\n",
    "print(\"\\n📊 Distribution de la variable cible (y) :\")\n",
    "# Ratio de déséquilibre\n",
    "print(f\"\\n📈 Ratio de déséquilibre : {imbalance_ratio:.2f}:1\")\n",
    "print(f\"   → Pour chaque publicité, il y a {imbalance_ratio:.1f} non-publicités\")\n",
    "ax1.set_ylabel('Nombre d\\'échantillons')\n",
    "ax1.set_xticklabels(['Non-publicité (0)', 'Publicité (1)'], rotation=0)\n",
    "# Sauvegarde sécurisée dans un sous-dossier\n",
    "# Impact pour la modélisation\n",
    "print(\"\\n💡 Implications pour la modélisation :\")\n",
    "print(f\"  - Dataset fortement déséquilibré ({imbalance_ratio:.1f}:1)\")\n",
    "print(\"  - Stratégies recommandées :\")\n",
    "print(\"    • Utiliser stratify=True lors du train/test split\")\n",
    "print(\"    • Appliquer SMOTE ou class_weight='balanced'\")\n",
    "print(\"    • Optimiser pour F1-score (métrique du challenge)\")\n",
    "print(\"    • Envisager un stacking ou un modèle robuste aux déséquilibres\")\n",
    "p = target_pct[1] / 100  # Précision et recall identiques si on prédit toujours 1\n",
    "print(f\"\\n📊 F1-score baseline (prédire toujours 'ad.') : {baseline_f1:.3f}\")\n",
    "print(\"   → Les modèles devront dépasser ce seuil pour être utiles\")\n"
    "**Objectif** : Identifier les valeurs manquantes dans le dataset d’entraînement et d’évaluation, analyser leur pattern (MCAR, MAR, MNAR), et proposer une stratégie d’imputation adaptée.\n",
    "**Théorie** : Les valeurs manquantes peuvent être MCAR (aléatoires), MAR (liées à d’autres variables observées), ou MNAR (liées à la variable elle-même). Une corrélation significative entre l’indicateur de valeurs manquantes et la variable cible suggère un pattern MAR, nécessitant une imputation sophistiquée (k-NN, MICE).\n",
    "**Méthodologie** : Nous calculons le pourcentage de valeurs manquantes par colonne, visualisons leur pattern via une heatmap, et analysons la corrélation entre les indicateurs de valeurs manquantes et la variable cible encodée. Une stratégie d’imputation est proposée en fonction des résultats.\n",
    "**Prochaines étapes** : Si un pattern MAR est confirmé, préparer une imputation multivariée (section 5.3). Vérifier l’impact des imputations sur les performances des modèles."
   "execution_count": null,
   "outputs": [],
    "print(\"🔍 Analyse des valeurs manquantes\")\n",
    "print(\"\\n📊 Analyse globale des valeurs manquantes :\")\n",
    "# Analyse détaillée pour les colonnes continues\n",
    "print(\"\\n📈 Détail des valeurs manquantes pour les variables continues :\")\n",
    "        # Créer une matrice binaire des valeurs manquantes\n",
    "        sns.heatmap(missing_matrix.T, cmap='RdYlBu', cbar_kws={'label': 'Manquant (1) / Présent (0)'})\n",
    "        plt.xlabel('Échantillons')\n",
    "        print(\"\\n🔍 Analyse du type de valeurs manquantes (MAR vs MCAR) :\")\n",
    "        # Corrélation entre les valeurs manquantes et la cible\n",
    "            print(f\"  - {col}: corrélation avec y = {correlation_with_target:.3f}\")\n",
    "                print(f\"    → Potentiellement MAR (Missing At Random)\")\n",
    "                print(f\"    → Potentiellement MCAR (Missing Completely At Random)\")\n",
    "    print(\"\\n✅ Aucune valeur manquante détectée dans le dataset !\")\n",
    "# Analyse pour le fichier d'évaluation aussi\n",
    "print(\"\\n📊 Analyse des valeurs manquantes dans le fichier d'évaluation :\")\n",
    "    print(\"\\n🔄 Comparaison des patterns de valeurs manquantes :\")\n",
    "    print(f\"  - Fichier d'étude : {missing_stats['percent_missing']:.2f}% manquant\")\n",
    "    print(f\"  - Fichier d'évaluation : {missing_stats_eval['percent_missing']:.2f}% manquant\")\n",
    "    # Stratégie d'imputation recommandée\n",
    "    print(\"\\n💡 Stratégie d'imputation recommandée :\")\n",
    "            print(f\"  - X4 ({x4_missing_pct:.1f}% manquant) : Imputation par la médiane\")\n",
    "        print(f\"  - X1, X2, X3 (variables continues) : KNN ou MICE (imputation multivariée)\")"
   "execution_count": null,
   "outputs": [],
    "print(\"\\n🔧 Correction du type de X4...\")\n",
    "# Vérifier que X4 ne contient que 0 et 1\n",
    "    # Imputer d'abord les valeurs manquantes par la médiane\n",
    "    print(f\"✅ X4 converti en int64 après imputation par la médiane ({X4_median})\")\n",
    "    # Mettre à jour la configuration\n",
    "    continuous_cols = ['X1', 'X2', 'X3']  # Mise à jour locale\n",
    "    print(\"⚠️ X4 contient des valeurs autres que 0 et 1, conservation en float64\")\n",
    "\n",
    "# Résumé final\n",
    "print(\"\\n📊 Résumé des valeurs manquantes après traitement de X4 :\")\n",
    "print(f\"  - X1, X2, X3 : ~27% manquant → À traiter avec KNN/MICE\")\n",
    "print(f\"  - X4 : Imputé et converti en binaire\")\n",
    "print(f\"  - Pattern MAR détecté pour X1, X2, X3 (corrélation avec y ≈ -0.10)\")\n",
    "print(f\"  - Les patterns sont cohérents entre fichiers d'étude et d'évaluation\")"
    "print(\"📊 Analyse statistique des variables quantitatives\")\n",
    "# Lancement de l’analyse complète\n",
   "execution_count": null,
   "outputs": []
    "print(\"📊 Visualisation des distributions et des boxplots\")\n",
   "execution_count": null,
   "outputs": []
    "## 📊 Synthèse de l'analyse statistique\n",
    "### Variables analysées : X1, X2, X3 (~1780 observations chacune)\n",
    "**🔍 Principales observations :**\n",
    "- **Distributions non-normales** : Toutes variables fortement asymétriques (skewness : 1.6 à 7.1)\n",
    "- **293 outliers** détectés (~16% des données)\n",
    "- **Corrélations notables** : X2-X3 (r=0.53), X1-X3 (r=-0.29)\n",
    "**⚠️ Points d'attention :**\n",
    "- Écart important moyenne/médiane pour toutes variables\n",
    "- X3 particulièrement problématique (skewness=7.06, kurtosis=63.4)\n",
    "- Tests de Shapiro-Wilk : p<0.001 (rejet normalité)\n",
    "**🔄 Actions requises :**\n",
    "- **Transformation Yeo-Johnson** recommandée avant analyse paramétrique\n",
    "- Considérer méthodes robustes/non-paramétriques\n",
   "execution_count": null,
   "outputs": [],
    "print(\"🔢 Analyse de la distribution des variables binaires\")\n",
    "print(f\"\\n📊 Nombre total de variables binaires : {len(binary_cols)}\")\n",
    "# Taux de présence (valeurs à 1)\n",
    "print(f\"\\n📊 Statistiques des taux de présence :\")\n",
    "print(f\"  - Médiane : {presence_series.median():.2f}%\")\n",
    "# Sparsité globale\n",
    "print(f\"\\n📊 Sparsité globale : {sparsity:.2f}% de zéros\")\n",
    "plt.xlabel('Taux de présence (%)')\n",
    "plt.title('Distribution des taux de présence des variables binaires')\n",
    "print(\"\\n✅ Analyse des variables binaires terminée\")\n",
    "print(\"   → Dataset très sparse, adapté pour des méthodes de sélection de features\")\n"
    "## 4.5 Analyse des corrélations combinées <a id=\"analyse-correlations-combinees\"></a>\n",
   "execution_count": null,
   "outputs": [],
    "## 4.5 Analyse des corrélations combinées <a id=\"analyse-correlations-combinees\"></a>\n",
    "print(\"🔗 Lancement de l'analyse combinée des corrélations (features ↔ cible, features ↔ features)...\")\n",
    "# Appel avec paramètres personnalisés\n",
    "## 📌 Synthèse de l'analyse des corrélations <a id=\"synthese-correlations\"></a>\n",
    "### 🔍 Corrélations avec la variable cible (`y`)\n",
    "- ✅ **Meilleure variable prédictive continue** : `X2` avec une corrélation de **0.573**\n",
    "- 📉 Les autres variables (continues ou binaires) ont une corrélation **faible à modérée** avec `y` (souvent < 0.2)\n",
    "- ℹ️ Cela suggère que la **modélisation devra combiner plusieurs variables** pour être efficace\n",
    "### 🔗 Corrélations entre variables\n",
    "- ⚠️ **1 paire** de variables (binaires ou continues) présente une **corrélation > 0.8**\n",
    "- ✅ **Multicolinéarité faible** → pas de besoin urgent de supprimer des variables continues\n",
    "- 📊 **3 506 paires** de variables binaires présentent une corrélation **> 0.95**\n",
    "- 🔁 Ces paires impliquent **de nombreuses variables dupliquées** ou très similaires\n",
    "- 🧠 Certaines variables sont impliquées dans **15+ paires corrélées**, suggérant des motifs de duplication\n",
    "### 🧭 Recommandations\n",
    "- 🧹 Appliquer une **réduction de dimension** avant la modélisation :\n",
    "  - Utilisation de **PCA**, **autoencoders** ou sélection par importance (e.g. **Random Forest**)\n",
    "- 🎯 Se concentrer sur `X2` et les variables binaires les plus corrélées à `y` comme features de base\n",
   "execution_count": null,
   "outputs": [],
    "print(\"📊 Visualisations exploratoires\")\n",
    "# Imports des fonctions refactorisées\n",
    "print(\"\\n📈 Distribution des variables continues par classe...\")\n",
    "# 2. Visualisation de la sparsité\n",
    "print(\"\\n📉 Visualisation de la sparsité des données binaires...\")\n",
    "# 3. Corrélations des variables continues avec la cible\n",
    "print(\"\\n🔗 Corrélations des variables continues avec la cible...\")\n",
    "# 4. Réduction de dimension avec UMAP / t-SNE / PCA\n",
    "print(\"\\n📊 Visualisation multidimensionnelle (PCA / t-SNE / UMAP)...\")\n",
    "df_study_viz['outcome'] = df_study_viz['y'].map({0: 'noad.', 1: 'ad.'})  # ✅ temporaire\n",
    "# 🔁 Recalcul des corrélations si besoin\n",
   "execution_count": null,
   "outputs": []
    "print(\"\\n🌲 Analyse de l’importance des features...\")\n",
    "    df_importance = df_sample.copy()  # contient outcome déjà transformée\n",
    "    print(f\"⚠️ Erreur lors de l’analyse d’importance des features : {e}\")\n",
    "# 6. Résumé visuel global\n",
    "print(\"\\n📊 Création du résumé visuel de l’EDA...\")\n",
    "print(\"\\n✅ Visualisations exploratoires terminées avec succès !\")"
   "execution_count": null,
   "outputs": []
    "## 🧭 Interprétations synthétiques des résultats de l'EDA <a id=\"interpretations-eda\"></a>\n",
    "### 🎯 1. Distribution des variables continues par classe\n",
    "- **X1, X2, X3** présentent des distributions très différentes entre les deux classes (`ad.` vs `noad.`).\n",
    "- **X2** se distingue particulièrement avec une séparation marquée entre les classes.\n",
    "- Les distributions sont asymétriques, avec la présence d’**outliers** visibles dans chaque classe.\n",
    "### 🧪 2. Corrélations avec la variable cible\n",
    "- **X2** est la variable la plus corrélée avec la cible (`corr = 0.573`), ce qui en fait une **feature clé**.\n",
    "- **X1** (`corr = 0.034`) et **X3** (`corr = 0.130`) ont des corrélations faibles mais non négligeables.\n",
    "- Cela suggère l’utilité de **modèles non linéaires** ou **ensemble methods** (e.g. Random Forest, Gradient Boosting).\n",
    "### 🧬 3. Visualisation de la sparsité des variables binaires\n",
    "- Le dataset est **extrêmement sparse**, avec **99.2% de zéros** dans les variables binaires.\n",
    "  - Risque de surapprentissage élevé si toutes les variables sont conservées.\n",
    "  - Nécessité de sélection de variables ou de techniques de réduction (PCA, autoencoders).\n",
    "  - Attention aux méthodes sensibles à la densité (ex : k-NN).\n",
    "### 🗺️ 4. Résumé visuel global\n",
    "- **Déséquilibre important** : 86% `noad.` vs 14% `ad.` → nécessite des stratégies adaptées :\n",
    "  - Métriques robustes (F1-score, recall).\n",
    "  - Rééchantillonnage ou `class_weight='balanced'`.\n",
    "- **Valeurs manquantes** (~27%) dans X1, X2, X3 : à imputer avec méthode robuste (KNN, MICE).\n",
    "- **Top corrélations** concentrées sur peu de variables → importance d’une **bonne sélection de features**.\n",
    "### ✅ Recommandations clés\n",
    "- **Prétraitement renforcé** :\n",
    "  - Transformation des variables continues (Yeo-Johnson recommandée).\n",
    "  - Suppression ou gestion des outliers extrêmes.\n",
    "- **Réduction de dimension** :\n",
    "  - Visualisation UMAP/t-SNE utile pour vérifier la structure.\n",
    "  - Sélection de features importante avant modélisation (selon importance ou redondance).\n",
    "- **Rééquilibrage des classes** indispensable pour éviter un biais fort du modèle vers la classe majoritaire.\n",
    "# 5. Prétraitement avancé <a id=\"pretraitement-avance\"></a>\n"
    "#### 🔁 Transformation des variables continues\n",
    "Les variables `X1`, `X2` et `X3` présentent une forte asymétrie positive ainsi que des valeurs extrêmes détectées via la règle de l’IQR.\n",
    "#### 📌 Objectif :\n",
    "- Réduire l’impact des outliers\n",
    "- Améliorer la distribution pour les modèles sensibles à la normalité (régression logistique, kNN…)\n",
    "#### ⚙️ Méthode :\n",
    "> 🔧 Les colonnes transformées seront ajoutées en tant que `X1_trans`, `X2_trans` et `X3_trans`."
    "# Vérification visuelle rapide des variables transformées\n",
   "execution_count": null,
   "outputs": []
    "# ✅ Liste des variables transformées\n",
    "# 📁 Dossier de sauvegarde\n",
    "# 🔁 Génération et sauvegarde des figures\n",
    "    # 💾 Sauvegarde\n",
    "    print(f\"✅ Figure sauvegardée : {fig_path}\")\n"
   "execution_count": null,
   "outputs": []
    "### 🔁 Transformation Yeo-Johnson des variables continues <a id=\"yeojohnson-interprétation\"></a>\n",
    "### Résultats visuels\n",
    "- ✅ **Meilleure symétrie** des distributions.\n",
    "- ✅ **Réduction de l'effet des outliers** (même si certains persistent, notamment sur `X2`).\n",
    "- ✅ **Concentration des valeurs** autour de la médiane, utile pour les modèles sensibles aux échelles et à la normalité.\n",
    "### Interprétation\n",
    "| Variable   | Résultat après transformation                             | Commentaire                                                                 |\n",
    "| `X1_trans` | Distribution plus centrée et symétrique                   | Forte amélioration visuelle, outliers encore présents mais moins extrêmes   |\n",
    "| `X2_trans` | Distribution toujours multimodale avec quelques extrêmes  | Transformation partiellement efficace – normalisation partielle             |\n",
    "| `X3_trans` | Distribution globalement normalisée                       | Très bon résultat – faible asymétrie et étendue réduite                     |\n",
    "### 📌 Conclusion\n",
    "- La transformation **Yeo-Johnson** est efficace pour réduire l'asymétrie des variables `X1`, `X2` et `X3`.\n",
    "- Elle **prépare les données à des modèles linéaires** ou sensibles aux distances (kNN, régression).\n",
    "- Un **traitement complémentaire des outliers** peut être envisagé, surtout pour `X2`.\n"
    "## 5.2 Détection et suppression des outliers <a id=\"detection-et-suppression-des-outliers\"></a>\n",
    "### 🎯 Objectifs :\n",
    "- Identifier les observations extrêmes susceptibles de perturber la modélisation.\n",
    "- Appliquer une stratégie de suppression uniquement sur les variables continues (`X1`, `X2`, `X3`), après transformation.\n",
    "### 🛠️ Méthode :\n",
    "- Utilisation de la règle de l’IQR (Interquartile Range).\n",
    "- Application sur les colonnes transformées : `X1_trans`, `X2_trans`, `X3_trans`.\n",
    "### 📉 Impact attendu :\n",
    "- Réduction de l’effet des valeurs extrêmes sur les modèles sensibles.\n",
    "- Meilleure normalité après transformation.\n",
    "- Perte contrôlée d’observations (généralement < 5%).\n",
    "### ✅ Étapes suivantes :\n",
    "1. Détection via IQR (Q1 - 1.5×IQR, Q3 + 1.5×IQR)\n",
    "2. Comptage des lignes extrêmes par variable\n",
    "4. Affichage du pourcentage de données supprimées\n",
    "> 🔍 Cette étape ne sera appliquée que sur `df_study` (jeu d'entraînement).\n"
    "## 5.2 Détection et suppression des outliers <a id=\"detection-et-suppression-des-outliers\"></a>\n",
    "print(\"🔍 Détection et suppression des outliers (méthode IQR)\")\n",
    "# ✅ Variables à traiter (transformées)\n",
    "# ✅ Sauvegarde de la version avant suppression\n",
    "# ✅ Chemin de sauvegarde après nettoyage\n",
    "# ✅ Suppression des outliers avec export CSV\n",
    "# ✅ Aperçu statistique post-nettoyage\n",
    "print(\"\\n📊 Statistiques descriptives après suppression des outliers :\")\n",
   "execution_count": null,
   "outputs": []
    "## 📊 Visualisation comparative avant/après suppression des outliers\n",
    "# ✅ Comparaison visuelle avant / après (X1_trans, X2_trans, X3_trans)\n",
   "execution_count": null,
   "outputs": []
    "## 📉 Analyse des effets de la suppression des outliers <a id=\"effet-suppression-outliers\"></a>\n",
    "La détection des outliers est effectuée sur les variables transformées (`X1_trans`, `X2_trans`, `X3_trans`) via la méthode de l’IQR (Interquartile Range), afin de réduire l’influence des valeurs extrêmes sur les modèles.\n",
    "### 🔍 Résultats :\n",
    "#### ✅ **X1_trans**\n",
    "- **Avant** : présence de plusieurs outliers extrêmes à gauche et à droite.\n",
    "- **Après** : distribution recentrée, disparition des extrêmes anormaux.\n",
    "- **Effet attendu** : meilleure stabilité pour les modèles linéaires sensibles à la variance.\n",
    "#### ✅ **X2_trans**\n",
    "- **Avant** : distribution asymétrique avec une concentration importante d’outliers à gauche (valeurs faibles).\n",
    "- **Après** : distribution plus compacte, réduction de l’asymétrie, moins d’observations extrêmes.\n",
    "- **Effet attendu** : amélioration de la normalité et du comportement statistique de la variable.\n",
    "#### ✅ **X3_trans**\n",
    "- **Avant** : très peu d’outliers détectés, distribution relativement homogène.\n",
    "- **Après** : suppression minimale, confirmant que X3_trans était déjà bien normalisée.\n",
    "- **Effet attendu** : impact marginal, mais bénéfique pour les modèles robustes.\n",
    "### 🎯 Conclusion :\n",
    "- La suppression des outliers permet d’obtenir des distributions plus resserrées et symétriques.\n",
    "- Elle améliore la qualité des données, tout en préservant la majorité des observations informatives.\n",
    "- Deux versions du dataset sont conservées :\n",
    "  - **Avec outliers** : pour tester la robustesse des modèles.\n",
    "  - **Sans outliers** : pour évaluer les gains en stabilité et performance.\n"
    "La gestion des valeurs manquantes est cruciale pour garantir la qualité des analyses et des modèles.\n",
    "### 🔎 Objectifs :\n",
    "- Préserver la structure statistique du dataset\n",
    "### ⚙️ Méthodologie adoptée :\n",
    "- Imputation simple (médiane) pour certaines variables\n",
    "- Sauvegarde des jeux de données imputés pour modélisation\n"
    "### ✅ 5.3.1 Imputation de X4 par la médiane <a id=\"imputation-x4-mediane\"></a>\n",
    "La variable `X4`, de type discrète (0/1), a été imputée **précocement par la médiane**, ce qui est adapté à une variable binaire avec peu de valeurs manquantes.  \n",
    "→ Aucun traitement supplémentaire n’est nécessaire ici.\n",
    "### 🚧 Prochaine étape : **imputation multiple** sur les variables continues `X1`, `X2`, `X3` via des méthodes plus robustes (KNN ou MICE).\n"
    "### 5.3.2 Préparation pour l'imputation multivariée <a id=\"preparation-imputation-multivariee\"></a>"
    "## 5.3.2 Préparation pour l'imputation multivariée <a id=\"preparation-imputation-multivariee\"></a>\n",
    "print(\"🔧 Préparation à l'imputation multiple (KNN / MICE)\")\n",
    "# 📁 Analyse sur les données AVEC outliers\n",
    "print(\"\\n📊 Analyse (données avec outliers)\")\n",
    "# ✅ Colonnes à imputer (si moins de 30 % de valeurs manquantes)\n",
    "print(\"\\n📌 Colonnes retenues (avec outliers) :\")\n",
    "# 📁 Analyse sur les données SANS outliers\n",
    "print(\"\\n📊 Analyse (données sans outliers)\")\n",
    "print(\"\\n📌 Colonnes retenues (sans outliers) :\")\n",
   "execution_count": null,
   "outputs": []
    "### 5.3.3 Imputation multivariée (MICE) <a id=\"imputation-multivariee-mice\"></a>"
    "#### Imputation MICE - données avec outliers"
    "#### Imputation multivariée (MICE) - données avec outliers\n",
    "print(\"🧩 Imputation multivariée (données avec outliers)\")\n",
    "# 📌 Colonnes concernées\n",
    "# 📁 Chemin de sauvegarde du résultat\n",
    "# ✅ Lancer l'imputation multiple sur les données avec outliers\n",
    "    mcar_cols=[],                      # X4 déjà traité\n",
    "# 🔍 Aperçu\n",
   "execution_count": null,
   "outputs": []
    "#### Imputation Mice - données avec outliers"
    "#### Imputation multivariée (MICE) - données sans outliers\n",
    "print(\"🧩 Imputation multivariée (données sans outliers)\")\n",
    "# 📁 Chemin de sauvegarde\n",
    "# ✅ Imputation sur les données nettoyées (sans outliers)\n",
    "    df=df_study,  # Ce DataFrame a les outliers supprimés\n",
    "    custom_filename=save_path_no_outliers.name  # ✅ Permet d'utiliser un nom de fichier personnalisé\n",
    "# 🔍 Aperçu des données imputées\n",
   "execution_count": null,
   "outputs": []
    "#### Coparaison imputation sur les données avec et sans outliers\n",
    "# Chargement des deux versions imputées\n",
    "# Variables à comparer\n",
    "# Création des graphiques\n",
   "execution_count": null,
   "outputs": []
    "# 📁 Chemins de sauvegarde\n",
    "# 🔍 Recherche du k optimal pour KNN (avec outliers)\n",
    "print(\"🔍 Recherche du k optimal pour KNN Imputer (avec outliers)\")\n",
    "print(f\"✅ k optimal déterminé (avec outliers) : {optimal_k}\")\n",
    "# ✅ Imputation avec outliers\n",
    "# 🔍 Recherche du k optimal pour KNN (sans outliers)\n",
    "print(\"\\n🔍 Recherche du k optimal pour KNN Imputer (sans outliers)\")\n",
    "print(f\"✅ k optimal déterminé (sans outliers) : {optimal_k_no_outliers}\")\n",
    "# ✅ Imputation sans outliers\n",
   "execution_count": null,
   "outputs": []
    "## 5.4 Détection et traitement des variables collinéaires <a id=\"detection-et-traitement-des-variables-collineaires\"></a>"
    "## 5.4 Détection et traitement des variables collinéaires <a id=\"detection-et-traitement-des-variables-collineaires\"></a>\n",
    "print(f\"🔗 {len(correlated_info['groups'])} groupes détectés\")\n",
    "print(f\"❌ {len(correlated_info['to_drop'])} variables à supprimer\")\n",
   "execution_count": null,
   "outputs": []
    "### 5.4.1 Suppression des variables collinéaires <a id=\"suppression-collineaires\"></a>"
    "print(\"🧹 Suppression des variables fortement corrélées\")\n",
    "# Suppression pour les données imputées par MICE\n",
    "# Suppression pour les données imputées par KNN\n",
    "# Vérification des dimensions\n",
    "print(f\"\\n📊 Dimensions après suppression (MICE - avec outliers)     : {df_with_outliers_filtered.shape}\")\n",
    "print(f\"📊 Dimensions après suppression (MICE - sans outliers)     : {df_no_outliers_filtered.shape}\")\n",
    "print(f\"📊 Dimensions après suppression (KNN  - avec outliers)     : {df_with_outliers_filtered_knn.shape}\")\n",
    "print(f\"📊 Dimensions après suppression (KNN  - sans outliers)     : {df_no_outliers_filtered_knn.shape}\")\n"
   "execution_count": null,
   "outputs": []
    "### 5.4.2 Sauvegarde des datasets filtrés <a id=\"sauvegarde-datasets-filtres\"></a>\n"
    "### 5.4.2 Sauvegarde des datasets filtrés <a id=\"sauvegarde-datasets-filtres\"></a>\n",
    "print(\"💾 Sauvegarde des jeux de données filtrés\")\n",
    "# ✅ Définition du dossier de sauvegarde\n",
    "# === Sauvegarde des versions imputées par MICE ===\n",
    "print(f\"✅ Fichier sauvegardé : {mice_with_path}\")\n",
    "print(f\"✅ Fichier sauvegardé : {mice_no_path}\")\n",
    "# === Sauvegarde des versions imputées par KNN ===\n",
    "print(f\"✅ Fichier sauvegardé : {knn_with_path}\")\n",
    "print(f\"✅ Fichier sauvegardé : {knn_no_path}\")\n",
   "execution_count": null,
   "outputs": []
    "# 📁 Chemins configurés automatiquement\n",
   "execution_count": null,
    "## 6.1 Application du pipeline de prétraitement (KNN) <a id=\"pipeline-knn\"></a>"
    "# Imputation KNN – sans outliers\n",
   "execution_count": null,
   "outputs": []
    "#Imputation KNN – avec outliers\n",
    "# Appel du pipeline pour les données imputées par knn, avec outliers\n",
   "execution_count": null,
   "outputs": []
    "#Imputation KNN – sans outliers\n",
    "# Appel du pipeline pour les données imputées par knn, sans outliers\n",
    "    save_transformer=False,  # déjà sauvegardé précédemment\n",
   "execution_count": null,
   "outputs": []
    "## 6.2 Application du pipeline de prétraitement (MICE) <a id=\"pipeline-mice\"></a>"
    "    mar_method=\"mice\",                # Méthode d’imputation : MICE\n",
    "    correlation_threshold=0.95,       # Seuil pour corrélation\n",
    "    save_transformer=False,            # déjà sauvegardé précédemment\n",
    "    display_info=True                 # Affichage détaillé\n",
   "execution_count": null,
   "outputs": []
    "    save_transformer=False,  # déjà sauvegardé précédemment\n",
   "execution_count": null,
   "outputs": []
    "## 6.3 Comparaison des méthodes d'imputation <a id=\"comparaison-methodes\"></a>"
    "# 7. Validation du prétraitement <a id=\"validation-pretraitement\"></a>\n"
    "## 7.1 Vérification de la qualité des données <a id=\"verification-qualite\"></a>"
    "# 9. Annexes / Visualisations complémentaires <a id=\"annexes\"></a>"
