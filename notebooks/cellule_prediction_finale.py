# =============================================================================
# CELLULE DE PRÃ‰DICTION FINALE - NOTEBOOK 03
# =============================================================================
# 
# Cette cellule gÃ©nÃ¨re les prÃ©dictions finales pour le challenge STA211
# en utilisant le modÃ¨le champion (Stacking sans refit KNN)
# 
# RÃ©sultat : Fichier de soumission conforme aux exigences R
# =============================================================================

print("ğŸš€ DÃ‰MARRAGE DE LA GÃ‰NÃ‰RATION DES PRÃ‰DICTIONS FINALES")
print("=" * 60)

# -----------------------------------------------------------------------------
# 1. CHARGEMENT DES DONNÃ‰ES DE TEST
# -----------------------------------------------------------------------------
print("\nğŸ“‚ Ã‰tape 1: Chargement des donnÃ©es de test...")

# Charger les donnÃ©es de test
test_data_path = RAW_DATA_DIR / "data_test.csv"
df_test = pd.read_csv(test_data_path, sep='\t', quoting=1, skipinitialspace=True)

# Nettoyer les noms de colonnes
df_test.columns = df_test.columns.str.strip().str.replace('"', '').str.replace("'", "")

# Extraire les IDs (premiÃ¨re colonne) et les features
ids = df_test.iloc[:, 0]  # PremiÃ¨re colonne = IDs
features = df_test.iloc[:, 1:]  # Reste = features

# Nettoyer les features
for col in features.columns:
    features[col] = features[col].astype(str).str.replace('"', '').str.replace("'", "")
    features[col] = pd.to_numeric(features[col], errors='coerce')

# Remplacer les valeurs NaN par 0
features = features.fillna(0)

print(f"âœ… DonnÃ©es chargÃ©es : {features.shape[0]} lignes, {features.shape[1]} colonnes")
print(f"ğŸ“‹ PremiÃ¨res colonnes : {list(features.columns[:5])}")

# -----------------------------------------------------------------------------
# 2. PRÃ‰TRAITEMENT COMPLET (NOTEBOOK 01)
# -----------------------------------------------------------------------------
print("\nğŸ”„ Ã‰tape 2: PrÃ©traitement complet avec KNN...")

# Ã‰tape 2.1: Imputation X4 (mÃ©diane)
median_path = MODELS_DIR / "notebook1" / "median_imputer_X4.pkl"
median_value = joblib.load(median_path)
features['X4'] = features['X4'].fillna(median_value)

# Ã‰tape 2.2: Imputation KNN pour X1, X2, X3
continuous_cols = ['X1', 'X2', 'X3']
df_continuous = features[continuous_cols].copy()

imputer_path = MODELS_DIR / "notebook1" / "knn" / "imputer_knn_k7.pkl"
imputer = joblib.load(imputer_path)
df_imputed_continuous = pd.DataFrame(
    imputer.transform(df_continuous), 
    columns=continuous_cols, 
    index=features.index
)

# Remplacer les colonnes originales
for col in continuous_cols:
    features[col] = df_imputed_continuous[col]

# Ã‰tape 2.3: Transformations (Yeo-Johnson + Box-Cox)
yj_path = MODELS_DIR / "notebook1" / "knn" / "knn_transformers" / "yeo_johnson_X1_X2.pkl"
bc_path = MODELS_DIR / "notebook1" / "knn" / "knn_transformers" / "box_cox_X3.pkl"

transformer_yj = joblib.load(yj_path)
transformer_bc = joblib.load(bc_path)

features[['X1', 'X2']] = transformer_yj.transform(features[['X1', 'X2']])
df_x3 = features[['X3']].copy()
if (df_x3['X3'] <= 0).any(): 
    df_x3['X3'] += 1e-6
features['X3'] = transformer_bc.transform(df_x3)

# Renommer les colonnes transformÃ©es
features.rename(columns={
    'X1': 'X1_transformed', 
    'X2': 'X2_transformed', 
    'X3': 'X3_transformed'
}, inplace=True)

# Ã‰tape 2.4: Capping
capping_path = MODELS_DIR / "notebook1" / "knn" / "capping_params_knn.pkl"
capping_params = joblib.load(capping_path)

for col in ['X1_transformed', 'X2_transformed', 'X3_transformed']:
    bounds = capping_params.get(col, {})
    features[col] = np.clip(
        features[col], 
        bounds.get('lower_bound'), 
        bounds.get('upper_bound')
    )

# Ã‰tape 2.5: Suppression de la colinÃ©aritÃ©
corr_path = MODELS_DIR / "notebook1" / "knn" / "cols_to_drop_corr_knn.pkl"
cols_to_drop = joblib.load(corr_path)
features = features.drop(columns=cols_to_drop, errors='ignore')

# Ã‰tape 2.6: IngÃ©nierie de caractÃ©ristiques polynomiales
poly_path = MODELS_DIR / "notebook1" / "knn" / "poly_transformer_knn.pkl"
poly_transformer = joblib.load(poly_path)

continuous_cols = ['X1_transformed', 'X2_transformed', 'X3_transformed']
continuous_features = features[continuous_cols]
poly_features = poly_transformer.transform(continuous_features)
poly_feature_names = poly_transformer.get_feature_names_out(continuous_cols)
df_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=features.index)

features = features.drop(columns=continuous_cols).join(df_poly)

print(f"âœ… PrÃ©traitement terminÃ©. Shape final : {features.shape}")

# -----------------------------------------------------------------------------
# 3. SÃ‰LECTION DES COLONNES ET CHARGEMENT DU SEUIL
# -----------------------------------------------------------------------------
print("\nğŸ“‹ Ã‰tape 3: SÃ©lection des colonnes et chargement du seuil...")

# Charger les colonnes attendues pour KNN
columns_knn = joblib.load(MODELS_DIR / "notebook2" / "knn" / "columns_knn.pkl")

# SÃ©lectionner seulement les colonnes utilisÃ©es dans l'entraÃ®nement
available_columns = [col for col in columns_knn if col in features.columns]
missing_columns = [col for col in columns_knn if col not in features.columns]

if missing_columns:
    print(f"âš ï¸ Colonnes manquantes : {missing_columns}")
    # Ajouter des colonnes avec des valeurs par dÃ©faut
    for col in missing_columns:
        features[col] = 0

features = features[columns_knn]

# Charger le seuil optimal
threshold_path = MODELS_DIR / "notebook3" / "stacking_champion_threshold.json"
with open(threshold_path, 'r') as f:
    threshold_data = json.load(f)
    threshold = threshold_data["threshold"]

print(f"âœ… Seuil optimal chargÃ© : {threshold:.4f}")

# -----------------------------------------------------------------------------
# 4. GÃ‰NÃ‰RATION DES META-FEATURES (STACKING SANS REFIT)
# -----------------------------------------------------------------------------
print("\nğŸ”„ Ã‰tape 4: GÃ©nÃ©ration des meta-features...")

meta_features = {}
model_names = ["SVM", "XGBoost", "RandForest", "GradBoost", "MLP"]

for model_name in model_names:
    try:
        pipeline_path = MODELS_DIR / "notebook2" / f"pipeline_{model_name.lower()}_knn.joblib"
        pipeline = joblib.load(pipeline_path)
        
        # GÃ©nÃ©rer les probabilitÃ©s pour chaque modÃ¨le
        proba = pipeline.predict_proba(features)[:, 1]
        meta_features[f"{model_name}_knn"] = proba
        print(f"âœ… Meta-features gÃ©nÃ©rÃ©es pour {model_name}")
    except Exception as e:
        print(f"âŒ Erreur pour {model_name}: {e}")
        # Utiliser des probabilitÃ©s par dÃ©faut en cas d'erreur
        meta_features[f"{model_name}_knn"] = np.full(len(features), 0.5)

# CrÃ©er le DataFrame des meta-features
df_meta = pd.DataFrame(meta_features)

# Calculer la moyenne des probabilitÃ©s (Stacking sans refit)
print("ğŸ“Š Calcul de la moyenne des probabilitÃ©s...")
proba_final = df_meta.mean(axis=1)

# -----------------------------------------------------------------------------
# 5. APPLICATION DU SEUIL ET GÃ‰NÃ‰RATION DES PRÃ‰DICTIONS
# -----------------------------------------------------------------------------
print("\nğŸ¯ Ã‰tape 5: Application du seuil et gÃ©nÃ©ration des prÃ©dictions...")

# Appliquer le seuil optimal
prediction_num = (proba_final >= threshold).astype(int)
prediction_label = np.where(prediction_num == 1, "ad.", "noad.")

# -----------------------------------------------------------------------------
# 6. CRÃ‰ATION DES FICHIERS DE SORTIE
# -----------------------------------------------------------------------------
print("\nğŸ’¾ Ã‰tape 6: CrÃ©ation des fichiers de sortie...")

# CrÃ©er le dossier de sortie
output_dir = OUTPUTS_DIR / "predictions"
output_dir.mkdir(parents=True, exist_ok=True)

# Convertir ids en liste si c'est une Series
if hasattr(ids, 'values'):
    ids_list = ids.values.tolist()
else:
    ids_list = list(ids)

# CrÃ©er le DataFrame dÃ©taillÃ© avec toutes les informations
detailed = pd.DataFrame({
    'ID': ids_list,
    'probabilite_stacking': proba_final.values,
    'prediction_stacking': prediction_label,
    'seuil_optimal': [threshold] * len(ids_list)
})

# CrÃ©er le fichier de soumission (seulement les prÃ©dictions, SANS EN-TÃŠTE)
submission = pd.DataFrame({
    'prediction': prediction_label
})

# Sauvegarder les fichiers
detailed_path = output_dir / "predictions_finales_stacking_knn_detailed.csv"
submission_path = output_dir / "predictions_finales_stacking_knn_submission.csv"

detailed.to_csv(detailed_path, index=False)
# Sauvegarder SANS en-tÃªte pour la soumission (format attendu par R)
submission.to_csv(submission_path, index=False, header=False)

# -----------------------------------------------------------------------------
# 7. AFFICHAGE DES STATISTIQUES FINALES
# -----------------------------------------------------------------------------
print("\nğŸ“Š STATISTIQUES FINALES")
print("=" * 40)

print(f"ğŸ“ˆ Nombre total de prÃ©dictions : {len(prediction_label)}")
print(f"ğŸ¯ PrÃ©dictions 'ad.' : {np.sum(prediction_num)} ({np.mean(prediction_num)*100:.1f}%)")
print(f"ğŸš« PrÃ©dictions 'noad.' : {len(prediction_num) - np.sum(prediction_num)} ({(1-np.mean(prediction_num))*100:.1f}%)")
print(f"âš–ï¸ Seuil utilisÃ© : {threshold:.4f}")

print(f"\nğŸ“ Fichiers gÃ©nÃ©rÃ©s :")
print(f"   ğŸ“„ DÃ©tailÃ© : {detailed_path}")
print(f"   ğŸ“„ Soumission : {submission_path}")

# -----------------------------------------------------------------------------
# 8. VALIDATION DU FORMAT
# -----------------------------------------------------------------------------
print("\nâœ… VALIDATION DU FORMAT")
print("=" * 30)

# VÃ©rifier le nombre de lignes
with open(submission_path, 'r') as f:
    lines = f.readlines()
    n_lines = len(lines)

print(f"ğŸ“Š Nombre de lignes dans le fichier de soumission : {n_lines}")
print(f"âœ… Conforme aux exigences (820 lignes) : {'OUI' if n_lines == 820 else 'NON'}")

# VÃ©rifier les valeurs uniques
unique_values = set(prediction_label)
print(f"ğŸ“‹ Valeurs uniques : {unique_values}")
print(f"âœ… Format correct (ad./noad.) : {'OUI' if unique_values == {'ad.', 'noad.'} else 'NON'}")

# VÃ©rifier qu'il n'y a pas d'en-tÃªte
first_line = lines[0].strip() if lines else ""
print(f"ğŸ“ PremiÃ¨re ligne : '{first_line}'")
print(f"âœ… Pas d'en-tÃªte : {'OUI' if first_line in ['ad.', 'noad.'] else 'NON'}")

# -----------------------------------------------------------------------------
# 9. RÃ‰CAPITULATIF FINAL
# -----------------------------------------------------------------------------
print("\nğŸ‰ RÃ‰CAPITULATIF FINAL")
print("=" * 40)

print("âœ… Pipeline de prÃ©diction terminÃ© avec succÃ¨s !")
print("âœ… Fichier de soumission conforme aux exigences R")
print("âœ… PrÃªt pour la soumission au challenge STA211")

print(f"\nğŸ“Š Distribution finale :")
print(f"   - PublicitÃ©s (ad.) : {np.sum(prediction_num)} ({np.mean(prediction_num)*100:.1f}%)")
print(f"   - Non-publicitÃ©s (noad.) : {len(prediction_num) - np.sum(prediction_num)} ({(1-np.mean(prediction_num))*100:.1f}%)")

print(f"\nğŸ¯ ModÃ¨le utilisÃ© : Stacking sans refit KNN")
print(f"âš–ï¸ Seuil optimal : {threshold:.4f}")
print(f"ğŸ“ Fichier de soumission : {submission_path}")

print("\n" + "=" * 60)
print("ğŸš€ PRÃ‰DICTIONS FINALES GÃ‰NÃ‰RÃ‰ES AVEC SUCCÃˆS !")
print("=" * 60) 