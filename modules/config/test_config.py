

#!/usr/bin/env python3

# modules/config/test_config.py

"""
Script de test pour valider les modules de configuration du projet STA211

Ce script teste tous les modules de configuration et valide que
l'environnement peut √™tre configur√© correctement.

Usage:
    python test_config.py
"""

import sys
import logging
from pathlib import Path
import traceback

# Configuration du logging pour les tests
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_paths_config():
    """Test du module paths_config"""
    print("\nüß™ Test: modules.config.paths_config")
    print("-" * 40)
    
    try:
        from paths_config import (
            detect_environment, 
            setup_project_paths,
            get_data_files_paths,
            validate_paths
        )
        
        # Test d√©tection environnement
        env = detect_environment()
        print(f"  ‚úÖ Environnement d√©tect√©: {env}")
        
        # Test configuration chemins (sans cr√©ation)
        paths = setup_project_paths(env, create_structure=False)
        print(f"  ‚úÖ Chemins configur√©s: {len(paths)} chemins")
        
        # Test validation
        validation = validate_paths(paths)
        print(f"  ‚úÖ Validation chemins: {len(validation)} chemins valid√©s")
        
        # Test chemins fichiers
        data_files = get_data_files_paths(paths)
        print(f"  ‚úÖ Fichiers de donn√©es: {len(data_files)} fichiers")
        
        return True, paths
        
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        traceback.print_exc()
        return False, None

def test_display_config():
    """Test du module display_config"""
    print("\nüß™ Test: modules.config.display_config")
    print("-" * 40)
    
    try:
        from display_config import (
            setup_display_config,
            get_color_palette,
            create_custom_style
        )
        
        # Test configuration affichage
        display_config = setup_display_config()
        print(f"  ‚úÖ Configuration affichage: {len(display_config)} modules")
        
        # Test palette couleurs
        colors = get_color_palette("project", 5)
        print(f"  ‚úÖ Palette couleurs: {len(colors)} couleurs")
        
        # Test style personnalis√©
        style = create_custom_style()
        print(f"  ‚úÖ Style personnalis√©: {len(style)} param√®tres")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        traceback.print_exc()
        return False

def test_project_config():
    """Test du module project_config"""
    print("\nüß™ Test: modules.config.project_config")
    print("-" * 40)
    
    try:
        from project_config import (
            ProjectConfig,
            create_config
        )
        
        # Test cr√©ation config
        test_paths = {
            'ROOT_DIR': Path('.'),
            'DATA_PROCESSED': Path('./data/processed'),
            'FIGURES_DIR': Path('./figures')
        }
        
        config = create_config(
            project_name="Test STA211",
            version="1.0", 
            author="Test",
            paths=test_paths
        )
        print(f"  ‚úÖ Configuration cr√©√©e: {type(config).__name__}")
        
        # Test acc√®s valeurs
        test_size = config.get('PROJECT_CONFIG.TEST_SIZE')
        print(f"  ‚úÖ Acc√®s valeur: TEST_SIZE = {test_size}")
        
        # Test mise √† jour
        config.update('PROJECT_CONFIG.SCORING', 'f1')
        scoring = config.get('PROJECT_CONFIG.SCORING')
        print(f"  ‚úÖ Mise √† jour: SCORING = {scoring}")
        
        return True, config
        
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        traceback.print_exc()
        return False, None

def test_environment_module():
    """Test du module environment"""
    print("\nüß™ Test: modules.config.environment")
    print("-" * 40)
    
    try:
        from environment import (
            detect_environment,
            import_core_libraries,
            import_optional_libraries,
            setup_random_seeds,
            verify_environment
        )
        
        # Test d√©tection environnement
        env = detect_environment()
        print(f"  ‚úÖ Environnement: {env}")
        
        # Test import biblioth√®ques principales
        core_libs = import_core_libraries()
        print(f"  ‚úÖ Biblioth√®ques principales: {len(core_libs)}")
        
        # Test import biblioth√®ques optionnelles
        opt_libs = import_optional_libraries()
        available_count = sum(1 for k, v in opt_libs.items() 
                            if k.endswith('_AVAILABLE') and v)
        print(f"  ‚úÖ Biblioth√®ques optionnelles: {available_count} disponibles")
        
        # Test graines al√©atoires
        seed = setup_random_seeds(42)
        print(f"  ‚úÖ Graines al√©atoires: {seed}")
        
        # Test v√©rification environnement
        env_info = verify_environment()
        print(f"  ‚úÖ Informations syst√®me: {len(env_info)} items")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        traceback.print_exc()
        return False

def test_full_setup():
    """Test de la configuration compl√®te"""
    print("\nüß™ Test: Configuration compl√®te")
    print("-" * 40)
    
    try:
        from environment import setup_environment
        
        project_info = {
            'name': 'Test STA211',
            'author': 'Test User',
            'version': '1.0',
            'target_metric': 'F1-Score'
        }
        
        config, paths, metadata = setup_environment(
            project_info=project_info,
            random_state=42,
            install_packages=False,  # Pas d'installation pour les tests
            quiet=True
        )
        
        print(f"  ‚úÖ Configuration compl√®te: {type(config).__name__}")
        print(f"  ‚úÖ Chemins: {len(paths)} chemins")
        print(f"  ‚úÖ M√©tadonn√©es: {len(metadata)} sections")
        
        # Test validation
        from environment import validate_environment_setup
        is_valid = validate_environment_setup(config, paths, metadata)
        print(f"  ‚úÖ Validation: {'Succ√®s' if is_valid else '√âchec'}")
        
        return True, (config, paths, metadata)
        
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        traceback.print_exc()
        return False, None

def test_config_integration():
    """Test d'int√©gration des modules config"""
    print("\nüß™ Test: Int√©gration modules config")
    print("-" * 40)
    
    try:
        # Import des modules individuels
        from environment import setup_environment, detect_environment
        from project_config import ProjectConfig, create_config
        from paths_config import setup_project_paths
        from display_config import setup_display_config
        
        print(f"  ‚úÖ Import modules: tous les modules disponibles")
        
        # Test workflow complet
        env = detect_environment()
        paths = setup_project_paths(env, create_structure=False)
        display_config = setup_display_config()
        
        project_config = create_config(
            project_name="Integration Test",
            version="1.0",
            author="Test",
            paths=paths
        )
        
        print(f"  ‚úÖ Workflow complet: configuration int√©gr√©e")
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        traceback.print_exc()
        return False

def test_config_persistence():
    """Test de la persistance de configuration"""
    print("\nüß™ Test: Persistance configuration")
    print("-" * 40)
    
    try:
        from project_config import create_config
        import tempfile
        import json
        
        # Cr√©ation config temporaire
        test_paths = {'ROOT_DIR': Path('.')}
        config = create_config("Test", "1.0", "Test", test_paths)
        
        # Test sauvegarde
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            temp_path = Path(f.name)
        
        config.save_config(temp_path)
        print(f"  ‚úÖ Sauvegarde: {temp_path.name}")
        
        # Test chargement
        with open(temp_path, 'r') as f:
            loaded_data = json.load(f)
        
        print(f"  ‚úÖ Chargement: {len(loaded_data)} sections")
        
        # Nettoyage
        temp_path.unlink()
        print(f"  ‚úÖ Nettoyage: fichier temporaire supprim√©")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        traceback.print_exc()
        return False

def run_all_tests():
    """Ex√©cute tous les tests de configuration"""
    print("üî¨ D√©but des tests des modules de configuration")
    print("=" * 60)
    
    results = {}
    test_data = {}
    
    # Tests individuels des modules
    results['paths_config'], test_data['paths'] = test_paths_config()
    results['display_config'] = test_display_config()
    results['project_config'], test_data['config'] = test_project_config()
    results['environment'] = test_environment_module()
    
    # Tests d'int√©gration
    results['full_setup'], test_data['full'] = test_full_setup()
    results['integration'] = test_config_integration()
    results['persistence'] = test_config_persistence()
    
    # R√©sum√© des r√©sultats
    print("\nüìä R√©sum√© des tests")
    print("=" * 30)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"  {status} {test_name.replace('_', ' ').title()}")
        if result:
            passed += 1
    
    success_rate = (passed / total) * 100
    print(f"\nüéØ R√©sultat global: {passed}/{total} tests r√©ussis ({success_rate:.1f}%)")
    
    if success_rate == 100:
        print("üéâ Tous les tests sont pass√©s - Modules config pr√™ts !")
        status = "SUCCESS"
    elif success_rate >= 80:
        print("‚ö†Ô∏è  La plupart des tests passent - Configuration utilisable")
        status = "WARNING"
    else:
        print("‚ùå Plusieurs tests √©chouent - V√©rifiez la configuration")
        status = "ERROR"
    
    return status, results, test_data

def main():
    """Fonction principale pour ex√©cuter les tests"""
    try:
        status, results, test_data = run_all_tests()
        
        # Code de sortie
        exit_codes = {
            "SUCCESS": 0,
            "WARNING": 1, 
            "ERROR": 2
        }
        
        return exit_codes.get(status, 2)
        
    except Exception as e:
        print(f"\nüí• Erreur critique lors des tests: {e}")
        traceback.print_exc()
        return 3

if __name__ == "__main__":
    print("üöÄ Script de test des modules de configuration STA211")
    print("=" * 60)
    
    # V√©rification du r√©pertoire courant
    current_dir = Path.cwd()
    print(f"üìÅ R√©pertoire courant: {current_dir}")
    
    # V√©rification que nous sommes dans le bon dossier
    if current_dir.name != "config":
        print("‚ö†Ô∏è  Attention: vous n'√™tes pas dans le dossier config/")
        print("üìç Changez vers le dossier modules/config/ avant d'ex√©cuter ce script")
    
    # Ex√©cution des tests
    exit_code = main()
    
    print(f"\nüèÅ Tests termin√©s (code de sortie: {exit_code})")
    sys.exit(exit_code)