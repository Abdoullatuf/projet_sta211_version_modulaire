


## Définition des métadonnées du projet

import json
from datetime import datetime
from pathlib import Path

# ============================================================================
# MÉTADONNÉES DU PROJET STA211
# ============================================================================

# Métadonnées principales
PROJECT_NAME = "STA211: Internet Advertisements Classification"
DATASET_NAME = "Internet Advertisements Dataset"
AUTHOR = "Abdoullatuf"
DATE = datetime.now().strftime("%Y-%m-%d %H:%M")
VERSION = "1.1"

# Métadonnées techniques du projet
PROJECT_METADATA = {
    "project_info": {
        "name": PROJECT_NAME,
        "dataset": DATASET_NAME,
        "author": AUTHOR,
        "date": DATE,
        "version": VERSION,
        "course": "STA211 - CNAM Master 2 Science des données"
    },
    "dataset_specs": {
        "train_samples": 2459,
        "test_samples": 820,
        "total_features": 1558,
        "continuous_features": 3,
        "binary_features": 1555,
        "target_variable": "y (ad./noad.)",
        "class_imbalance": "13.99% ad. vs 86.01% noad."
    },
    "objectives": [
        "Optimisation du F1-score",
        "Gestion du déséquilibre des classes",
        "Imputation robuste (MICE/KNN)",
        "Réduction de la colinéarité",
        "Pipeline modulaire et reproductible"
    ]
}

# Validation et affichage
def validate_metadata(metadata_dict):
    """Valide la structure des métadonnées."""
    required_sections = ["project_info", "dataset_specs", "objectives"]

    for section in required_sections:
        if section not in metadata_dict:
            raise KeyError(f"Section manquante dans les métadonnées : {section}")

    return True

# Validation
validate_metadata(PROJECT_METADATA)

# Affichage structuré
print("🎯 " + "="*60)
print(f"📊 {PROJECT_NAME}")
print("🎯 " + "="*60)

# Informations projet
project_info = PROJECT_METADATA["project_info"]
print(f"👤 Auteur      : {project_info['author']}")
print(f"📅 Date       : {project_info['date']}")
print(f"🔢 Version    : {project_info['version']}")
print(f"🎓 Cours      : {project_info['course']}")
print(f"📁 Dataset    : {project_info['dataset']}")

# Spécifications dataset
print(f"\n📊 Caractéristiques des données :")
dataset_specs = PROJECT_METADATA["dataset_specs"]
for key, value in dataset_specs.items():
    formatted_key = key.replace('_', ' ').title()
    print(f"   • {formatted_key}: {value}")

# Objectifs
print(f"\n🎯 Objectifs techniques :")
for i, objective in enumerate(PROJECT_METADATA["objectives"], 1):
    print(f"   {i}. {objective}")












#execute cette cellule sur colab
#!pip install numpy==1.26.4 pandas==2.2.2 --force-reinstall


## 2.1 Paramètres et Imports


import sys, os, logging
from pathlib import Path

# ── 0. Logger clair (avec Rich si dispo)
try:
    from rich.logging import RichHandler
    logging.basicConfig(level="INFO",
                        format="%(message)s",
                        handlers=[RichHandler(rich_tracebacks=True, markup=True)],
                        force=True)
except ModuleNotFoundError:
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s - %(levelname)s - %(message)s",
                        stream=sys.stdout,
                        force=True)
logger = logging.getLogger(__name__)

# ── 1. Détection environnement Colab
def _in_colab() -> bool:
    try: import google.colab
    except ImportError: return False
    else: return True

# ── 2. Montage Drive manuel rapide
if _in_colab():
    from google.colab import drive
    if not Path("/content/drive/MyDrive").exists():
        logger.info("🔗 Montage de Google Drive en cours…")
        drive.mount("/content/drive", force_remount=False)

# ── 3. Localisation racine projet STA211
def find_project_root() -> Path:
    env_path = os.getenv("STA211_PROJECT_PATH")
    if env_path and (Path(env_path) / "modules").exists():
        return Path(env_path).expanduser().resolve()

    default_colab = Path("/content/drive/MyDrive/projet_sta211")
    if _in_colab() and (default_colab / "modules").exists():
        return default_colab.resolve()

    cwd = Path.cwd()
    for p in [cwd, *cwd.parents]:
        if (p / "modules").exists():
            return p.resolve()

    raise FileNotFoundError("❌ Impossible de localiser un dossier contenant 'modules/'.")

# ── 4. Définition racine + PYTHONPATH
ROOT_DIR = find_project_root()
os.environ["STA211_PROJECT_PATH"] = str(ROOT_DIR)
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))
logger.info(f"📂 Racine projet détectée : {ROOT_DIR}")
logger.info(f"PYTHONPATH ← {ROOT_DIR}")

# ── 5. Initialisation des chemins et configuration projet
from modules.config import init_project, set_display_options

init_result = init_project()
paths = init_result["paths"]
set_display_options()

# ── 6. Affichage des chemins utiles
def display_paths(style: bool = True):
    import pandas as pd
    rows = [{"Clé": k, "Chemin": os.fspath(v)} for k, v in paths.items() if "DIR" in k]
    df = pd.DataFrame(rows).set_index("Clé")
    from IPython.display import display
    display(df.style.set_table_styles([
        {"selector": "th", "props": [("text-align", "left")]},
        {"selector": "td", "props": [("text-align", "left")]},
    ]) if style else df)



# Ajout de toutes les variables nécessaires y compris RAW_DATA_DIR


# Variables principales de données
RAW_DATA_DIR          = paths["RAW_DATA_DIR"]
DATA_PROCESSED        = paths["DATA_PROCESSED"]
MODELS_DIR            = paths["MODELS_DIR"]
FIGURES_DIR           = paths["FIGURES_DIR"]
THRESHOLDS_DIR        = paths["THRESHOLDS_DIR"]
OUTPUTS_DIR           = paths["OUTPUTS_DIR"]

display_paths()






## 2.2 Chargement des modules et chemins
# ───────────────────────────────────────────────────────────────────────────
from IPython.display import Markdown, display
import logging

# Initialisation du logger si nécessaire
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ⬇️ Importation centralisée après installation des dépendances
try:
    from imports_sta211 import *
    logger.info("📚 Bibliothèques importées depuis imports_sta211")
except ModuleNotFoundError as e:
    logger.error(f"❌ Erreur d'importation : {e}. Exécutez d'abord init_project() pour installer les dépendances.")
    raise

# ───────────────────────────────────────────────────────────────────────────
# ✅ Affichage des versions principales
# ───────────────────────────────────────────────────────────────────────────
def _safe_version(mod, fallback="—"):
    """Retourne mod.__version__ ou un fallback si le module est absent."""
    try:
        return mod.__version__
    except Exception:
        return fallback

def display_modeling_library_versions():
    mods = {
        "pandas"           : pd,
        "numpy"            : np,
        "scikit-learn"     : sklearn,
        "imbalanced-learn" : imblearn,
        "xgboost"          : xgb,
        "matplotlib"       : matplotlib,
        "seaborn"          : sns,
        "scipy"            : scipy,
        "joblib"           : joblib,
        "tqdm"             : __import__("tqdm"),
        "ipython"          : __import__("IPython"),
        #"catboost"         : __import__("catboost")
    }
    versions_md = "\n".join(f"- `{k}` : {_safe_version(v)}" for k, v in mods.items())
    display(Markdown(f"### ✅ Versions des bibliothèques de modélisation\n{versions_md}"))

display_modeling_library_versions()
logger.info("✅ Chargement des bibliothèques terminé")









## 3.1 Chargement des jeux de données bruts

# ============================================================================
# CHARGEMENT ROBUSTE DES DONNÉES STA211
# ============================================================================

print("📂 Chargement des jeux de données STA211...")

# Validation de l'environnement
if 'RAW_DATA_DIR' not in globals():
    raise NameError("❌ RAW_DATA_DIR non défini. Vérifiez la section 2.1.")

# Import du module de chargement
try:
    from preprocessing.data_loader import load_data
    print("✅ Module data_loader importé")
except ImportError as e:
    print(f"❌ Erreur import : {e}")
    raise

# ============================================================================
# CHARGEMENT DES DATASETS AVEC VALIDATION
# ============================================================================

# Dimensions attendues
expected_train_shape = (2459, 1559)
expected_test_shape = (820, 1558)

print(f"📊 Dimensions attendues - Train: {expected_train_shape}, Test: {expected_test_shape}")

# Chargement dataset d'entraînement
print("\n📥 Chargement du dataset d'entraînement...")
df_study = load_data(
    file_path="data_train.csv",
    require_outcome=True,
    display_info=True,
    raw_data_dir=RAW_DATA_DIR,
    encode_target=True
)

# Chargement dataset de test
print("\n📥 Chargement du dataset de test...")
df_eval = load_data(
    file_path="data_test.csv",
    require_outcome=False,
    display_info=True,
    raw_data_dir=RAW_DATA_DIR,
    encode_target=False
)

# ============================================================================
# STANDARDISATION ET VALIDATION
# ============================================================================

# Renommage standardisé de la variable cible
if 'outcome' in df_study.columns:
    df_study = df_study.rename(columns={'outcome': 'y'})
    print("✅ Colonne 'outcome' renommée en 'y'")
elif 'y' not in df_study.columns:
    raise ValueError("❌ Variable cible manquante dans df_study")

# Validation des dimensions
def validate_dimensions(df, expected_shape, name):
    if df.shape != expected_shape:
        print(f"⚠️ {name}: {df.shape} (attendu: {expected_shape})")
        return False
    else:
        print(f"✅ {name}: {df.shape} ✓")
        return True

dimensions_ok = validate_dimensions(df_study, expected_train_shape, "df_study")
dimensions_ok &= validate_dimensions(df_eval, expected_test_shape, "df_eval")

# Validation de la variable cible
print(f"\n🎯 Variable cible 'y': {df_study['y'].dtype}, valeurs: {sorted(df_study['y'].unique())}")

# ============================================================================
# RÉSUMÉ FINAL
# ============================================================================

print(f"\n📋 RÉSUMÉ DU CHARGEMENT:")
print(f"  ✅ Dataset d'entraînement: {df_study.shape}")
print(f"  ✅ Dataset de test: {df_eval.shape}")
print(f"  ✅ Variable cible: encodée 0/1")
print(f"  ✅ Dimensions: {'✓' if dimensions_ok else '⚠️'}")

# Contrôle qualité rapide
missing_study = df_study.isnull().sum().sum()
missing_eval = df_eval.isnull().sum().sum()
print(f"  📊 Valeurs manquantes: {missing_study} (train) + {missing_eval} (test)")






## 3.3 Distribution de la variable cible
# ============================================================================
# INSPECTION AUTOMATISÉE DES COLONNES
# ============================================================================

print("🔍 Inspection automatisée des colonnes et types")
print("=" * 55)

# Import du module d'inspection
try:
    from modules.preprocessing.column_inspector import (
        inspect_datasets,
        update_column_config,
        print_inspection_summary
    )
    print("✅ Module column_inspector importé")
except ImportError as e:
    print(f"❌ Erreur import : {e}")
    raise

# ============================================================================
# INSPECTION COMPLÈTE DES DATASETS
# ============================================================================

# Récupération du nombre de features attendu
try:
    if 'init_result' in globals():
        print(f"📋 init_result disponible avec les clés: {list(init_result.keys())}")
        expected_features = 1558  # Valeur par défaut
    else:
        expected_features = 1558
        print("⚠️ init_result non disponible, utilisation de la valeur par défaut")
except NameError:
    expected_features = 1558
    print("⚠️ Utilisation de la valeur par défaut pour expected_features")

print(f"Nombre de features attendu: {expected_features}")

# Sélection des datasets
if 'df_study' in globals() and 'df_eval' in globals():
    df_train_inspect = df_study
    df_test_inspect = df_eval
    print(f"Datasets utilisés: df_study (train) et df_eval (test)")
else:
    print("❌ Datasets df_study et df_eval non disponibles")
    raise NameError("Datasets train/test non disponibles")

# Inspection automatisée
try:
    inspection_results = inspect_datasets(
        df_train=df_train_inspect,
        df_test=df_test_inspect,
        target_col='y',
        verbose=True
    )
    print("✅ Inspection des datasets réalisée")
except Exception as e:
    print(f"❌ Erreur lors de l'inspection: {e}")
    raise

# ============================================================================
# EXPORT DES VARIABLES
# ============================================================================

train_structure = inspection_results['train_structure']
continuous_cols = train_structure['continuous']
binary_cols = train_structure['binary']
categorical_cols = train_structure['categorical']

print(f"\n🌍 Variables exportées :")
print(f"  ✅ continuous_cols  : {len(continuous_cols)} variables")
print(f"  ✅ binary_cols      : {len(binary_cols)} variables")
print(f"  ✅ categorical_cols : {len(categorical_cols)} variables")

# Variables globales
globals()['continuous_cols'] = continuous_cols
globals()['binary_cols'] = binary_cols
globals()['categorical_cols'] = categorical_cols
globals()['train_structure'] = train_structure
globals()['inspection_results'] = inspection_results

print_inspection_summary(inspection_results, expected_features)





## 3.3 Distribution de la variable cible

# ============================================================================
# ANALYSE AUTOMATISÉE DE LA VARIABLE CIBLE VIA MODULE
# ============================================================================

print("🎯 Analyse automatisée de la variable cible")
print("=" * 50)

# Import du module d'analyse cible
try:
    from exploration.target_analyzer import (
        analyze_target_complete,
        update_config_with_target_stats
    )
    print("✅ Module target_analyzer importé")
except ImportError as e:
    print(f"❌ Erreur import : {e}")
    raise

# Validation de la variable cible
if 'y' not in df_study.columns:
    raise ValueError("❌ Colonne cible 'y' introuvable dans df_study")

# ============================================================================
# ANALYSE COMPLÈTE AUTOMATISÉE
# ============================================================================

# Analyse complète avec visualisations et recommandations
target_stats = analyze_target_complete(
    df=df_study,
    target_col='y',
    figures_dir=FIGURES_DIR / "figures_notebook1",
    verbose=True
)


# ============================================================================
# EXPORT DES VARIABLES POUR LA SUITE
# ============================================================================

# Variables importantes pour les sections suivantes
imbalance_ratio = target_stats['imbalance_ratio']
baseline_f1 = target_stats['baseline_f1']
target_distribution = target_stats['counts']
minority_proportion = target_stats['minority_proportion']

print(f"\n🌍 Variables exportées pour la suite :")
print(f"  ✅ imbalance_ratio      : {imbalance_ratio:.2f}")
print(f"  ✅ baseline_f1          : {baseline_f1:.3f}")
print(f"  ✅ target_distribution  : {target_distribution}")
print(f"  ✅ minority_proportion  : {minority_proportion:.3f}")

# Validation pour la suite
if imbalance_ratio > 3:
    print(f"\n⚠️ Dataset fortement déséquilibré - Stratégies spéciales requises")
    print(f"💡 Utilisez class_weight='balanced' et optimisez pour F1-score")

print(f"\n✅ Analyse de la variable cible terminée")
print(f"🚀 Prêt pour l'analyse exploratoire approfondie (section 4)")









## 4.1 Analyse des valeurs manquantes

print("🔍 Analyse des valeurs manquantes")
print("="*60)

# Utilisation de la fonction du module
from preprocessing.missing_values import (
    analyze_missing_values,
    handle_missing_values,
    find_optimal_k
)



print("\n📊 Analyse globale des valeurs manquantes :")
missing_stats = analyze_missing_values(df_study)

# Analyse détaillée pour les colonnes continues
print("\n📈 Détail des valeurs manquantes pour les variables continues :")
for col in continuous_cols:
    missing_count = df_study[col].isnull().sum()
    missing_pct = (missing_count / len(df_study)) * 100
    print(f"  - {col}: {missing_count} ({missing_pct:.2f}%)")

# Visualisation des patterns de valeurs manquantes
if missing_stats['total_missing'] > 0:
    # Heatmap des valeurs manquantes pour les colonnes avec des NaN
    cols_with_missing = [col for col in df_study.columns if df_study[col].isnull().sum() > 0]

    if len(cols_with_missing) > 0:
        plt.figure(figsize=(10, 5))

        # Créer une matrice binaire des valeurs manquantes
        missing_matrix = df_study[cols_with_missing].isnull().astype(int)

        # Heatmap
        sns.heatmap(missing_matrix.T, cmap='RdYlBu', cbar_kws={'label': 'Manquant (1) / Présent (0)'})
        plt.title('Pattern des valeurs manquantes', fontsize=14)
        plt.xlabel('Échantillons')
        plt.ylabel('Variables')
        plt.tight_layout()
        plt.savefig(FIGURES_DIR / 'figures_notebook1' / 'eda' / 'missing_values_pattern.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Analyse du pattern MAR vs MCAR
        print("\n🔍 Analyse du type de valeurs manquantes (MAR vs MCAR) :")

        # Corrélation entre les valeurs manquantes et la cible
        for col in cols_with_missing:
            missing_indicator = df_study[col].isnull().astype(int)
            correlation_with_target = missing_indicator.corr(df_study['y'])
            print(f"  - {col}: corrélation avec y = {correlation_with_target:.3f}")

            if abs(correlation_with_target) > 0.1:
                print(f"    → Potentiellement MAR (Missing At Random)")
            else:
                print(f"    → Potentiellement MCAR (Missing Completely At Random)")
else:
    print("\n✅ Aucune valeur manquante détectée dans le dataset !")

# Analyse pour le fichier d'évaluation aussi
print("\n📊 Analyse des valeurs manquantes dans le fichier d'évaluation :")
missing_stats_eval = analyze_missing_values(df_eval)

# Comparaison des patterns
if missing_stats['total_missing'] > 0 or missing_stats_eval['total_missing'] > 0:
    print("\n🔄 Comparaison des patterns de valeurs manquantes :")
    print(f"  - Fichier d'étude : {missing_stats['percent_missing']:.2f}% manquant")
    print(f"  - Fichier d'évaluation : {missing_stats_eval['percent_missing']:.2f}% manquant")

    # Stratégie d'imputation recommandée
    print("\n💡 Stratégie d'imputation recommandée :")
    if 'X4' in missing_stats['cols_missing']:
        x4_missing_pct = missing_stats['percent_per_col'].get('X4', 0)
        if x4_missing_pct < 5:
            print(f"  - X4 ({x4_missing_pct:.1f}% manquant) : Imputation par la médiane")

    mar_cols = ['X1', 'X2', 'X3']
    mar_missing = any(col in missing_stats['cols_missing'] for col in mar_cols)
    if mar_missing:
        print(f"  - X1, X2, X3 (variables continues) : KNN ou MICE (imputation multivariée)")





# Correction du type et imputation de X4
print("\n🔧 Correction du type de X4...")
print(f"Valeurs uniques de X4 (avant correction) : {sorted(df_study['X4'].dropna().unique())}")
print(f"Type actuel : {df_study['X4'].dtype}")

# Vérifier que X4 ne contient que 0 et 1
unique_values = df_study['X4'].dropna().unique()
if set(unique_values).issubset({0.0, 1.0}):
    # Imputer d'abord les valeurs manquantes par la médiane
    X4_median = df_study['X4'].median()
    df_study['X4'] = df_study['X4'].fillna(X4_median)
    df_eval['X4'] = df_eval['X4'].fillna(X4_median)

    # Convertir en int
    df_study['X4'] = df_study['X4'].astype(int)
    df_eval['X4'] = df_eval['X4'].astype(int)

    print(f"✅ X4 converti en int64 après imputation par la médiane ({X4_median})")
    print(f"Nouveau type : {df_study['X4'].dtype}")


else:
    print("⚠️ X4 contient des valeurs autres que 0 et 1, conservation en float64")

# Résumé final
print("\n📊 Résumé des valeurs manquantes après traitement de X4 :")
print(f"  - X1, X2, X3 : ~27% manquant → À traiter avec KNN/MICE")
print(f"  - X4 : Imputé et converti en binaire")
print(f"  - Pattern MAR détecté pour X1, X2, X3 (corrélation avec y ≈ -0.10)")
print(f"  - Les patterns sont cohérents entre fichiers d'étude et d'évaluation")






## 4.2 Analyse statistique des variables quantitatives

print("📊 Analyse statistique des variables quantitatives")
print("="*60)

from exploration.statistics import analyze_continuous_variables
#from modules.exploration.statistics import analyze_continuous_variables


# Lancement de l’analyse complète
results_stats = analyze_continuous_variables(
    df=df_study,
    continuous_cols=continuous_cols,
    target_col='y',
    save_figures_path=str(FIGURES_DIR / "figures_notebook1" / "eda")
)







## 4.3 Visualisation des distributions et des boxplots

print("📊 Visualisation des distributions et des boxplots")
print("="*60)

from exploration.visualization import visualize_distributions_and_boxplots
# Appel de la fonction
visualize_distributions_and_boxplots(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / "figures_notebook1" / "eda"
)








## 4.4 Distribution des variables binaires <a id="distribution-des-variables-binaires"></a>

print("🔢 Analyse de la distribution des variables binaires")
print("="*60)

from exploration.visualization import save_fig

# Variables binaires (exclut les variables continues)
binary_cols = [col for col in df_study.columns if col.startswith('X') and col not in continuous_cols]
print(f"\n📊 Nombre total de variables binaires : {len(binary_cols)}")

# Taux de présence (valeurs à 1)
presence_rates = {
    col: (df_study[col] == 1).sum() / len(df_study) * 100 for col in binary_cols
}
presence_series = pd.Series(presence_rates)

# Statistiques globales
print(f"\n📊 Statistiques des taux de présence :")
print(f"  - Moyenne : {presence_series.mean():.2f}%")
print(f"  - Médiane : {presence_series.median():.2f}%")
print(f"  - Min : {presence_series.min():.2f}%")
print(f"  - Max : {presence_series.max():.2f}%")

# Sparsité globale
total_values = len(df_study) * len(binary_cols)
total_ones = df_study[binary_cols].sum().sum()
sparsity = (1 - total_ones / total_values) * 100
print(f"\n📊 Sparsité globale : {sparsity:.2f}% de zéros")

# Visualisation
plt.figure(figsize=(8, 4))
presence_series.hist(bins=50, color='skyblue', edgecolor='black')
plt.axvline(presence_series.mean(), color='red', linestyle='--', label=f'Moyenne: {presence_series.mean():.1f}%')
plt.xlabel('Taux de présence (%)')
plt.ylabel('Nombre de variables')
plt.title('Distribution des taux de présence des variables binaires')
plt.legend()
plt.tight_layout()

save_fig("binary_presence_distribution.png", directory=FIGURES_DIR / "figures_notebook1"/ "eda", dpi=300, show=True)

print("\n✅ Analyse des variables binaires terminée")
print("   → Dataset très sparse, adapté pour des méthodes de sélection de features")






## 4.5 Analyse des corrélations combinées <a id="analyse-correlations-combinees"></a>

print("🔗 Lancement de l'analyse combinée des corrélations (features ↔ cible, features ↔ features)...")
print("=" * 80)

from exploration.eda_analysis import full_correlation_analysis

# Appel avec paramètres personnalisés
full_correlation_analysis(
    df_study=df_study,
    continuous_cols=continuous_cols,
    presence_rates=presence_rates,
    FIGURES_DIR=FIGURES_DIR / "figures_notebook1",
    ROOT_DIR=ROOT_DIR,
    figsize_corr_matrix=(7, 5),
    figsize_binary=(8, 4)
)








## 4.6 Visualisations globales de l'EDA

print("📊 Visualisations exploratoires")
print("=" * 60)

# Imports des fonctions refactorisées
from exploration.visualization import (
    plot_continuous_by_class,
    plot_binary_sparsity,
    plot_continuous_target_corr
)
from exploration.statistics import optimized_feature_importance

# 1. Distribution des variables continues par classe
print("\n📈 Distribution des variables continues par classe...")
plot_continuous_by_class(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / 'figures_notebook1' / 'eda'
)

# 2. Visualisation de la sparsité
print("\n📉 Visualisation de la sparsité des données binaires...")
plot_binary_sparsity(
    df=df_study,
    binary_cols=binary_cols,
    output_dir=FIGURES_DIR / 'figures_notebook1' / 'eda'
)







# 3. Corrélations des variables continues avec la cible
print("\n🔗 Corrélations des variables continues avec la cible...")
plot_continuous_target_corr(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / 'notebook1' / 'eda'
)







import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

print("\n📊 Visualisation multidimensionnelle (PCA / t-SNE / UMAP)...")

df_study_viz = df_study.copy()
df_study_viz['outcome'] = df_study_viz['y'].map({0: 'noad.', 1: 'ad.'})  # ✅ temporaire

target_corr = df_study[continuous_cols + ['y']].corr()['y'].drop('y')
important_features = continuous_cols + list(target_corr.abs().nlargest(30).index)
df_sample = df_study_viz[important_features + ['outcome']].dropna()

try:
    import umap
    umap_available = True
except ImportError:
    umap_available = False
    print("UMAP n'est pas installé. Pour l'utiliser : pip install umap-learn")

X = df_sample[important_features]
y = df_sample['outcome']

# Calcul des projections
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X)

tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
X_tsne = tsne.fit_transform(X)

if umap_available:
    reducer = umap.UMAP(n_components=2, random_state=42)
    X_umap = reducer.fit_transform(X)
else:
    X_umap = None

# Affichage côte à côte
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# PCA
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y, palette='Set1', alpha=0.7, ax=axes[0])
axes[0].set_title("Projection PCA (2D)")
axes[0].set_xlabel("PC1")
axes[0].set_ylabel("PC2")
axes[0].legend(title="Outcome")

# t-SNE
sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette='Set1', alpha=0.7, ax=axes[1])
axes[1].set_title("Projection t-SNE (2D)")
axes[1].set_xlabel("t-SNE 1")
axes[1].set_ylabel("t-SNE 2")
axes[1].legend(title="Outcome")

# UMAP (si disponible)
if X_umap is not None:
    sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=y, palette='Set1', alpha=0.7, ax=axes[2])
    axes[2].set_title("Projection UMAP (2D)")
    axes[2].set_xlabel("UMAP 1")
    axes[2].set_ylabel("UMAP 2")
    axes[2].legend(title="Outcome")
else:
    axes[2].set_visible(False)
    axes[2].set_title("UMAP non disponible")

plt.tight_layout()

plt.savefig(FIGURES_DIR / 'figures_notebook1' / 'eda'/ "projection_multidim.png", dpi=150, bbox_inches='tight')
plt.show()





from exploration.visualization import plot_eda_summary

# 5. Importance des variables
print("\n🌲 Analyse de l’importance des features...")
try:
    df_importance = df_sample.copy()  # contient outcome déjà transformée
    importance_results = optimized_feature_importance(
        df=df_importance,
        target_col='outcome',
        method='all',
        top_n=10,
        figsize=(8, 4),
        save_path=FIGURES_DIR / 'figures_notebook1' / 'eda' / 'feature_importance.png',
        show=True
    )
    if not importance_results.empty:
        print("\nTop 10 features les plus importantes :")
        print(importance_results[['feature', 'Combined_Score']].head(10))
except Exception as e:
    print(f"⚠️ Erreur lors de l’analyse d’importance des features : {e}")

# 6. Résumé visuel global
print("\n📊 Création du résumé visuel de l’EDA...")
plot_eda_summary(
    df=df_study,
    continuous_cols=continuous_cols,
    binary_cols=binary_cols,
    target_corr=target_corr,
    sparsity=sparsity,
    imbalance_ratio=imbalance_ratio,
    output_dir=FIGURES_DIR / 'figures_notebook1' / 'eda',
    presence_series=presence_series
)

print("\n✅ Visualisations exploratoires terminées")















# 📊 SECTION 5.1 - GESTION DES VALEURS MANQUANTES (code)

# ═══════════════════════════════════════════════════════════════════════════════
# 🔍 5.1.1 ÉTAT DES VARIABLES APRÈS TRANSFORMATIONS
# ═══════════════════════════════════════════════════════════════════════════════

print("📊 GESTION DES VALEURS MANQUANTES")
print("=" * 60)
print("🎯 Analyse sur les variables continues")
print("📋 Variables cibles : X1, X2, X3")
print()

import pandas as pd
import numpy as np
from preprocessing.missing_values import analyze_missing_values

# ✅ Variables transformées à analyser (nomenclature corrigée)
cols_to_check = ["X1", "X2", "X3"]

# 📊 Vérification de la présence des variables
print("🔍 VÉRIFICATION DES VARIABLES")
print("-" * 50)

variables_disponibles = []
for col in cols_to_check:
    if col in df_study.columns:
        variables_disponibles.append(col)
        print(f"✅ {col} : Disponible")
    else:
        print(f"❌ {col} : Non trouvée")
        # Recherche de variations possibles
        alternatives = [c for c in df_study.columns if col.split('_')[0] in c]
        if alternatives:
            print(f"   🔍 Alternatives possibles : {alternatives[:3]}")

if not variables_disponibles:
    print("⚠️ Aucune variable transformée trouvée avec la nomenclature attendue")
    print("📋 Variables disponibles dans le dataset :")
    transformed_vars = [col for col in df_study.columns if 'transform' in col.lower()]
    for var in transformed_vars[:10]:
        print(f"   - {var}")
else:
    cols_to_check = variables_disponibles

print()








### 5.3.4 Imputation multivariée (MICE)  (code)

# Variables de référence
cols_to_impute = ["X1", "X2", "X3"]

print("🧩 Imputation multivariée")
print("=" * 60)

from preprocessing.missing_values import handle_missing_values
from sklearn.ensemble import RandomForestRegressor

# ✅ Définition de l'estimateur RandomForest
rf_estimator = RandomForestRegressor(
    n_estimators=400,
    max_depth=20,
    min_samples_leaf=2,
    max_features=0.5,  # ou 'sqrt'
    random_state=42,
    n_jobs=-1
)

# 📁 Chemins de sauvegarde
processed_data_dir = paths["DATA_PROCESSED"] / "notebook1"
imputers_dir = paths["MODELS_DIR"] / "notebook1"

# ✅ Lancer l'imputation multiple avec RandomForest
df_imputed_mice = handle_missing_values(
    df=df_study,
    strategy="mixed_mar_mcar",
    mar_method="mice",
    mar_cols=cols_to_impute,
    processed_data_dir=processed_data_dir,  # Répertoire uniquement
    imputers_dir=imputers_dir,
    custom_filename="df_mice_imputed.csv",
    mice_estimator=rf_estimator,
    display_info=True
)

# 🔍 Aperçu
df_imputed_mice[cols_to_impute].describe()



import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 1 ligne, 3 colonnes

variables = ['X1', 'X2', 'X3']

for idx, var in enumerate(variables):
    axes[idx].hist(df_study[var].dropna(), alpha=0.5, label='Avant', bins=30)
    axes[idx].hist(df_imputed_mice[var].dropna(), alpha=0.5, label='Après', bins=30)
    axes[idx].set_title(f'Distribution {var}')
    axes[idx].legend()


fig.suptitle("Comparaison des distribution après imputataion par MICE")
plt.tight_layout()
# Ajustons le rectangle de la mise en page pour faire de la place au suptitle
#plt.subplots_adjust(top=0.9)
fig.savefig( FIGURES_DIR / "figures_notebook1"/ "pretraitement_avance" / "distribution_imputation.png", dpi=150)

plt.show()







### 5.3.4 Imputation par KNN <a id="imputation-knn"></a>

from pathlib import Path
#from preprocessing.missing_values import handle_missing_values, find_optimal_k_v2

# 📁 Chemins de sauvegarde
processed_data_dir_knn = paths["DATA_PROCESSED"] / "notebook1"
imputers_dir_knn = paths["MODELS_DIR"] / "notebook1"

# 🔍 Recherche du k optimal pour KNN (avec outliers)
print("🔍 Recherche du k optimal pour KNN Imputer (avec outliers)")
features = ['X1', 'X2', 'X3']
df_sample = df_study[features].copy()

from modules.preprocessing.find_optimal_k import find_optimal_k_v2

results_knn = find_optimal_k_v2(
    df=df_study,
    columns_to_impute=features,
    k_range=range(3, 30, 2),
    cv_folds=20,
    sample_size=3000,
    verbose=True,
    random_state=42
)

optimal_k = results_knn['optimal_k']
print(f"\n✅ Le K optimal final est : {optimal_k}")

# ✅ Imputation sur les données avec KNN
df_imputed_knn = handle_missing_values(
    df=df_study.copy(deep=True),
    strategy="mixed_mar_mcar",
    mar_method='knn',
    knn_k=optimal_k,
    mar_cols=features,
    mcar_cols=[],
    processed_data_dir=processed_data_dir_knn,
    imputers_dir=imputers_dir_knn,  # ✅ Nouveau paramètre
    save_results=True,
    display_info=True
)



import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 1 ligne, 3 colonnes

variables = ['X1', 'X2', 'X3']

for idx, var in enumerate(variables):
    axes[idx].hist(df_study[var].dropna(), alpha=0.5, label='Avant', bins=30)
    axes[idx].hist(df_imputed_knn[var], alpha=0.5, label='Après', bins=30)
    axes[idx].set_title(f'Distribution {var}')
    axes[idx].legend()

fig.suptitle("Comparaison des distribution après imputataion par KNN")
plt.tight_layout()

fig.savefig( FIGURES_DIR / "figures_notebook1"/ "pretraitement_avance" / "distribution_imputation_knn.png", dpi=150)

plt.show()





# Vérification rapide
print(f"📊 Valeurs manquantes après imputation knn : {df_imputed_knn.isnull().sum().sum()} total")
print(f"📊 Valeurs manquantes après imputation mice: {df_imputed_mice.isnull().sum().sum()} total")





### 5.2.1 Transformation sur les données imputées par MICE (code)

# Import du module
from modules.preprocessing.transformation_optimale_mixte import appliquer_transformation_optimale

# Application de la transformation optimale (Yeo-Johnson pour X1,X2 + Box-Cox pour X3)
df_imputed_mice_transformed = appliquer_transformation_optimale(df_imputed_mice)

# 🎯 C'est tout ! Vos nouvelles variables optimales sont prêtes :
# • X1_transformed (Yeo-Johnson - optimal)
# • X2_transformed (Yeo-Johnson - optimal)
# • X3_transformed (Box-Cox - optimal)

print("\n🚀 VARIABLES POUR LA MODÉLISATION:")
variables_optimales = ['X1_transformed', 'X2_transformed', 'X3_transformed']
print(f"Variables transformées: {variables_optimales}")

# Vérification rapide
print(f"\n📊 VÉRIFICATION:")
print(f"• Données originales: {df_study.shape}")
print(f"• Données transformées: {df_imputed_mice_transformed.shape}")
print(f"• Nouvelles colonnes ajoutées: {len(variables_optimales)}")



from modules.preprocessing.transformation_optimale_mixte import generer_graphiques_comparaison

generer_graphiques_comparaison(df_study, df_imputed_mice_transformed)





### 5.2. Transformation sur les données imputées par KNN (code)

# Import du module
from modules.preprocessing.transformation_optimale_mixte import appliquer_transformation_optimale

# Application de la transformation optimale (Yeo-Johnson pour X1,X2 + Box-Cox pour X3)
df_imputed_knn_transformed = appliquer_transformation_optimale(df_imputed_knn)

# 🎯 C'est tout ! Vos nouvelles variables optimales sont prêtes :
# • X1_transformed (Yeo-Johnson - optimal)
# • X2_transformed (Yeo-Johnson - optimal)
# • X3_transformed (Box-Cox - optimal)

print("\n🚀 VARIABLES POUR LA MODÉLISATION:")
variables_optimales = ['X1_trans', 'X2_trans', 'X3_trans']
print(f"Variables transformées: {variables_optimales}")

# Vérification rapide
print(f"\n📊 VÉRIFICATION:")
print(f"• Données originales: {df_study.shape}")
print(f"• Données transformées: {df_imputed_knn_transformed.shape}")
print(f"• Nouvelles colonnes ajoutées: {len(variables_optimales)}")




# ✅ Liste des variables transformées
transformed_vars = ["X1_transformed", "X2_transformed", "X3_transformed"]

# 📁 Dossier de sauvegarde
output_dir = FIGURES_DIR / "figures_notebook1"/ 'pretraitement_avance'
output_dir.mkdir(parents=True, exist_ok=True)

# 🔁 Génération et sauvegarde des figures
for col in transformed_vars:
    fig, ax = plt.subplots(1, 2, figsize=(6, 2))

    # Histogramme + KDE
    sns.histplot(x=df_imputed_knn_transformed[col], bins=30, kde=True, ax=ax[0], color="mediumseagreen")
    ax[0].set_title(f"{col} - Histogramme")
    ax[0].set_xlabel(col)

    # Boxplot
    sns.boxplot(x=df_imputed_knn_transformed[col], ax=ax[1], color="salmon")
    ax[1].set_title(f"{col} - Boxplot")

    plt.tight_layout()

    # 💾 Sauvegarde
    fig_path = output_dir / f"{col}_distribution_boxplot.png"
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"✅ Figure sauvegardée : {fig_path}")






import numpy as np
import joblib

def apply_capping_and_save(df, cols_to_cap, save_path, display_info=True):
    """
    Applique le capping (Winsorizing) sur les colonnes spécifiées d'un DataFrame
    et sauvegarde les paramètres dans un fichier pickle.

    Args:
        df (pd.DataFrame): DataFrame source.
        cols_to_cap (list): Liste des noms de colonnes à capper.
        save_path (Path): Chemin complet vers le fichier pickle.
        display_info (bool): Active ou désactive l'affichage des logs.

    Returns:
        pd.DataFrame: Nouveau DataFrame avec capping appliqué.
    """
    capped_df = df.copy()
    capping_params = {}

    for col in cols_to_cap:
        Q1 = capped_df[col].quantile(0.25)
        Q3 = capped_df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        capping_params[col] = {'lower_bound': lower_bound, 'upper_bound': upper_bound}

        outliers_low = (capped_df[col] < lower_bound).sum()
        outliers_high = (capped_df[col] > upper_bound).sum()

        capped_df[col] = np.clip(capped_df[col], lower_bound, upper_bound)

        if display_info:
            print(f"\n--- Variable : {col} ---")
            print(f"  Limites IQR : [{lower_bound:.3f}, {upper_bound:.3f}]")
            print(f"  Outliers détectés : {outliers_low + outliers_high} ({outliers_low} bas, {outliers_high} hauts)")
            print("  ✅ Capping appliqué.")

    joblib.dump(capping_params, save_path)

    if display_info:
        print(f"\n🚀 Capping terminé et paramètres sauvegardés dans {save_path}")

    return capped_df



import numpy as np
import joblib

print("Application du Capping (Winsorizing) sur les variables imputées par MICE et transformées")

df_mice_capped = apply_capping_and_save(
    df=df_imputed_mice_transformed,
    cols_to_cap=['X1_transformed', 'X2_transformed', 'X3_transformed'],
    save_path=paths["MODELS_DIR"] / "notebook1" / "capping_params_mice.pkl"
)



## 📊 Visualisation comparative avant/après traitement des outliers

from exploration.visualization import plot_outlier_comparison


transformed_cols = ['X1_transformed', 'X2_transformed', 'X3_transformed']
# ✅ Comparaison visuelle avant / après (X1_trans, X2_trans, X3_trans)
plot_outlier_comparison(
    df_before=df_imputed_mice_transformed,
    df_after=df_mice_capped,
    cols=transformed_cols,
    output_dir=FIGURES_DIR / "figures_notebook1" / "pretraitement_avance",
    show=True
)



import numpy as np
import joblib

print("🔧 Application du Capping (Winsorizing) sur les variables imputées par KNN et transformées")
df_knn_capped = apply_capping_and_save(
    df=df_imputed_knn_transformed,
    cols_to_cap=['X1_transformed', 'X2_transformed', 'X3_transformed'],
    save_path=paths["MODELS_DIR"] / "notebook1" / "capping_params_knn.pkl"
)




## 📊 Visualisation comparative avant/après traitement des outliers

from exploration.visualization import plot_outlier_comparison


transformed_cols = ['X1_transformed', 'X2_transformed', 'X3_transformed']
# ✅ Comparaison visuelle avant / après (X1_trans, X2_trans, X3_trans)
plot_outlier_comparison(
    df_before=df_imputed_knn_transformed,
    df_after=df_knn_capped,
    cols=transformed_cols,
    output_dir=FIGURES_DIR / "figures_notebook1" / "pretraitement_avance",
    show=True
)




# Vérification rapide
print(f"📊 Valeurs manquantes dans les données outliers traités (données knn) : {df_knn_capped.isnull().sum().sum()} total")


print(f"📊 Valeurs manquantes dans les données outliers traités (MICE) : {df_mice_capped.isnull().sum().sum()} total")








df_mice_no_outliers = df_mice_capped.copy(deep=True)
df_knn_no_outliers = df_knn_capped.copy(deep=True)
df_mice_with_outliers = df_imputed_mice_transformed.copy(deep=True)
df_knn_with_outliers = df_imputed_knn_transformed.copy(deep=True)


print(df_knn_no_outliers.shape)
print(df_knn_with_outliers.shape)


## 5.4.1 Détection et traitement des variables collinéaires <a id="detection-et-traitement-des-variables-collineaires"></a>

from preprocessing.final_preprocessing import find_highly_correlated_groups

datasets = {
    "mice_no_outliers": df_mice_no_outliers,
    "knn_no_outliers": df_knn_no_outliers,
    "mice_with_outliers": df_mice_with_outliers,
    "knn_with_outliers": df_knn_with_outliers
}

correlated_results = {}

for name, df in datasets.items():
    print(f"\n🔍 Détection collinéarité sur : {name}")

    save_path = FIGURES_DIR / "figures_notebook1" / "pretraitement_avance" / f"correlation_heatmap_collinearity_{name}.png"

    result = find_highly_correlated_groups(
        df=df,
        threshold=0.95,
        exclude_cols=['y', 'X4'],
        show_plot=False,
        save_path=save_path
    )

    correlated_results[name] = result

    print(f" {len(result['groups'])} groupes détectés pour {name}")
    print(f" {len(result['to_drop'])} variables à supprimer pour {name}")






print("🧹 Suppression des variables fortement corrélées")
print("=" * 60)

from preprocessing.final_preprocessing import apply_collinearity_filter

filtered_datasets = {}

for name, df in datasets.items():
    print(f"\nSuppression pour les données : {name}")

    cols_to_drop = correlated_results[name]['to_drop']

    df_filtered = apply_collinearity_filter(
        df=df,
        cols_to_drop=cols_to_drop,
        imputation_method='mice' if 'mice' in name else 'knn',
        models_dir=paths["MODELS_DIR"] / "notebook1",
        display_info=True
    )

    filtered_datasets[name] = df_filtered
    print(f"✅ {name} : {len(cols_to_drop)} variables supprimées, nouvelle forme : {df_filtered.shape}")






### 5.6 Sauvegarde des datasets filtrés

print("💾 Sauvegarde des jeux de données imputées, avec ou sans outliers, filtrés")
print("=" * 60)

from pathlib import Path

# ✅ Dossier de sauvegarde
filtered_dir = paths["DATA_PROCESSED"] / "notebook1" / "datasets_filtered_final"
filtered_dir.mkdir(parents=True, exist_ok=True)

# ✅ Sauvegarde automatique de toutes les versions
for name, df_filtered in filtered_datasets.items():
    save_path = filtered_dir / f"df_{name}_filtered.csv"
    df_filtered.to_csv(save_path, index=False)
    print(f"✅ Fichier sauvegardé : {save_path}")

print("✅ Toutes les versions filtrées ont été sauvegardées avec succès.")






from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import joblib

print("🔧 Application de l'ingénierie de caractéristiques (Feature Engineering) sur les 4 jeux de données")

continuous_cols_clean = ['X1_transformed', 'X2_transformed', 'X3_transformed']

# Dictionnaire pour stocker les versions finales
engineered_datasets = {}

# Boucle automatique sur les 4 jeux de données filtrés
for name, df in filtered_datasets.items():
    print(f"\n➡️ Traitement pour : {name}")

    df_continuous = df[continuous_cols_clean]
    other_cols = [col for col in df.columns if col not in continuous_cols_clean]
    df_others = df[other_cols]

    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)
    poly_features = poly.fit_transform(df_continuous)

    # Sauvegarde du transformer
    transformer_path = paths["MODELS_DIR"] / "notebook1" / f"poly_transformer_{name}.pkl"
    joblib.dump(poly, transformer_path)
    print(f"✅ Transformateur PolynomialFeatures sauvegardé : {transformer_path.name}")

    # Création du DataFrame avec les nouvelles caractéristiques
    poly_feature_names = poly.get_feature_names_out(continuous_cols_clean)
    df_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=df.index)

    # Combinaison finale
    df_engineered = pd.concat([df_others, df_poly], axis=1)

    print(f"   Nombre de caractéristiques avant : {len(df.columns)}")
    print(f"   Nombre de caractéristiques après : {len(df_engineered.columns)}")

    engineered_datasets[name] = df_engineered

    print("✅ Ingénierie terminée pour :", name)

print("\n🎯 Tous les datasets ont été transformés et sauvegardés avec leur transformer associé.")






from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.linear_model import LogisticRegression
import pandas as pd
import joblib

print("🚀 Recherche automatique du k optimal et sélection finale pour les 4 jeux de données")
print("=" * 60)

selected_datasets = {}

for name, df in engineered_datasets.items():
    print(f"\n🔍 Traitement pour : {name}")

    X = df.drop(columns=['y'])
    y = df['y']

    pipeline = Pipeline([
        ('selector', SelectKBest(score_func=f_classif)),
        ('model', LogisticRegression(solver='liblinear', random_state=42))
    ])

    param_grid = {'selector__k': [50, 100, 150, 200, 250, 300, 400, 500]}

    cv_strategy = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)

    grid_search = GridSearchCV(
        estimator=pipeline,
        param_grid=param_grid,
        scoring='f1',
        cv=cv_strategy,
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X, y)

    best_k = grid_search.best_params_['selector__k']
    print(f"✅ Meilleur k pour {name} : {best_k}")

    selector = SelectKBest(score_func=f_classif, k=best_k)
    selector.fit(X, y)

    selected_cols_mask = selector.get_support()
    selected_cols = X.columns[selected_cols_mask]

    df_final = df[selected_cols.tolist() + ['y']]
    
    # ✅ Renommage ici AVANT ajout au dictionnaire et sauvegarde :
    df_final = df_final.rename(columns={"y": "outcome"})

    selected_datasets[name] = df_final

# Nettoyage après sélection complète
print("\n🧹 Nettoyage : suppression des variables d'origine avant sauvegarde finale")
cols_to_remove = ['X1', 'X2', 'X3']

for name, df in selected_datasets.items():
    cols_in_df = [col for col in cols_to_remove if col in df.columns]
    if cols_in_df:
        print(f"   → {name}: suppression de {cols_in_df}")
        df.drop(columns=cols_in_df, inplace=True)
    else:
        print(f"   → {name}: aucune variable d'origine à supprimer")

    selected_datasets[name] = df

    output_path = paths["DATA_PROCESSED"] / "notebook1/final_data_for_modeling" / f"df_final_for_modeling_{name}.csv"
    df.to_csv(output_path, index=False)
    print(f"💾 Jeu de données final sauvegardé : {output_path}")

    selector_path = paths["MODELS_DIR"] / "notebook1" / f"selector_{name}.pkl"
    joblib.dump(selector, selector_path)
    print(f"💾 Sélecteur sauvegardé : {selector_path}")

print("\n✅ Processus complet terminé pour les 4 jeux de données. Nous pouvons passer à la modélisation.")
















# 6. Construction d'un pipeline pour générer les datasets finaux (pour la modélisation) <a id="construction-des-datasets-finaux"></a>
# prompt: Je voudrais un pipeline complet qui reprend tous les pretraitement effectué jusqu'à cette section et qui me permetra de générer des prediction sur des nouvelles données après la phase de modélisation que j'aurai à choisir le data set,  le model, la méthode d'imputation, traitement des outliers ou pas et la liste des colonnes à utiliser en paramètre.

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import PowerTransformer
from sklearn.ensemble import RandomForestRegressor
import joblib

class PreprocessingPipeline:
    def __init__(self,
                 imputation_method='knn',  # 'knn' or 'mice'
                 outliers_method='capping', # 'capping' or None
                 features_list=None, # list of columns to use, None for all
                 degree_poly=2, # degree for polynomial features on continuous cols
                 k_best_features=200 # number of features after selection
                ):

        self.imputation_method = imputation_method
        self.outliers_method = outliers_method
        self.features_list = features_list
        self.degree_poly = degree_poly
        self.k_best_features = k_best_features

        self.continuous_cols = ['X1', 'X2', 'X3']
        self.binary_col = 'X4'
        self.target_col = 'y'
        self.pipeline = None
        self.capping_params = {} # To store bounds for capping

    def build_pipeline(self):
        """Builds the preprocessing pipeline based on initialized parameters."""

        steps = []

        # 1. Handle X4 missing values (simple median imputation)
        # This step is done separately before any complex pipeline because X4 is binary
        # and already handled in previous code. We assume it's handled correctly.
        # If not, a SimpleImputer step could be added here specifically for X4.
        # For this pipeline, we assume X4 is either handled or not included if features_list is provided.


        # 2. Imputation for continuous columns (X1, X2, X3)
        if self.imputation_method == 'knn':
            # KNNImputer works best on scaled data, but it can handle NaNs.
            # Transformation and scaling should happen after imputation.
            # We'll perform imputation first, then transform, then cap.
            imputer = KNNImputer(n_neighbors=optimal_k) # Use optimal k found previously
            steps.append(('knn_imputer', imputer))
        elif self.imputation_method == 'mice':
             # Use RandomForestRegressor estimator for IterativeImputer
            imputer = IterativeImputer(estimator=RandomForestRegressor(
                                            n_estimators=400,
                                            max_depth=20,
                                            min_samples_leaf=2,
                                            max_features=0.5,
                                            random_state=42,
                                            n_jobs=-1),
                                        max_iter=10,
                                        random_state=42)
            steps.append(('mice_imputer', imputer))
        else:
            raise ValueError("Invalid imputation_method. Choose 'knn' or 'mice'.")

        # 3. Power Transformation for continuous columns (after imputation)
        # Yeo-Johnson for X1, X2; Box-Cox for X3 as determined optimal
        # PowerTransformer works on non-negative data for Box-Cox and any data for Yeo-Johnson
        # Since we impute first, we assume imputed values are reasonable for these.
        # A custom transformer might be needed if applying different methods per column.
        # For simplicity here, we'll use Yeo-Johnson for all, or assume transformation is handled post-imputation
        # For this pipeline, we'll apply Yeo-Johnson to the imputed continuous cols.
        # A more robust pipeline would use a ColumnTransformer or custom logic for mixed transformations.
        # Let's assume PowerTransformer with method='yeo-johnson' on the continuous cols.
        # NOTE: Applying PowerTransformer *after* imputation on the imputed data.
        steps.append(('power_transformer', PowerTransformer(method='yeo-johnson')))


        # 4. Outlier Treatment (Capping) - Applied AFTER transformation
        # Capping should be applied based on bounds learned on the training data
        # This requires a custom transformer or handling outside the standard pipeline if bounds depend on data.
        # For simplicity, we'll skip the capping step within the standard sklearn pipeline.
        # Capping parameters should be saved and applied manually during prediction.
        # If capping is critical and must be in the pipeline, a custom transformer inheriting from BaseEstimator, TransformerMixin is needed.
        # For this example, we assume capping parameters are applied manually.

        # 5. Feature Engineering (Polynomial Features on continuous cols)
        steps.append(('polynomial_features', PolynomialFeatures(degree=self.degree_poly, include_bias=False, interaction_only=False)))


        # 6. Feature Selection (SelectKBest)
        # Apply SelectKBest after feature engineering
        steps.append(('feature_selection', SelectKBest(score_func=f_classif, k=self.k_best_features)))

        self.pipeline = Pipeline(steps)

    def fit(self, X, y):
        """Fits the pipeline on the training data."""
        if self.pipeline is None:
            self.build_pipeline()

        # Select only the continuous columns for transformation and polynomial features
        # The pipeline steps related to continuous vars will apply to these
        # Binary features and X4 (if not in features_list) are expected to pass through if using ColumnTransformer.
        # Given the current simple pipeline structure, we need to align X and y.

        # If features_list is provided, filter X
        if self.features_list is not None:
            # Ensure continuous cols and X4 (if needed) are in the features_list
            required_cols = self.continuous_cols.copy()
            if self.binary_col in X.columns:
                 required_cols.append(self.binary_col)

            # Filter X to include only specified features and required ones
            X_processed = X[self.features_list + [col for col in required_cols if col not in self.features_list]].copy()
        else:
            # Use all features if no list is provided
             X_processed = X.copy()


        # Handle X4 imputation if it wasn't done prior to calling the pipeline fit
        # This is a potential point of failure if X4 has NaNs and is included but not handled.
        # Based on the previous code, X4 is imputed by median before the pipeline.
        # We'll assume X4 is clean if present in X_processed.

        # Apply the pipeline
        print(f"Fitting pipeline with imputation_method='{self.imputation_method}', outliers_method='{self.outliers_method}'...")
        self.pipeline.fit(X_processed, y)
        print("Pipeline fitting complete.")

        # Store capping parameters if outliers_method is 'capping' (outside the pipeline fit)
        # This part would typically be done manually after transformation but before PolynomialFeatures
        # or integrated into a custom transformer. Let's skip it here for pipeline simplicity,
        # noting that capping would need separate application during prediction.
        if self.outliers_method == 'capping':
            print("NOTE: Capping parameters for prediction are not stored within this standard sklearn pipeline fit.")
            print("You would need to manually compute and store bounds from the transformed training data.")


    def transform(self, X):
        """Transforms the data using the fitted pipeline."""
        if self.pipeline is None:
            raise RuntimeError("Pipeline has not been fitted yet. Call .fit() first.")

         # If features_list is provided, filter X
        if self.features_list is not None:
            # Ensure continuous cols and X4 (if needed) are in the features_list
            required_cols = self.continuous_cols.copy()
            if self.binary_col in X.columns:
                 required_cols.append(self.binary_col)

            # Filter X to include only specified features and required ones
            X_processed = X[self.features_list + [col for col in required_cols if col not in self.features_list]].copy()
        else:
            # Use all features if no list is provided
             X_processed = X.copy()

        # Handle X4 imputation if it wasn't done prior to calling the pipeline fit
        # This is a potential point of failure if X4 has NaNs and is included but not handled.
        # Based on the previous code, X4 is imputed by median before the pipeline.
        # We'll assume X4 is clean if present in X_processed.

        # Apply the pipeline
        print(f"Transforming data using the fitted pipeline...")
        X_transformed = self.pipeline.transform(X_processed)
        print("Data transformation complete.")

        # Get feature names after transformation (might be tricky with multiple steps)
        # For SelectKBest, we can get the final features.
        selector = self.pipeline.named_steps['feature_selection']
        # Need to get names from the step *before* selection first
        # This requires more complex handling of feature names through the pipeline.
        # For simplicity, we'll return a numpy array. If feature names are critical,
        # a ColumnTransformer and careful naming conventions are needed.
        # A common practice is to just return the numpy array from transform
        # and handle feature names separately if needed for model interpretation.

        return X_transformed

    def fit_transform(self, X, y):
        """Fits the pipeline and transforms the data."""
        self.fit(X, y)
        return self.transform(X)

    def predict(self, X, model):
        """Transforms new data and makes predictions using a fitted model."""
        X_processed = self.transform(X)
        # Assuming the model takes a numpy array as input
        predictions = model.predict(X_processed)
        return predictions

    def save_pipeline(self, path):
        """Saves the fitted pipeline to a file."""
        if self.pipeline is None:
             raise RuntimeError("Pipeline has not been fitted yet. Call .fit() first.")
        joblib.dump(self.pipeline, path)
        print(f"Pipeline saved successfully to {path}")

    def load_pipeline(self, path):
        """Loads a fitted pipeline from a file."""
        self.pipeline = joblib.load(path)
        print(f"Pipeline loaded successfully from {path}")



