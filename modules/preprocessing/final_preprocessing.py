"""
preprocessing/final_preprocessing.py - VERSION CORRIGÃ‰E AVEC PROTECTION X4

Module de prÃ©traitement complet pour le projet STA211 Internet Advertisements.
Inclut la protection de X4, l'ordre des colonnes optimisÃ©, et la validation Ã  chaque Ã©tape.

CORRECTIONS:
- Fix TypeError dans find_highly_correlated_groups
- Validation robuste des types de retour
- Gestion d'erreurs amÃ©liorÃ©e

Auteur: Abdoullatuf
Version: 2.1 (CorrigÃ©e)
Date: 2025
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from pathlib import Path
from typing import List, Optional, Union, Dict, Tuple

from sklearn.preprocessing import PowerTransformer

from config.paths_config import setup_project_paths
from preprocessing.missing_values import handle_missing_values
from preprocessing.outliers import detect_and_remove_outliers
from preprocessing.data_loader import load_data
from exploration.visualization import save_fig


# ============================================================================
# 1. UTILITAIRES DE BASE
# ============================================================================

def convert_X4_to_int(df: pd.DataFrame, column: str = "X4", verbose: bool = True) -> pd.DataFrame:
    """
    Convertit X4 en Int64 si elle contient uniquement des valeurs binaires (0, 1).
    
    Args:
        df: DataFrame d'entrÃ©e
        column: Nom de la colonne Ã  convertir (dÃ©faut: "X4")
        verbose: Affichage des informations
        
    Returns:
        DataFrame avec X4 convertie en Int64
    """
    df = df.copy()
    
    if column not in df.columns:
        if verbose:
            print(f"âš ï¸ Colonne '{column}' absente du DataFrame.")
        return df

    unique_vals = df[column].dropna().unique()
    if set(unique_vals).issubset({0, 1}):
        df[column] = df[column].astype("Int64")
        if verbose:
            print(f"âœ… Colonne '{column}' convertie en Int64 (binaire).")
    elif verbose:
        print(f"âŒ Colonne '{column}' contient {unique_vals}. Conversion ignorÃ©e.")

    return df


def apply_yeojohnson(
    df: pd.DataFrame,
    columns: List[str],
    standardize: bool = False,
    save_model: bool = False,
    model_path: Optional[Union[str, Path]] = None,
    return_transformer: bool = False
) -> Union[pd.DataFrame, Tuple[pd.DataFrame, PowerTransformer]]:
    """
    Applique la transformation Yeo-Johnson sur les colonnes spÃ©cifiÃ©es.
    
    Args:
        df: DataFrame d'entrÃ©e
        columns: Colonnes Ã  transformer
        standardize: Standardiser aprÃ¨s transformation
        save_model: Sauvegarder le transformateur
        model_path: Chemin de sauvegarde du modÃ¨le
        return_transformer: Retourner aussi le transformateur
        
    Returns:
        DataFrame transformÃ© (et transformateur si demandÃ©)
    """
    df_transformed = df.copy()

    # Charger ou crÃ©er le transformateur
    if model_path and Path(model_path).exists():
        pt = joblib.load(model_path)
        print(f"ðŸ”„ Transformateur rechargÃ© depuis : {model_path}")
    else:
        pt = PowerTransformer(method="yeo-johnson", standardize=standardize)
        pt.fit(df[columns])
        
        if save_model and model_path:
            Path(model_path).parent.mkdir(parents=True, exist_ok=True)
            joblib.dump(pt, model_path)
            print(f"âœ… Transformateur Yeo-Johnson sauvegardÃ© Ã  : {model_path}")

    # Appliquer la transformation
    transformed_values = pt.transform(df[columns])
    for i, col in enumerate(columns):
        df_transformed[f"{col}_trans"] = transformed_values[:, i]

    return (df_transformed, pt) if return_transformer else df_transformed


# ============================================================================
# 2. GESTION DE LA CORRÃ‰LATION ET RÃ‰DUCTION DE DIMENSIONNALITÃ‰ (CORRIGÃ‰E)
# ============================================================================

def find_highly_correlated_groups(
    df: pd.DataFrame,
    threshold: float = 0.90,
    exclude_cols: Optional[List[str]] = None,
    protected_cols: Optional[List[str]] = None,
    show_plot: bool = False,
    save_path: Optional[Union[str, Path]] = None,
    figsize: tuple = (10, 8)
) -> Dict[str, Union[List[List[str]], List[str]]]:
    """
    Identifie les groupes de variables fortement corrÃ©lÃ©es avec protection de certaines colonnes.
    
    CORRECTION: Garantit toujours un retour en dictionnaire avec validation.
    
    Args:
        df: DataFrame d'entrÃ©e
        threshold: Seuil de corrÃ©lation (dÃ©faut: 0.90)
        exclude_cols: Colonnes Ã  exclure de l'analyse
        protected_cols: Colonnes Ã  protÃ©ger de la suppression (dÃ©faut: ['X4'])
        show_plot: Afficher la heatmap de corrÃ©lation
        save_path: Chemin de sauvegarde de la figure
        figsize: Taille de la figure
        
    Returns:
        Dictionnaire avec groups, to_drop, et protected
    """
    # ðŸ›¡ï¸ Protection par dÃ©faut de X4
    if protected_cols is None:
        protected_cols = ['X4']
    
    # ðŸ”§ VALIDATION D'ENTRÃ‰E (NOUVELLE)
    if df.empty:
        print("âš ï¸ DataFrame vide - retour structure par dÃ©faut")
        return {"groups": [], "to_drop": [], "protected": protected_cols}
    
    # Combinaison des colonnes Ã  exclure
    all_exclude_cols = (exclude_cols or []) + (protected_cols or [])
    
    # Calcul de la matrice de corrÃ©lation
    df_corr = df.drop(columns=all_exclude_cols, errors='ignore') if all_exclude_cols else df.copy()
    
    # ðŸ”§ VALIDATION INTERMÃ‰DIAIRE (NOUVELLE)
    if df_corr.empty:
        print("âš ï¸ Aucune colonne Ã  analyser aprÃ¨s exclusions")
        return {"groups": [], "to_drop": [], "protected": protected_cols}
    
    try:
        corr_matrix = df_corr.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    except Exception as e:
        print(f"âš ï¸ Erreur dans le calcul de corrÃ©lation: {e}")
        return {"groups": [], "to_drop": [], "protected": protected_cols}

    # Identification des groupes corrÃ©lÃ©s
    groups, visited = [], set()
    for col in upper.columns:
        if col in visited:
            continue
        correlated = upper[col][upper[col] > threshold].index.tolist()
        if correlated:
            group = sorted(set([col] + correlated))
            groups.append(group)
            visited.update(group)

    # ðŸ›¡ï¸ CrÃ©ation de la liste to_drop avec protection
    to_drop = []
    for group in groups:
        # Dans chaque groupe, on garde le premier ET les colonnes protÃ©gÃ©es
        protected_in_group = [col for col in group if col in protected_cols]
        non_protected = [col for col in group if col not in protected_cols]
        
        if non_protected:
            # Garder le premier non-protÃ©gÃ©, supprimer les autres non-protÃ©gÃ©s
            to_drop.extend(non_protected[1:])
        # Les colonnes protÃ©gÃ©es ne sont jamais ajoutÃ©es Ã  to_drop

    # Visualisation optionnelle
    if show_plot and not corr_matrix.empty:
        try:
            plt.figure(figsize=figsize)
            sns.heatmap(corr_matrix, cmap="coolwarm", center=0, square=True, 
                       cbar_kws={"shrink": 0.75})
            plt.title(f"Matrice de corrÃ©lation (>{threshold})")
            plt.tight_layout()
            
            if save_path:
                save_fig(Path(save_path).name, directory=Path(save_path).parent, figsize=figsize)
            else:
                plt.show()
        except Exception as e:
            print(f"âš ï¸ Erreur dans la visualisation: {e}")

    # ðŸ›¡ï¸ VÃ©rification finale de protection
    protected_in_drop = set(to_drop) & set(protected_cols)
    if protected_in_drop:
        print(f"ðŸ›¡ï¸ PROTECTION ACTIVÃ‰E: Retrait de {protected_in_drop} de la liste de suppression")
        to_drop = [col for col in to_drop if col not in protected_cols]

    # ðŸ”§ CONSTRUCTION ET VALIDATION DU RETOUR (NOUVELLE)
    result = {
        "groups": groups,
        "to_drop": to_drop,
        "protected": protected_cols
    }
    
    # Validation du type de retour
    assert isinstance(result, dict), "Le retour doit Ãªtre un dictionnaire"
    assert "groups" in result, "La clÃ© 'groups' doit Ãªtre prÃ©sente"
    assert isinstance(result["groups"], list), "groups doit Ãªtre une liste"
    assert isinstance(result["to_drop"], list), "to_drop doit Ãªtre une liste"
    assert isinstance(result["protected"], list), "protected doit Ãªtre une liste"

    return result


def drop_correlated_duplicates(
    df: pd.DataFrame,
    groups: List[List[str]],
    target_col: str = "outcome",
    extra_cols: List[str] = None,
    protected_cols: List[str] = None,
    priority_cols: List[str] = None,
    verbose: bool = False,
    summary: bool = True
) -> Tuple[pd.DataFrame, List[str], List[str]]:
    """
    Supprime les variables corrÃ©lÃ©es avec protection et ordre des colonnes optimisÃ©.
    
    Args:
        df: DataFrame d'entrÃ©e
        groups: Groupes de variables corrÃ©lÃ©es
        target_col: Variable cible
        extra_cols: Colonnes supplÃ©mentaires Ã  conserver
        protected_cols: Colonnes Ã  protÃ©ger (dÃ©faut: ['X4'])
        priority_cols: Colonnes prioritaires pour l'ordre (dÃ©faut: ['X1_trans', 'X2_trans', 'X3_trans', 'X4'])
        verbose: Affichage dÃ©taillÃ©
        summary: Afficher le rÃ©sumÃ©
        
    Returns:
        Tuple (DataFrame rÃ©duit, colonnes supprimÃ©es, colonnes gardÃ©es)
    """
    # ðŸ›¡ï¸ Protection par dÃ©faut
    if protected_cols is None:
        protected_cols = ['X4']
    
    # ðŸ“Œ Colonnes prioritaires par dÃ©faut
    if priority_cols is None:
        priority_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4']
    
    # ðŸ”§ VALIDATION D'ENTRÃ‰E (NOUVELLE)
    if not isinstance(groups, list):
        print(f"âš ï¸ groups doit Ãªtre une liste, reÃ§u: {type(groups)}")
        groups = []
    
    to_drop, to_keep = [], []

    # Traitement des groupes corrÃ©lÃ©s
    for group in groups:
        if not group or not isinstance(group, list):
            continue
        
        # ðŸ›¡ï¸ SÃ©parer les colonnes protÃ©gÃ©es des autres
        protected_in_group = [col for col in group if col in protected_cols]
        non_protected = [col for col in group if col not in protected_cols and col in df.columns]
        
        if non_protected:
            # Garder le premier non-protÃ©gÃ©
            keep = non_protected[0]
            drop = non_protected[1:]
            to_keep.append(keep)
            to_drop.extend(drop)
            
            if verbose:
                print(f"ðŸ§¹ Groupe : {group} â†’ garde {keep}, retire {drop}")
                if protected_in_group:
                    print(f"ðŸ›¡ï¸   ProtÃ©gÃ©es dans ce groupe : {protected_in_group}")
        
        # ðŸ›¡ï¸ Les colonnes protÃ©gÃ©es sont toujours gardÃ©es
        for protected in protected_in_group:
            if protected not in to_keep:
                to_keep.append(protected)

    to_drop = sorted(set(to_drop))
    to_keep = sorted(set(to_keep))

    # Colonnes binaires restantes (non corrÃ©lÃ©es)
    all_binary = [col for col in df.select_dtypes(include=['int64', 'Int64']).columns 
                  if col != target_col]
    untouched = [col for col in all_binary if col not in to_drop + to_keep]

    # ðŸ“Œ CONSTRUCTION DE L'ORDRE FINAL DES COLONNES
    
    # 1. Colonnes prioritaires (en premier)
    priority_existing = [col for col in priority_cols if col in df.columns]
    
    # 2. Variable cible (aprÃ¨s les prioritaires)
    target_existing = [target_col] if target_col and target_col in df.columns else []
    
    # 3. Extra cols (variables transformÃ©es, etc.)
    extra_existing = []
    if extra_cols:
        extra_existing = [col for col in extra_cols 
                         if col in df.columns and col not in priority_existing + target_existing]
    
    # 4. Variables gardÃ©es par corrÃ©lation (pas dÃ©jÃ  dans prioritaires/extra)
    kept_remaining = [col for col in to_keep 
                     if col not in priority_existing + target_existing + extra_existing]
    
    # 5. Variables intactes (pas dÃ©jÃ  listÃ©es)
    untouched_remaining = [col for col in untouched 
                          if col not in priority_existing + target_existing + extra_existing + kept_remaining]
    
    # ðŸ›¡ï¸ S'assurer que les colonnes protÃ©gÃ©es sont prÃ©sentes
    protected_remaining = []
    for protected in protected_cols:
        if (protected in df.columns and 
            protected not in priority_existing + target_existing + extra_existing + 
            kept_remaining + untouched_remaining):
            protected_remaining.append(protected)
    
    # ðŸ“Œ ORDRE FINAL : prioritaires â†’ cible â†’ extra â†’ gardÃ©es â†’ intactes â†’ protÃ©gÃ©es restantes
    final_cols = (priority_existing + target_existing + extra_existing + 
                  kept_remaining + untouched_remaining + protected_remaining)
    
    # Filtrage des colonnes existantes (sÃ©curitÃ©)
    existing_cols = [col for col in final_cols if col in df.columns]
    df_reduced = df[existing_cols].copy()

    # Affichage du rÃ©sumÃ©
    if summary:
        print(f"\nðŸ“Š RÃ©duction : {len(to_drop)} supprimÃ©es, {len(to_keep)} gardÃ©es, {len(untouched)} intactes.")
        print(f"ðŸ“Œ Ordre final : {priority_existing[:3]}{'...' if len(priority_existing) > 3 else ''} â†’ {target_existing} â†’ reste")
        
        if protected_cols:
            protected_in_final = [col for col in protected_cols if col in existing_cols]
            print(f"ðŸ›¡ï¸ {len(protected_in_final)} colonnes protÃ©gÃ©es : {protected_in_final}")
        
        if extra_cols:
            existing_extra = [col for col in extra_cols if col in existing_cols]
            print(f"ðŸ§© {len(existing_extra)} extra conservÃ©es : {existing_extra}")
        
        print(f"ðŸ“ Dimensions : {df_reduced.shape}")

    return df_reduced, to_drop, to_keep


def apply_collinearity_filter(
    df: pd.DataFrame, 
    cols_to_drop: List[str], 
    protected_cols: List[str] = None, 
    display_info: bool = True
) -> pd.DataFrame:
    """
    Supprime les colonnes corrÃ©lÃ©es en protÃ©geant certaines colonnes.
    
    Args:
        df: DataFrame d'entrÃ©e
        cols_to_drop: Colonnes Ã  supprimer
        protected_cols: Colonnes Ã  protÃ©ger (dÃ©faut: ['X4'])
        display_info: Afficher les informations
        
    Returns:
        DataFrame filtrÃ©
    """
    # ðŸ›¡ï¸ Protection par dÃ©faut
    if protected_cols is None:
        protected_cols = ['X4']
    
    # ðŸ›¡ï¸ Retirer les colonnes protÃ©gÃ©es de la liste de suppression
    original_drop_count = len(cols_to_drop)
    cols_to_drop_filtered = [col for col in cols_to_drop if col not in protected_cols]
    protected_saved = original_drop_count - len(cols_to_drop_filtered)
    
    if protected_saved > 0 and display_info:
        saved_cols = [col for col in cols_to_drop if col in protected_cols]
        print(f"ðŸ›¡ï¸ {protected_saved} colonnes protÃ©gÃ©es de la suppression : {saved_cols}")
    
    # Suppression sÃ©curisÃ©e
    df_filtered = df.drop(columns=[col for col in cols_to_drop_filtered if col in df.columns])
    
    if display_info:
        print(f"âœ… Colonnes supprimÃ©es : {len(cols_to_drop_filtered)}")
        print(f"ðŸ“ Dimensions finales : {df_filtered.shape}")
        
        # ðŸ›¡ï¸ VÃ©rification finale que les colonnes protÃ©gÃ©es sont prÃ©sentes
        for protected in protected_cols:
            if protected in df.columns:
                status = "âœ…" if protected in df_filtered.columns else "âŒ"
                print(f"ðŸ›¡ï¸ {protected} : {status}")
    
    return df_filtered


# ============================================================================
# 3. FONCTIONS DE VALIDATION ET PROTECTION X4
# ============================================================================

def validate_x4_presence(df: pd.DataFrame, step_name: str = "", verbose: bool = True) -> bool:
    """
    Valide que X4 est prÃ©sente et correcte dans le DataFrame.
    
    Args:
        df: DataFrame Ã  vÃ©rifier
        step_name: Nom de l'Ã©tape (pour l'affichage)
        verbose: Affichage des informations
        
    Returns:
        True si X4 est prÃ©sente et correcte
    """
    if 'X4' not in df.columns:
        if verbose:
            print(f"âŒ {step_name}: X4 MANQUANTE !")
        return False
    
    # VÃ©rifier le type et les valeurs
    unique_vals = sorted(df['X4'].dropna().unique())
    expected_vals = [0, 1]
    
    if set(unique_vals).issubset(set(expected_vals)):
        if verbose:
            print(f"âœ… {step_name}: X4 prÃ©sente et correcte (valeurs: {unique_vals})")
        return True
    else:
        if verbose:
            print(f"âš ï¸ {step_name}: X4 prÃ©sente mais valeurs inattendues: {unique_vals}")
        return False


def quick_x4_check(df_or_dict, name: str = "Dataset") -> bool:
    """
    VÃ©rification rapide de X4 dans un DataFrame ou dictionnaire de DataFrames.
    
    Args:
        df_or_dict: DataFrame ou dictionnaire de DataFrames
        name: Nom pour l'affichage
        
    Returns:
        True si X4 est prÃ©sente dans tous les datasets
    """
    if isinstance(df_or_dict, dict):
        print(f"ðŸ” VÃ©rification X4 dans {len(df_or_dict)} datasets:")
        all_good = True
        for dataset_name, df in df_or_dict.items():
            has_x4 = 'X4' in df.columns if df is not None else False
            print(f"  {dataset_name}: {'âœ…' if has_x4 else 'âŒ'}")
            if not has_x4:
                all_good = False
        return all_good
    else:
        # DataFrame unique
        has_x4 = 'X4' in df_or_dict.columns
        print(f"ðŸ” {name}: {'âœ…' if has_x4 else 'âŒ'} X4")
        return has_x4


# ============================================================================
# 4. GESTION DE L'ORDRE DES COLONNES
# ============================================================================

def reorder_columns_priority(
    df: pd.DataFrame, 
    priority_cols: List[str] = None,
    target_col: str = "outcome"
) -> pd.DataFrame:
    """
    RÃ©organise les colonnes avec un ordre prioritaire.
    
    Args:
        df: DataFrame Ã  rÃ©organiser
        priority_cols: Colonnes prioritaires (dÃ©faut: ['X1_trans', 'X2_trans', 'X3_trans', 'X4'])
        target_col: Variable cible Ã  placer aprÃ¨s les prioritaires
        
    Returns:
        DataFrame avec colonnes rÃ©organisÃ©es
    """
    if priority_cols is None:
        priority_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4']
    
    current_cols = df.columns.tolist()
    
    # 1. Variables prioritaires (en premier)
    final_priority = [col for col in priority_cols if col in current_cols]
    
    # 2. Variable cible (aprÃ¨s les prioritaires)
    final_target = [target_col] if target_col and target_col in current_cols else []
    
    # 3. Toutes les autres colonnes (dans l'ordre actuel)
    final_others = [col for col in current_cols if col not in final_priority + final_target]
    
    # Ordre final : prioritaires â†’ cible â†’ reste
    final_order = final_priority + final_target + final_others
    
    return df[final_order]


def check_column_order(
    df: pd.DataFrame, 
    expected_first: List[str] = None,
    display_info: bool = True
) -> bool:
    """
    VÃ©rifie l'ordre des colonnes dans le DataFrame.
    
    Args:
        df: DataFrame Ã  vÃ©rifier
        expected_first: Colonnes attendues en premier
        display_info: Afficher les informations
        
    Returns:
        True si l'ordre est correct
    """
    if expected_first is None:
        expected_first = ['X1_trans', 'X2_trans', 'X3_trans', 'X4', 'outcome']
    
    current_order = df.columns.tolist()
    
    if display_info:
        print(f"ðŸ“Œ Ordre actuel des colonnes (premiers 8) :")
        print(f"   {current_order[:8]}")
        
        print(f"ðŸ“Œ Colonnes attendues en premier :")
        print(f"   {expected_first}")
    
    # VÃ©rifier si les colonnes attendues sont bien en dÃ©but
    matches = []
    for i, expected_col in enumerate(expected_first):
        if expected_col in current_order:
            actual_position = current_order.index(expected_col)
            expected_position = i
            matches.append({
                'column': expected_col,
                'expected_pos': expected_position,
                'actual_pos': actual_position,
                'correct': actual_position == expected_position
            })
    
    all_correct = all(match['correct'] for match in matches)
    
    if display_info:
        print(f"ðŸ“Š VÃ©rification de l'ordre :")
        for match in matches:
            status = "âœ…" if match['correct'] else "âŒ"
            print(f"   {status} {match['column']}: position {match['actual_pos']} (attendu: {match['expected_pos']})")
    
    return all_correct


def reorganize_existing_datasets(
    datasets_dict: Dict[str, pd.DataFrame],
    priority_cols: List[str] = None,
    target_col: str = "outcome",
    verbose: bool = True
) -> Dict[str, pd.DataFrame]:
    """
    RÃ©organise plusieurs datasets existants avec l'ordre de colonnes souhaitÃ©.
    
    Args:
        datasets_dict: Dictionnaire {nom: DataFrame}
        priority_cols: Colonnes prioritaires
        target_col: Variable cible
        verbose: Affichage dÃ©taillÃ©
        
    Returns:
        Dictionnaire des datasets rÃ©organisÃ©s
    """
    if priority_cols is None:
        priority_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4']
    
    reorganized = {}
    
    for name, df in datasets_dict.items():
        if df is None:
            if verbose:
                print(f"âš ï¸ {name}: DataFrame vide, ignorÃ©")
            reorganized[name] = df
            continue
        
        # RÃ©organiser
        df_reordered = reorder_columns_priority(df, priority_cols, target_col)
        reorganized[name] = df_reordered
        
        if verbose:
            print(f"âœ… {name}: colonnes rÃ©organisÃ©es")
            print(f"   PremiÃ¨res colonnes : {df_reordered.columns[:min(6, len(df_reordered.columns))].tolist()}")
    
    return reorganized


# ============================================================================
# 5. PIPELINE PRINCIPAL DE PRÃ‰TRAITEMENT (CORRIGÃ‰)
# ============================================================================

def prepare_final_dataset(
    file_path: Union[str, Path],
    strategy: str = "mixed_mar_mcar",
    mar_method: str = "knn",
    knn_k: Optional[int] = None,
    mar_cols: List[str] = ["X1_trans", "X2_trans", "X3_trans"],
    mcar_cols: List[str] = ["X4"],
    drop_outliers: bool = False,
    correlation_threshold: float = 0.90,
    save_transformer: bool = False,
    processed_data_dir: Optional[Union[str, Path]] = None,
    models_dir: Optional[Union[str, Path]] = None,
    display_info: bool = True,
    raw_data_dir: Optional[Union[str, Path]] = None,
    require_outcome: bool = True,
    protect_x4: bool = True,
    priority_cols: List[str] = None,
    return_objects: bool = False
) -> Union[pd.DataFrame, Tuple[pd.DataFrame, Dict]]:
    """
    Pipeline de prÃ©traitement complet avec protection de X4 et ordre des colonnes optimisÃ©.
    
    CORRECTIONS:
    - Validation robuste de find_highly_correlated_groups
    - Gestion d'erreur pour les cas edge
    - Type checking systÃ©matique
    
    Args:
        file_path: Chemin vers le fichier de donnÃ©es
        strategy: StratÃ©gie d'imputation ("mixed_mar_mcar")
        mar_method: MÃ©thode d'imputation MAR ("knn" ou "mice")
        knn_k: ParamÃ¨tre k pour KNN (None = auto)
        mar_cols: Colonnes Ã  imputer avec mÃ©thode MAR
        mcar_cols: Colonnes Ã  imputer avec mÃ©thode MCAR
        drop_outliers: Supprimer les outliers
        correlation_threshold: Seuil de corrÃ©lation pour suppression
        save_transformer: Sauvegarder les transformateurs
        processed_data_dir: Dossier de sauvegarde des donnÃ©es
        models_dir: Dossier de sauvegarde des modÃ¨les
        display_info: Affichage des informations
        raw_data_dir: Dossier des donnÃ©es brutes
        require_outcome: NÃ©cessite la variable cible
        protect_x4: ProtÃ©ger X4 de la suppression
        priority_cols: Colonnes prioritaires pour l'ordre
        return_objects: Retourner aussi les objets de transformation
        
    Returns:
        DataFrame prÃ©traitÃ© (et objets si demandÃ©)
        
    Raises:
        ValueError: Si X4 est perdue pendant le preprocessing
    """
    paths = setup_project_paths()
    
    # ðŸ›¡ï¸ Configuration de protection
    protected_cols = ['X4'] if protect_x4 else []
    
    # ðŸ“Œ Colonnes prioritaires par dÃ©faut
    if priority_cols is None:
        priority_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4']

    # Dictionnaire pour stocker les objets de transformation
    transform_objects = {
        'scaler': None,
        'imputer': None,
        'yeojohnson': None,
        'correlation_info': None
    }

    if display_info:
        print("ðŸ”„ DÃ‰MARRAGE DU PIPELINE DE PRÃ‰TRAITEMENT (VERSION CORRIGÃ‰E)")
        print("=" * 70)

    # ========================================================================
    # Ã‰TAPE 1: CHARGEMENT DES DONNÃ‰ES
    # ========================================================================
    
    if display_info:
        print("ðŸ“‚ Ã‰tape 1: Chargement des donnÃ©es...")
    
    try:
        df = load_data(
            file_path=file_path,
            require_outcome=require_outcome,
            display_info=display_info,
            raw_data_dir=raw_data_dir,
            encode_target=True
        )
    except Exception as e:
        print(f"âŒ Erreur lors du chargement : {e}")
        raise
    
    # ðŸ›¡ï¸ Validation X4 aprÃ¨s chargement
    if protect_x4:
        validate_x4_presence(df, "AprÃ¨s chargement", display_info)

    # ========================================================================
    # Ã‰TAPE 2: CONVERSION DE X4
    # ========================================================================
    
    if display_info:
        print("\nðŸ”§ Ã‰tape 2: Conversion de X4...")
    
    df = convert_X4_to_int(df, verbose=display_info)
    
    # ðŸ›¡ï¸ Validation X4 aprÃ¨s conversion
    if protect_x4:
        validate_x4_presence(df, "AprÃ¨s conversion X4", display_info)

    # ========================================================================
    # Ã‰TAPE 3: TRANSFORMATION YEO-JOHNSON
    # ========================================================================
    
    if display_info:
        print("\nðŸ”„ Ã‰tape 3: Transformation Yeo-Johnson (X1, X2, X3)...")
    
    try:
        if return_objects:
            df, yeojohnson_transformer = apply_yeojohnson(
                df=df,
                columns=["X1", "X2", "X3"],
                standardize=False,
                save_model=save_transformer,
                model_path=models_dir / "yeojohnson.pkl" if save_transformer and models_dir else None,
                return_transformer=True
            )
            transform_objects['yeojohnson'] = yeojohnson_transformer
        else:
            df = apply_yeojohnson(
                df=df,
                columns=["X1", "X2", "X3"],
                standardize=False,
                save_model=save_transformer,
                model_path=models_dir / "yeojohnson.pkl" if save_transformer and models_dir else None,
                return_transformer=False
            )
        
        # Suppression des colonnes originales
        df.drop(columns=["X1", "X2", "X3"], inplace=True, errors="ignore")
        
    except Exception as e:
        print(f"âŒ Erreur lors de la transformation Yeo-Johnson : {e}")
        if display_info:
            print("âš ï¸ Poursuite sans transformation...")
    
    # ðŸ›¡ï¸ Validation X4 aprÃ¨s transformation
    if protect_x4:
        validate_x4_presence(df, "AprÃ¨s Yeo-Johnson", display_info)

    # ========================================================================
    # Ã‰TAPE 4: IMPUTATION DES VALEURS MANQUANTES
    # ========================================================================
    
    if display_info:
        print(f"\nðŸ”§ Ã‰tape 4: Imputation des valeurs manquantes ({mar_method})...")
    
    try:
        df = handle_missing_values(
            df=df,
            strategy=strategy,
            mar_method=mar_method,
            knn_k=knn_k,
            mar_cols=mar_cols,
            mcar_cols=mcar_cols,
            display_info=display_info,
            save_results=False,
            processed_data_dir=processed_data_dir,
            models_dir=models_dir
        )
    except Exception as e:
        print(f"âŒ Erreur lors de l'imputation : {e}")
        if display_info:
            print("âš ï¸ Poursuite sans imputation...")
    
    # ðŸ›¡ï¸ Validation X4 aprÃ¨s imputation
    if protect_x4:
        validate_x4_presence(df, "AprÃ¨s imputation", display_info)

    # ========================================================================
    # Ã‰TAPE 5: RÃ‰DUCTION DE LA COLINÃ‰ARITÃ‰ (SECTION CORRIGÃ‰E)
    # ========================================================================
    
    if display_info:
        print(f"\nðŸ”— Ã‰tape 5: RÃ©duction de la colinÃ©aritÃ© (seuil={correlation_threshold})...")
    
    try:
        binary_vars = [col for col in df.columns 
                       if pd.api.types.is_integer_dtype(df[col]) and col != "outcome"]
        
        if display_info:
            print(f"ðŸ”¢ Variables binaires candidates : {len(binary_vars)}")

        # ðŸ”§ CORRECTION PRINCIPALE: Gestion robuste de find_highly_correlated_groups
        if binary_vars:
            groups_corr = find_highly_correlated_groups(
                df[binary_vars], 
                threshold=correlation_threshold,
                protected_cols=protected_cols
            )
            
            # ðŸ”§ VALIDATION DU TYPE DE RETOUR
            if isinstance(groups_corr, list):
                # Si c'est une liste (ancien format), on l'adapte
                if display_info:
                    print("âš ï¸ Format de retour dÃ©tectÃ© comme liste - conversion en dictionnaire")
                groups_corr = {
                    "groups": groups_corr,
                    "to_drop": [],
                    "protected": protected_cols
                }
            elif not isinstance(groups_corr, dict):
                # Si ce n'est ni liste ni dict, erreur
                if display_info:
                    print(f"âš ï¸ Type de retour inattendu: {type(groups_corr)} - utilisation valeurs par dÃ©faut")
                groups_corr = {
                    "groups": [],
                    "to_drop": [],
                    "protected": protected_cols
                }
            elif "groups" not in groups_corr:
                # Si c'est un dict mais sans la clÃ© "groups"
                if display_info:
                    print("âš ï¸ ClÃ© 'groups' manquante - ajout de structure par dÃ©faut")
                groups_corr["groups"] = []
                if "to_drop" not in groups_corr:
                    groups_corr["to_drop"] = []
                if "protected" not in groups_corr:
                    groups_corr["protected"] = protected_cols
            
            # Stockage des informations de corrÃ©lation
            transform_objects['correlation_info'] = groups_corr
            
        else:
            # Pas de variables binaires Ã  analyser
            groups_corr = {
                "groups": [],
                "to_drop": [],
                "protected": protected_cols
            }
            if display_info:
                print("âš ï¸ Aucune variable binaire trouvÃ©e pour l'analyse de corrÃ©lation")

        target_col = "outcome" if "outcome" in df.columns and require_outcome else None

        # ðŸ›¡ï¸ Protection dans drop_correlated_duplicates
        df_reduced, dropped_cols, kept_cols = drop_correlated_duplicates(
            df=df,
            groups=groups_corr["groups"],  # âœ… Maintenant sÃ»r d'accÃ©der Ã  cette clÃ©
            target_col=target_col,
            extra_cols=mar_cols + mcar_cols,
            protected_cols=protected_cols,
            priority_cols=priority_cols,
            verbose=False,
            summary=display_info
        )
        
        # RÃ©assignation du DataFrame
        df = df_reduced
        
    except Exception as e:
        print(f"âŒ Erreur lors de la rÃ©duction de colinÃ©aritÃ© : {e}")
        if display_info:
            print("âš ï¸ Poursuite sans rÃ©duction de colinÃ©aritÃ©...")
    
    # ðŸ›¡ï¸ Validation X4 aprÃ¨s rÃ©duction colinÃ©aritÃ©
    if protect_x4:
        validate_x4_presence(df, "AprÃ¨s rÃ©duction colinÃ©aritÃ©", display_info)

    # ========================================================================
    # Ã‰TAPE 6: SUPPRESSION DES OUTLIERS (OPTIONNELLE)
    # ========================================================================
    
    if drop_outliers and target_col:
        if display_info:
            print(f"\nðŸŽ¯ Ã‰tape 6: Suppression des outliers...")
        
        try:
            df = detect_and_remove_outliers(
                df=df,
                columns=mar_cols,
                method='iqr',
                remove=True,
                verbose=display_info
            )
        except Exception as e:
            print(f"âŒ Erreur lors de la suppression des outliers : {e}")
            if display_info:
                print("âš ï¸ Poursuite sans suppression des outliers...")
        
        # ðŸ›¡ï¸ Validation X4 aprÃ¨s suppression outliers
        if protect_x4:
            validate_x4_presence(df, "AprÃ¨s suppression outliers", display_info)
    elif display_info:
        print(f"\nâ­ï¸ Ã‰tape 6: Suppression des outliers ignorÃ©e (drop_outliers={drop_outliers})")

    # ========================================================================
    # Ã‰TAPE 7: SUPPRESSION DES COLONNES DUPLIQUÃ‰ES
    # ========================================================================
    
    duplicate_cols = df.columns[df.columns.duplicated()].tolist()
    if duplicate_cols:
        if display_info:
            print(f"\nðŸ”„ Ã‰tape 7: Suppression des colonnes dupliquÃ©es...")
        
        # ðŸ›¡ï¸ VÃ©rifier qu'on ne supprime pas X4 par accident
        if 'X4' in duplicate_cols and protect_x4:
            print("ðŸ›¡ï¸ ALERTE: X4 dÃ©tectÃ©e comme dupliquÃ©e - protection activÃ©e")
            # Garder la premiÃ¨re occurrence de X4
            df = df.loc[:, ~df.columns.duplicated(keep='first')]
        else:
            df = df.loc[:, ~df.columns.duplicated()]
            
        if display_info:
            print(f"âš ï¸ Colonnes dupliquÃ©es dÃ©tectÃ©es : {duplicate_cols}")
            print(f"ðŸ”¹ Duplication supprimÃ©e : {df.shape}")
        
        # ðŸ›¡ï¸ Validation X4 aprÃ¨s suppression doublons
        if protect_x4:
            validate_x4_presence(df, "AprÃ¨s suppression doublons", display_info)
    elif display_info:
        print(f"\nâœ… Ã‰tape 7: Aucune colonne dupliquÃ©e dÃ©tectÃ©e")

    # ========================================================================
    # Ã‰TAPE 8: RÃ‰ORGANISATION FINALE DES COLONNES
    # ========================================================================
    
    if display_info:
        print(f"\nðŸ“Œ Ã‰tape 8: RÃ©organisation finale des colonnes...")
    
    try:
        # Colonnes actuelles
        current_cols = df.columns.tolist()
        
        # 1. Variables prioritaires (en premier)
        final_priority = [col for col in priority_cols if col in current_cols]
        
        # 2. Variable cible (aprÃ¨s les prioritaires)
        final_target = [target_col] if target_col and target_col in current_cols else []
        
        # 3. Toutes les autres colonnes (dans l'ordre actuel)
        final_others = [col for col in current_cols if col not in final_priority + final_target]
        
        # Ordre final : prioritaires â†’ cible â†’ reste
        final_order = final_priority + final_target + final_others
        
        # RÃ©organiser le DataFrame
        df = df[final_order]
        
        if display_info:
            print(f"ðŸ“Œ Ordre final : {final_priority} â†’ {final_target} â†’ {len(final_others)} autres")
            print(f"ðŸ“Œ PremiÃ¨res colonnes : {df.columns[:min(8, len(df.columns))].tolist()}")
    
    except Exception as e:
        print(f"âŒ Erreur lors de la rÃ©organisation : {e}")
        if display_info:
            print("âš ï¸ Poursuite avec ordre actuel...")

    # ========================================================================
    # Ã‰TAPE 9: VALIDATION FINALE
    # ========================================================================
    
    if display_info:
        print(f"\nðŸ” Ã‰tape 9: Validation finale...")
        print(f"âœ… Pipeline complet terminÃ© â€“ Dimensions finales : {df.shape}")
        
        # ðŸ›¡ï¸ Validation finale X4
        if protect_x4:
            final_status = validate_x4_presence(df, "VALIDATION FINALE", True)
            if not final_status:
                print("ðŸš¨ ERREUR CRITIQUE: X4 manquante en fin de pipeline !")
                raise ValueError("X4 a Ã©tÃ© perdue pendant le preprocessing !")

    # ========================================================================
    # Ã‰TAPE 10: SAUVEGARDE
    # ========================================================================
    
    if processed_data_dir:
        if display_info:
            print(f"\nðŸ’¾ Ã‰tape 10: Sauvegarde...")
        
        try:
            processed_data_dir = Path(processed_data_dir)
            processed_data_dir.mkdir(parents=True, exist_ok=True)
            suffix = f"{mar_method}{'_no_outliers' if drop_outliers else ''}"
            filename = f"final_dataset_{suffix}.parquet"
            df.to_parquet(processed_data_dir / filename, index=False)
            
            if display_info:
                print(f"ðŸ’¾ Sauvegarde Parquet : {processed_data_dir / filename}")
        except Exception as e:
            print(f"âŒ Erreur lors de la sauvegarde : {e}")
    elif display_info:
        print(f"\nâ­ï¸ Ã‰tape 10: Sauvegarde ignorÃ©e (processed_data_dir=None)")

    if display_info:
        print("\n" + "=" * 70)
        print("ðŸŽ‰ PIPELINE DE PRÃ‰TRAITEMENT TERMINÃ‰ AVEC SUCCÃˆS")
        print("=" * 70)

    # Retour selon les options
    if return_objects:
        return df, transform_objects
    else:
        return df


# ============================================================================
# 6. FONCTIONS UTILITAIRES POUR DATASETS EXISTANTS
# ============================================================================

def apply_full_preprocessing_to_existing(
    df: pd.DataFrame,
    **kwargs
) -> pd.DataFrame:
    """
    Applique le prÃ©traitement complet Ã  un DataFrame existant.
    
    Args:
        df: DataFrame Ã  prÃ©traiter
        **kwargs: Arguments Ã  passer Ã  prepare_final_dataset
        
    Returns:
        DataFrame prÃ©traitÃ©
        
    Note:
        Cette fonction sauvegarde temporairement le DataFrame et utilise prepare_final_dataset
    """
    import tempfile
    
    # Sauvegarde temporaire
    with tempfile.NamedTemporaryFile(suffix='.csv', delete=False) as tmp_file:
        df.to_csv(tmp_file.name, index=False)
        
        # Application du pipeline
        result = prepare_final_dataset(
            file_path=tmp_file.name,
            raw_data_dir=Path(tmp_file.name).parent,
            **kwargs
        )
        
        # Nettoyage
        Path(tmp_file.name).unlink()
        
    return result


def batch_process_datasets(
    datasets_dict: Dict[str, Union[pd.DataFrame, str, Path]],
    **kwargs
) -> Dict[str, pd.DataFrame]:
    """
    Traite plusieurs datasets en lot avec le pipeline complet.
    
    Args:
        datasets_dict: Dictionnaire {nom: DataFrame ou chemin vers fichier}
        **kwargs: Arguments Ã  passer Ã  prepare_final_dataset
        
    Returns:
        Dictionnaire des datasets traitÃ©s
    """
    processed_datasets = {}
    
    for name, data in datasets_dict.items():
        print(f"\nðŸ”„ Traitement de {name}...")
        
        try:
            if isinstance(data, pd.DataFrame):
                # DataFrame existant
                result = apply_full_preprocessing_to_existing(data, **kwargs)
            else:
                # Chemin vers fichier
                result = prepare_final_dataset(file_path=data, **kwargs)
            
            processed_datasets[name] = result
            print(f"âœ… {name} traitÃ© avec succÃ¨s : {result.shape}")
            
        except Exception as e:
            print(f"âŒ Erreur lors du traitement de {name} : {e}")
            processed_datasets[name] = None
    
    return processed_datasets


def validate_all_datasets(
    datasets_dict: Dict[str, pd.DataFrame],
    expected_cols: List[str] = None,
    protect_x4: bool = True
) -> Dict[str, Dict]:
    """
    Valide la qualitÃ© de plusieurs datasets.
    
    Args:
        datasets_dict: Dictionnaire des datasets Ã  valider
        expected_cols: Colonnes attendues en premier
        protect_x4: VÃ©rifier la prÃ©sence de X4
        
    Returns:
        Dictionnaire des rapports de validation
    """
    if expected_cols is None:
        expected_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4', 'outcome']
    
    validation_reports = {}
    
    for name, df in datasets_dict.items():
        if df is None:
            validation_reports[name] = {'status': 'error', 'message': 'DataFrame vide'}
            continue
        
        report = {
            'status': 'success',
            'shape': df.shape,
            'columns_order_correct': check_column_order(df, expected_cols, display_info=False),
            'has_x4': 'X4' in df.columns if protect_x4 else True,
            'has_outcome': 'outcome' in df.columns,
            'missing_values': df.isnull().sum().sum(),
            'first_columns': df.columns[:min(8, len(df.columns))].tolist()
        }
        
        # Score de qualitÃ© global
        quality_checks = [
            report['columns_order_correct'],
            report['has_x4'],
            report['has_outcome'],
            report['missing_values'] == 0
        ]
        report['quality_score'] = sum(quality_checks) / len(quality_checks)
        
        validation_reports[name] = report
    
    return validation_reports


def print_validation_summary(validation_reports: Dict[str, Dict]):
    """
    Affiche un rÃ©sumÃ© des validations.
    
    Args:
        validation_reports: Rapports de validation
    """
    print("\nðŸ“Š RÃ‰SUMÃ‰ DE LA VALIDATION DES DATASETS")
    print("=" * 60)
    
    for name, report in validation_reports.items():
        if report['status'] == 'error':
            print(f"âŒ {name}: {report['message']}")
            continue
        
        quality = report['quality_score']
        status_icon = "âœ…" if quality == 1.0 else "âš ï¸" if quality >= 0.75 else "âŒ"
        
        print(f"{status_icon} {name}:")
        print(f"   ðŸ“ Shape: {report['shape']}")
        print(f"   ðŸŽ¯ Score qualitÃ©: {quality:.2f}/1.0")
        print(f"   ðŸ“Œ Ordre colonnes: {'âœ…' if report['columns_order_correct'] else 'âŒ'}")
        print(f"   ðŸ›¡ï¸ X4 prÃ©sente: {'âœ…' if report['has_x4'] else 'âŒ'}")
        print(f"   ðŸŽ¯ Outcome prÃ©sente: {'âœ…' if report['has_outcome'] else 'âŒ'}")
        print(f"   ðŸ’§ Valeurs manquantes: {report['missing_values']}")
        print(f"   ðŸ“‹ PremiÃ¨res colonnes: {report['first_columns']}")
        print()


# ============================================================================
# 7. FONCTIONS DE DIAGNOSTIC ET DEBUG (AMÃ‰LIORÃ‰ES)
# ============================================================================

def diagnose_pipeline_issue(
    file_path: Union[str, Path],
    step_by_step: bool = True,
    **kwargs
) -> Dict[str, any]:
    """
    Diagnostique les problÃ¨mes potentiels dans le pipeline.
    
    Args:
        file_path: Chemin vers le fichier de donnÃ©es
        step_by_step: ExÃ©cuter Ã©tape par Ã©tape avec validation
        **kwargs: Arguments pour prepare_final_dataset
        
    Returns:
        Dictionnaire avec les rÃ©sultats de chaque Ã©tape
    """
    print("ðŸ” DIAGNOSTIC DU PIPELINE (VERSION AMÃ‰LIORÃ‰E)")
    print("=" * 60)
    
    results = {}
    
    try:
        # Test de chargement
        print("ðŸ“‚ Test de chargement...")
        df = load_data(file_path, display_info=False, encode_target=True)
        results['loading'] = {
            'status': 'success',
            'shape': df.shape,
            'has_x4': 'X4' in df.columns,
            'has_outcome': 'outcome' in df.columns,
            'columns': df.columns.tolist()
        }
        print(f"âœ… Chargement OK: {df.shape}")
        
        if step_by_step:
            # Test de chaque Ã©tape
            steps = [
                ('conversion_x4', lambda d: convert_X4_to_int(d, verbose=False)),
                ('yeo_johnson', lambda d: apply_yeojohnson(d, ['X1', 'X2', 'X3'])),
                ('correlation_analysis', lambda d: test_correlation_step(d)),
            ]
            
            for step_name, step_func in steps:
                try:
                    df = step_func(df)
                    results[step_name] = {
                        'status': 'success',
                        'shape': df.shape,
                        'has_x4': 'X4' in df.columns
                    }
                    print(f"âœ… {step_name} OK: {df.shape}")
                except Exception as e:
                    results[step_name] = {
                        'status': 'error',
                        'error': str(e)
                    }
                    print(f"âŒ {step_name} ERREUR: {e}")
                    break
        
    except Exception as e:
        results['loading'] = {
            'status': 'error',
            'error': str(e)
        }
        print(f"âŒ Chargement ERREUR: {e}")
    
    return results


def test_correlation_step(df: pd.DataFrame) -> pd.DataFrame:
    """Teste spÃ©cifiquement l'Ã©tape de corrÃ©lation."""
    binary_vars = [col for col in df.columns 
                   if pd.api.types.is_integer_dtype(df[col]) and col != "outcome"]
    
    if not binary_vars:
        print("âš ï¸ Aucune variable binaire pour test de corrÃ©lation")
        return df
    
    groups_corr = find_highly_correlated_groups(
        df[binary_vars], 
        threshold=0.90,
        protected_cols=['X4']
    )
    
    print(f"ðŸ”§ Test corrÃ©lation - Type retour: {type(groups_corr)}")
    print(f"ðŸ”§ Contenu: {groups_corr}")
    
    # Test d'accÃ¨s
    if isinstance(groups_corr, dict) and "groups" in groups_corr:
        print(f"âœ… AccÃ¨s groups rÃ©ussi: {len(groups_corr['groups'])} groupes")
    else:
        raise ValueError(f"Format retour incorrect: {type(groups_corr)}")
    
    return df


def quick_pipeline_test(file_path: Union[str, Path]) -> bool:
    """Test rapide du pipeline complet."""
    print("âš¡ TEST RAPIDE DU PIPELINE COMPLET")
    print("=" * 40)
    
    try:
        df_result = prepare_final_dataset(
            file_path=file_path,
            strategy="mixed_mar_mcar",
            mar_method="knn",
            correlation_threshold=0.90,
            drop_outliers=False,
            display_info=False
        )
        
        # Validations
        has_x4 = 'X4' in df_result.columns
        has_outcome = 'outcome' in df_result.columns
        no_missing = df_result.isnull().sum().sum() == 0
        
        print(f"âœ… Pipeline exÃ©cutÃ© avec succÃ¨s")
        print(f"ðŸ“Š Shape finale: {df_result.shape}")
        print(f"ðŸ›¡ï¸ X4 prÃ©sente: {'âœ…' if has_x4 else 'âŒ'}")
        print(f"ðŸŽ¯ Outcome prÃ©sente: {'âœ…' if has_outcome else 'âŒ'}")
        print(f"ðŸ’§ Pas de valeurs manquantes: {'âœ…' if no_missing else 'âŒ'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur dans le pipeline: {e}")
        return False


# ============================================================================
# 8. FONCTIONS D'EXPORT ET SAUVEGARDE AVANCÃ‰ES
# ============================================================================

def export_datasets_multiple_formats(
    datasets_dict: Dict[str, pd.DataFrame],
    output_dir: Union[str, Path],
    formats: List[str] = ['parquet', 'csv'],
    compress: bool = True
) -> Dict[str, Dict[str, Path]]:
    """
    Exporte les datasets dans plusieurs formats.
    
    Args:
        datasets_dict: Dictionnaire des datasets
        output_dir: Dossier de sortie
        formats: Formats d'export ('parquet', 'csv', 'xlsx')
        compress: Compresser les fichiers
        
    Returns:
        Dictionnaire des chemins de sauvegarde
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    export_paths = {}
    
    for dataset_name, df in datasets_dict.items():
        if df is None:
            continue
        
        export_paths[dataset_name] = {}
        
        for fmt in formats:
            try:
                if fmt == 'parquet':
                    file_path = output_dir / f"{dataset_name}.parquet"
                    df.to_parquet(file_path, index=False, compression='snappy' if compress else None)
                    
                elif fmt == 'csv':
                    file_path = output_dir / f"{dataset_name}.csv"
                    compression = 'gzip' if compress else None
                    if compression:
                        file_path = file_path.with_suffix('.csv.gz')
                    df.to_csv(file_path, index=False, compression=compression)
                    
                elif fmt == 'xlsx':
                    file_path = output_dir / f"{dataset_name}.xlsx"
                    df.to_excel(file_path, index=False)
                
                export_paths[dataset_name][fmt] = file_path
                print(f"ðŸ’¾ {dataset_name}.{fmt} sauvegardÃ©: {file_path}")
                
            except Exception as e:
                print(f"âŒ Erreur sauvegarde {dataset_name}.{fmt}: {e}")
    
    return export_paths


def create_preprocessing_report(
    datasets_dict: Dict[str, pd.DataFrame],
    validation_reports: Dict[str, Dict],
    output_path: Union[str, Path],
    transform_objects: Dict = None
) -> None:
    """
    CrÃ©e un rapport dÃ©taillÃ© du prÃ©traitement.
    
    Args:
        datasets_dict: Dictionnaire des datasets
        validation_reports: Rapports de validation
        output_path: Chemin de sauvegarde du rapport
        transform_objects: Objets de transformation utilisÃ©s
    """
    from datetime import datetime
    
    report_content = f"""
# RAPPORT DE PRÃ‰TRAITEMENT STA211 - VERSION CORRIGÃ‰E
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## RÃ©sumÃ©

- **Nombre de datasets traitÃ©s**: {len(datasets_dict)}
- **Datasets valides**: {sum(1 for r in validation_reports.values() if r.get('status') == 'success')}
- **Pipeline version**: 2.1 (CorrigÃ©e)

## Corrections apportÃ©es

- âœ… Fix TypeError dans find_highly_correlated_groups
- âœ… Validation robuste des types de retour
- âœ… Gestion d'erreurs amÃ©liorÃ©e Ã  chaque Ã©tape
- âœ… Protection X4 renforcÃ©e

## DÃ©tails par dataset

"""
    
    for name, report in validation_reports.items():
        if report['status'] == 'error':
            report_content += f"### âŒ {name}\n- **Statut**: Erreur\n- **Message**: {report['message']}\n\n"
            continue
        
        df = datasets_dict[name]
        report_content += f"""### âœ… {name}

- **Dimensions**: {report['shape']}
- **Score qualitÃ©**: {report['quality_score']:.2f}/1.0
- **X4 prÃ©sente**: {'âœ…' if report['has_x4'] else 'âŒ'}
- **Outcome prÃ©sente**: {'âœ…' if report['has_outcome'] else 'âŒ'}
- **Valeurs manquantes**: {report['missing_values']}
- **PremiÃ¨res colonnes**: {', '.join(report['first_columns'])}

#### Statistiques descriptives (premiÃ¨res variables)
```
{df[df.columns[:5]].describe().round(3).to_string()}
```

"""
    
    # Informations sur les transformations si disponibles
    if transform_objects:
        report_content += "\n## Objets de transformation\n\n"
        for obj_name, obj in transform_objects.items():
            if obj is not None:
                report_content += f"- **{obj_name}**: {type(obj).__name__}\n"
    
    # Sauvegarde du rapport
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"ðŸ“ Rapport sauvegardÃ©: {output_path}")
    except Exception as e:
        print(f"âŒ Erreur sauvegarde rapport: {e}")


# ============================================================================
# 9. FONCTIONS DE TEST ET VALIDATION
# ============================================================================

def run_comprehensive_test(file_path: Union[str, Path]) -> Dict:
    """Execute un test complet du pipeline avec diagnostic."""
    
    print("ðŸ§ª TEST COMPLET DU PIPELINE CORRIGÃ‰")
    print("=" * 50)
    
    # Test 1: Pipeline de base
    print("\n1ï¸âƒ£ Test pipeline de base...")
    basic_success = quick_pipeline_test(file_path)
    
    # Test 2: Pipeline avec objets
    print("\n2ï¸âƒ£ Test pipeline avec objets de transformation...")
    try:
        df_result, transform_objects = prepare_final_dataset(
            file_path=file_path,
            return_objects=True,
            display_info=False
        )
        objects_success = True
        print("âœ… Pipeline avec objets rÃ©ussi")
    except Exception as e:
        objects_success = False
        print(f"âŒ Pipeline avec objets Ã©chouÃ©: {e}")
        transform_objects = {}
    
    # Test 3: Diagnostic dÃ©taillÃ©
    print("\n3ï¸âƒ£ Diagnostic dÃ©taillÃ©...")
    diagnostic_results = diagnose_pipeline_issue(file_path, step_by_step=True)
    
    # Test 4: Validation X4
    print("\n4ï¸âƒ£ Validation protection X4...")
    x4_protected = True
    if basic_success:
        df_test = prepare_final_dataset(file_path, display_info=False)
        x4_protected = 'X4' in df_test.columns
        print(f"ðŸ›¡ï¸ X4 protÃ©gÃ©e: {'âœ…' if x4_protected else 'âŒ'}")
    
    # RÃ©sumÃ© final
    print("\nðŸ“Š RÃ‰SUMÃ‰ DU TEST COMPLET")
    print("=" * 30)
    print(f"Pipeline de base: {'âœ…' if basic_success else 'âŒ'}")
    print(f"Pipeline avec objets: {'âœ…' if objects_success else 'âŒ'}")
    print(f"Protection X4: {'âœ…' if x4_protected else 'âŒ'}")
    print(f"Diagnostic: {'âœ…' if all(r.get('status') == 'success' for r in diagnostic_results.values()) else 'âš ï¸'}")
    
    return {
        'basic_success': basic_success,
        'objects_success': objects_success,
        'x4_protected': x4_protected,
        'diagnostic_results': diagnostic_results,
        'transform_objects': transform_objects if objects_success else {}
    }


# ============================================================================
# 10. FONCTIONS D'UTILISATION SIMPLIFIÃ‰E
# ============================================================================

def prepare_dataset_safe(file_path: Union[str, Path], **kwargs) -> pd.DataFrame:
    """
    Version sÃ©curisÃ©e du pipeline avec gestion d'erreurs automatique.
    
    Args:
        file_path: Chemin vers le fichier
        **kwargs: Arguments pour prepare_final_dataset
        
    Returns:
        DataFrame prÃ©traitÃ©
    """
    print("ðŸ”’ PIPELINE SÃ‰CURISÃ‰ - VERSION CORRIGÃ‰E")
    print("=" * 45)
    
    # ParamÃ¨tres par dÃ©faut sÃ©curisÃ©s
    safe_defaults = {
        'strategy': 'mixed_mar_mcar',
        'mar_method': 'knn',
        'correlation_threshold': 0.90,
        'drop_outliers': False,
        'protect_x4': True,
        'display_info': True
    }
    
    # Fusion avec les paramÃ¨tres utilisateur
    final_params = {**safe_defaults, **kwargs}
    
    try:
        # Tentative normale
        df_result = prepare_final_dataset(file_path=file_path, **final_params)
        print("âœ… Pipeline exÃ©cutÃ© avec succÃ¨s en mode normal")
        return df_result
        
    except TypeError as e:
        if "list indices must be integers" in str(e) or "groups" in str(e):
            print("ðŸ”§ Erreur de corrÃ©lation dÃ©tectÃ©e - application du mode de rÃ©cupÃ©ration...")
            
            # Mode de rÃ©cupÃ©ration avec seuil plus strict
            recovery_params = final_params.copy()
            recovery_params['correlation_threshold'] = 0.95
            recovery_params['display_info'] = True
            
            try:
                df_result = prepare_final_dataset(file_path=file_path, **recovery_params)
                print("âœ… Pipeline exÃ©cutÃ© avec succÃ¨s en mode rÃ©cupÃ©ration")
                return df_result
            except Exception as e2:
                print(f"âŒ Ã‰chec en mode rÃ©cupÃ©ration: {e2}")
                raise
                
    except Exception as e:
        print(f"âŒ Erreur critique: {e}")
        print("ðŸ” Lancement du diagnostic...")
        
        # Diagnostic automatique
        diagnostic = diagnose_pipeline_issue(file_path, step_by_step=False)
        print("ðŸ“‹ Diagnostic terminÃ© - consultez les rÃ©sultats ci-dessus")
        raise


def get_preprocessing_summary(df: pd.DataFrame) -> Dict:
    """
    GÃ©nÃ¨re un rÃ©sumÃ© des caractÃ©ristiques du dataset prÃ©traitÃ©.
    
    Args:
        df: DataFrame prÃ©traitÃ©
        
    Returns:
        Dictionnaire avec le rÃ©sumÃ©
    """
    summary = {
        'shape': df.shape,
        'columns': df.columns.tolist(),
        'dtypes': df.dtypes.to_dict(),
        'missing_values': df.isnull().sum().to_dict(),
        'has_x4': 'X4' in df.columns,
        'has_outcome': 'outcome' in df.columns,
        'binary_vars': [],
        'continuous_vars': [],
        'memory_usage': df.memory_usage(deep=True).sum() / 1024**2  # MB
    }
    
    # Classification des variables
    for col in df.columns:
        if col == 'outcome':
            continue
        elif pd.api.types.is_integer_dtype(df[col]):
            summary['binary_vars'].append(col)
        elif pd.api.types.is_float_dtype(df[col]):
            summary['continuous_vars'].append(col)
    
    # Statistiques additionnelles
    if 'outcome' in df.columns:
        summary['class_distribution'] = df['outcome'].value_counts().to_dict()
        summary['class_balance'] = df['outcome'].value_counts(normalize=True).to_dict()
    
    return summary


def print_preprocessing_summary(df: pd.DataFrame):
    """Affiche un rÃ©sumÃ© formatÃ© du dataset prÃ©traitÃ©."""
    
    summary = get_preprocessing_summary(df)
    
    print("\nðŸ“Š RÃ‰SUMÃ‰ DU DATASET PRÃ‰TRAITÃ‰")
    print("=" * 40)
    
    print(f"ðŸ“ Dimensions: {summary['shape'][0]} lignes Ã— {summary['shape'][1]} colonnes")
    print(f"ðŸ’¾ MÃ©moire utilisÃ©e: {summary['memory_usage']:.2f} MB")
    print(f"ðŸ›¡ï¸ X4 prÃ©sente: {'âœ…' if summary['has_x4'] else 'âŒ'}")
    print(f"ðŸŽ¯ Outcome prÃ©sente: {'âœ…' if summary['has_outcome'] else 'âŒ'}")
    
    # Variables par type
    print(f"\nðŸ”¢ Variables binaires ({len(summary['binary_vars'])}): {summary['binary_vars'][:5]}{'...' if len(summary['binary_vars']) > 5 else ''}")
    print(f"ðŸ“ˆ Variables continues ({len(summary['continuous_vars'])}): {summary['continuous_vars'][:5]}{'...' if len(summary['continuous_vars']) > 5 else ''}")
    
    # Valeurs manquantes
    missing_count = sum(v for v in summary['missing_values'].values() if v > 0)
    if missing_count > 0:
        print(f"ðŸ’§ Valeurs manquantes: {missing_count} au total")
        missing_cols = {k: v for k, v in summary['missing_values'].items() if v > 0}
        for col, count in list(missing_cols.items())[:3]:
            print(f"   {col}: {count}")
    else:
        print("ðŸ’§ Valeurs manquantes: âœ… Aucune")
    
    # Distribution des classes
    if 'class_distribution' in summary:
        print(f"\nðŸŽ¯ Distribution des classes:")
        for classe, count in summary['class_distribution'].items():
            pct = summary['class_balance'][classe] * 100
            print(f"   Classe {classe}: {count} ({pct:.1f}%)")


# ============================================================================
# 11. FONCTIONS DE COMPATIBILITÃ‰ ET MIGRATION
# ============================================================================

def migrate_old_results(old_results_dir: Union[str, Path], new_results_dir: Union[str, Path]):
    """
    Migre les anciens rÃ©sultats vers le nouveau format.
    
    Args:
        old_results_dir: Dossier des anciens rÃ©sultats
        new_results_dir: Dossier pour les nouveaux rÃ©sultats
    """
    old_dir = Path(old_results_dir)
    new_dir = Path(new_results_dir)
    new_dir.mkdir(parents=True, exist_ok=True)
    
    print("ðŸ”„ MIGRATION DES ANCIENS RÃ‰SULTATS")
    print("=" * 40)
    
    # Recherche des fichiers Ã  migrer
    old_files = list(old_dir.glob("*.csv")) + list(old_dir.glob("*.parquet"))
    
    for old_file in old_files:
        try:
            print(f"ðŸ”„ Migration de {old_file.name}...")
            
            # Chargement
            if old_file.suffix == '.csv':
                df = pd.read_csv(old_file)
            else:
                df = pd.read_parquet(old_file)
            
            # Retraitement avec le nouveau pipeline
            if 'X1' in df.columns:  # Dataset brut
                df_new = apply_full_preprocessing_to_existing(df)
            else:  # Dataset dÃ©jÃ  traitÃ© - validation seulement
                df_new = df.copy()
                # Validation et correction si nÃ©cessaire
                if 'X4' not in df_new.columns:
                    print(f"âš ï¸ {old_file.name}: X4 manquante - fichier ignorÃ©")
                    continue
            
            # Sauvegarde dans le nouveau format
            new_file = new_dir / f"migrated_{old_file.stem}.parquet"
            df_new.to_parquet(new_file, index=False)
            
            print(f"âœ… {old_file.name} â†’ {new_file.name}")
            
        except Exception as e:
            print(f"âŒ Erreur migration {old_file.name}: {e}")
    
    print("ðŸŽ‰ Migration terminÃ©e")


def check_compatibility(df: pd.DataFrame) -> Dict:
    """
    VÃ©rifie la compatibilitÃ© d'un dataset avec le nouveau pipeline.
    
    Args:
        df: DataFrame Ã  vÃ©rifier
        
    Returns:
        Rapport de compatibilitÃ©
    """
    compatibility = {
        'version': '2.1',
        'compatible': True,
        'issues': [],
        'recommendations': []
    }
    
    # VÃ©rifications de base
    if 'X4' not in df.columns:
        compatibility['compatible'] = False
        compatibility['issues'].append("X4 manquante")
        compatibility['recommendations'].append("Retraiter avec le pipeline corrigÃ©")
    
    if 'outcome' not in df.columns:
        compatibility['issues'].append("Variable cible manquante")
        compatibility['recommendations'].append("VÃ©rifier l'encodage de la variable cible")
    
    # VÃ©rification des colonnes transformÃ©es
    expected_transformed = ['X1_trans', 'X2_trans', 'X3_trans']
    missing_transformed = [col for col in expected_transformed if col not in df.columns]
    if missing_transformed:
        compatibility['issues'].append(f"Colonnes transformÃ©es manquantes: {missing_transformed}")
        compatibility['recommendations'].append("Appliquer la transformation Yeo-Johnson")
    
    # VÃ©rification des valeurs manquantes
    missing_count = df.isnull().sum().sum()
    if missing_count > 0:
        compatibility['issues'].append(f"{missing_count} valeurs manquantes dÃ©tectÃ©es")
        compatibility['recommendations'].append("Appliquer l'imputation MICE/KNN")
    
    # VÃ©rification des doublons de colonnes
    duplicate_cols = df.columns[df.columns.duplicated()].tolist()
    if duplicate_cols:
        compatibility['issues'].append(f"Colonnes dupliquÃ©es: {duplicate_cols}")
        compatibility['recommendations'].append("Supprimer les doublons")
    
    return compatibility


def print_compatibility_report(compatibility: Dict):
    """Affiche un rapport de compatibilitÃ© formatÃ©."""
    
    print(f"\nðŸ” RAPPORT DE COMPATIBILITÃ‰ - VERSION {compatibility['version']}")
    print("=" * 50)
    
    if compatibility['compatible']:
        print("âœ… Dataset compatible avec le pipeline actuel")
    else:
        print("âŒ Dataset non compatible - corrections nÃ©cessaires")
    
    if compatibility['issues']:
        print(f"\nâš ï¸ ProblÃ¨mes dÃ©tectÃ©s ({len(compatibility['issues'])}):")
        for i, issue in enumerate(compatibility['issues'], 1):
            print(f"   {i}. {issue}")
    
    if compatibility['recommendations']:
        print(f"\nðŸ’¡ Recommandations ({len(compatibility['recommendations'])}):")
        for i, rec in enumerate(compatibility['recommendations'], 1):
            print(f"   {i}. {rec}")


def compare_preprocessing_results(
    file_path: Union[str, Path],
    configs: List[Dict],
    config_names: List[str] = None
) -> pd.DataFrame:
    """
    Compare les rÃ©sultats de diffÃ©rentes configurations de prÃ©traitement.
    
    Args:
        file_path: Chemin vers le fichier de donnÃ©es
        configs: Liste des configurations Ã  tester
        config_names: Noms des configurations
        
    Returns:
        DataFrame de comparaison
    """
    if config_names is None:
        config_names = [f"Config_{i+1}" for i in range(len(configs))]
    
    results = []
    
    for i, config in enumerate(configs):
        print(f"\nðŸ§ª Test configuration {config_names[i]}...")
        
        try:
            df_result = prepare_final_dataset(
                file_path=file_path,
                display_info=False,
                **config
            )
            
            result = {
                'config_name': config_names[i],
                'status': 'success',
                'final_shape': df_result.shape,
                'has_x4': 'X4' in df_result.columns,
                'has_outcome': 'outcome' in df_result.columns,
                'missing_values': df_result.isnull().sum().sum(),
                'first_5_columns': df_result.columns[:5].tolist()
            }
            
        except Exception as e:
            result = {
                'config_name': config_names[i],
                'status': 'error',
                'error': str(e),
                'final_shape': None,
                'has_x4': False,
                'has_outcome': False,
                'missing_values': None,
                'first_5_columns': None
            }
        
        results.append(result)
    
    return pd.DataFrame(results)


# ============================================================================
# FIN DU MODULE - INFORMATIONS DE VERSION
# ============================================================================

__version__ = "2.1"
__status__ = "CorrigÃ©"
__corrections__ = [
    "Fix TypeError dans find_highly_correlated_groups",
    "Validation robuste des types de retour",
    "Gestion d'erreurs amÃ©liorÃ©e",
    "Protection X4 renforcÃ©e",
    "Fonctions de diagnostic avancÃ©es",
    "Mode de rÃ©cupÃ©ration automatique"
]

def print_version_info():
    """Affiche les informations de version du module."""
    print(f"\nðŸ“‹ MODULE final_preprocessing.py")
    print(f"Version: {__version__}")
    print(f"Statut: {__status__}")
    print(f"Corrections apportÃ©es:")
    for correction in __corrections__:
        print(f"  âœ… {correction}")


if __name__ == "__main__":
    print_version_info()
    print(f"\nðŸš€ UTILISATION RECOMMANDÃ‰E:")
    print(f"   # Test rapide")
    print(f"   df = prepare_dataset_safe('data_train.csv')")
    print(f"   ")
    print(f"   # Test complet")
    print(f"   test_results = run_comprehensive_test('data_train.csv')")
    print(f"   ")
    print(f"   # Pipeline avec objets")
    print(f"   df, objects = prepare_final_dataset('data_train.csv', return_objects=True)")