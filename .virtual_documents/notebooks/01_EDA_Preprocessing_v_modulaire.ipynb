





## D√©finition des m√©tadonn√©es du projet

from datetime import datetime
from pathlib import Path

# M√©tadonn√©es du projet
PROJECT_NAME = "Projet STA 211: Internet Advertisements Classification"
DATASET_NAME = "Internet Advertisements Dataset"
AUTHOR = "Abdoullatuf"
DATE = datetime.now().strftime("%Y-%m-%d")  # Date dynamique
VERSION = "1.0"

# V√©rification des m√©tadonn√©es
metadata = {
    "Projet": PROJECT_NAME,
    "Dataset": DATASET_NAME,
    "Auteur": AUTHOR,
    "Date": DATE,
    "Version": VERSION
}

# Affichage des informations
print("üìã M√©tadonn√©es du projet")
print("="*60)
for key, value in metadata.items():
    if not isinstance(value, str):
        raise TypeError(f"La m√©tadonn√©e '{key}' doit √™tre une cha√Æne de caract√®res, re√ßu {type(value)}")
    print(f"{key}: {value}")

# Sauvegarde des m√©tadonn√©es dans un fichier (optionnel)
METADATA_DIR = Path("metadata")
METADATA_DIR.mkdir(exist_ok=True)
metadata_file = METADATA_DIR / f"metadata_v{VERSION}.txt"
with open(metadata_file, "w", encoding="utf-8") as f:
    for key, value in metadata.items():
        f.write(f"{key}: {value}\n")
print(f"‚úÖ M√©tadonn√©es sauvegard√©es dans : {metadata_file}")








## 2.1 Configuration de l'environnement <a id="configuration-environnement"></a>

# Installation des packages (d√©commenter si n√©cessaire)
# !pip install -q prince scikit-learn xgboost lightgbm imbalanced-learn

import sys
import os
from pathlib import Path
from IPython.display import Markdown, display
import warnings

# Configuration des warnings et de pandas
warnings.filterwarnings('ignore')

import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', '{:.4f}'.format)

# D√©tection de l'environnement
def detect_environment():
    """D√©tecte l'environnement d'ex√©cution"""
    try:
        import google.colab
        return "colab"
    except ImportError:
        return "local"

ENV = detect_environment()
print(f"üîß Environnement d√©tect√© : {ENV}")

# MONTAGE DE GOOGLE DRIVE EN PREMIER (si Colab)
if ENV == "colab":
    try:
        from google.colab import drive
        drive.mount('/content/drive', force_remount=True)
        print("‚úÖ Google Drive mont√© avec succ√®s")
    except Exception as e:
        print(f"‚ùå Erreur lors du montage de Google Drive : {e}")
        raise

# Configuration du chemin vers les modules
if ENV == "colab":
    module_path = Path("/content/drive/MyDrive/projet_sta211/modules")
else:
    module_path = Path("G:/Mon Drive/projet_sta211/modules")

# V√©rification que le chemin existe
if not module_path.exists():
    print(f"‚ùå Le chemin {module_path} n'existe pas!")
    print("   Cr√©ation du dossier modules...")
    module_path.mkdir(parents=True, exist_ok=True)

# Ajout au sys.path
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))
    print(f"‚úÖ Module path ajout√© : {module_path}")

# Import et ex√©cution de project_setup
try:
    from project_setup import setup_project_paths
    paths = setup_project_paths()
    print("‚úÖ Configuration des chemins r√©ussie")
except ImportError as e:
    print(f"‚ùå Erreur : Impossible d'importer project_setup")
    print(f"   Assurez-vous que le fichier project_setup.py existe dans : {module_path}")
    raise

# Conversion des Path objects en strings pour l'affichage
paths_str = {k: str(v) for k, v in paths.items()}

# Affichage format√© des chemins
def display_paths():
    """Affiche les chemins configur√©s dans un tableau format√©"""
    status_icons = {}
    for key, path in paths.items():
        if Path(path).exists():
            status_icons[key] = "‚úÖ"
        else:
            status_icons[key] = "‚ùå"

    paths_md = f"""
### üìÇ **Chemins configur√©s pour le projet**

| Status | Nom du dossier      | Chemin                                        |
|--------|---------------------|-----------------------------------------------|
| {status_icons.get("ROOT_DIR", "?")} | `ROOT_DIR`          | `{paths_str["ROOT_DIR"]}`                     |
| {status_icons.get("MODULE_DIR", "?")} | `MODULE_DIR`        | `{paths_str["MODULE_DIR"]}`                   |
| {status_icons.get("RAW_DATA_DIR", "?")} | `RAW_DATA_DIR`      | `{paths_str["RAW_DATA_DIR"]}`                 |
| {status_icons.get("DATA_PROCESSED", "?")} | `DATA_PROCESSED`    | `{paths_str["DATA_PROCESSED"]}`               |
| {status_icons.get("MODELS_DIR", "?")} | `MODELS_DIR`        | `{paths_str["MODELS_DIR"]}`                   |
| {status_icons.get("FIGURES_DIR", "?")} | `FIGURES_DIR`       | `{paths_str["FIGURES_DIR"]}`                  |

**L√©gende :** ‚úÖ = Existe | ‚ùå = N'existe pas
"""
    display(Markdown(paths_md))

# Affichage des chemins
display_paths()

# Informations syst√®me
print(f"\nüêç Python version : {sys.version.split()[0]}")
print(f"üìç Working directory : {os.getcwd()}")

# Variables globales pour un acc√®s facile
ROOT_DIR = paths["ROOT_DIR"]
MODULE_DIR = paths["MODULE_DIR"]
RAW_DATA_DIR = paths["RAW_DATA_DIR"]
DATA_PROCESSED = paths["DATA_PROCESSED"]
MODELS_DIR = paths["MODELS_DIR"]
FIGURES_DIR = paths["FIGURES_DIR"]





## 2.2 Import des biblioth√®ques <a id="import-des-bibliotheques"></a>

# Import du module centralis√©
from imports_sta211 import *

# D√©finition du RANDOM_STATE si non d√©fini dans le module
if 'RANDOM_STATE' not in globals():
    RANDOM_STATE = 42
    np.random.seed(RANDOM_STATE)

# V√©rification des imports critiques
required_modules = {
    'pandas': pd,
    'numpy': np,
    'sklearn': 'sklearn',
    'matplotlib': plt,
    'seaborn': sns
}

print("\nüîç V√©rification des modules critiques :")
for name, module in required_modules.items():
    if module is not None:
        print(f"  ‚úÖ {name}")
    else:
        print(f"  ‚ùå {name} - ERREUR!")

# Affichage de la configuration
print(f"\n‚öôÔ∏è Configuration :")
print(f"  - Random State : {RANDOM_STATE}")
print(f"  - Style matplotlib : seaborn-v0_8-whitegrid")
print(f"  - Palette seaborn : husl")
print(f"  - Nombre max de colonnes pandas : {pd.get_option('display.max_columns')}")
print(f"  - Warnings : Filtr√©s")

# Test rapide des imports
print(f"\nüìä Versions des biblioth√®ques principales :")
print(f"  - Pandas : {pd.__version__}")
print(f"  - NumPy : {np.__version__}")
print(f"  - Scikit-learn : {sklearn.__version__}")





## 2.3 Configuration des param√®tres du projet <a id="configuration-parametres-projet"></a>

# Import du module de configuration
import importlib
import project_config
importlib.reload(project_config)
from project_config import ProjectConfig, create_config

# Recr√©er la configuration avec les nouvelles valeurs
config = create_config(
    project_name=PROJECT_NAME,
    version=VERSION,
    author=AUTHOR,
    paths=paths
)

# Affichage de la (nouvelle) configuration
config.display_config()

# Affichage sp√©cifique pour l'optimisation F1
print("\nüéØ Configuration sp√©cifique pour l'optimisation F1 :")
print(f"  - Recherche de seuil optimal : {config.F1_OPTIMIZATION['threshold_search']}")
print(f"  - Plage de seuils : {config.F1_OPTIMIZATION['threshold_range']}")
print(f"  - Utilisation class_weight : {config.F1_OPTIMIZATION['use_class_weight']}")
print(f"  - SMOTE ratio : {config.F1_OPTIMIZATION['smote_ratio']}")

# Re-sauvegarde avec les nouvelles configurations
config_file = ROOT_DIR / "config" / f"project_config_v{VERSION}_f1.json"
config.save_config(config_file)

# Mise √† jour des variables globales
PROJECT_CONFIG = config.PROJECT_CONFIG
F1_OPTIMIZATION = config.F1_OPTIMIZATION
MODEL_CONFIG = config.MODEL_CONFIG





## 3.1 Chargement des jeux de donn√©es bruts <a id="chargement-des-jeux-de-donnees-bruts"></a>

from data_preprocessing import load_data
from pathlib import Path

# V√©rification de l‚Äôexistence de RAW_DATA_DIR
if 'RAW_DATA_DIR' not in globals():
    raise NameError("‚ùå RAW_DATA_DIR n‚Äôest pas d√©fini. V√©rifiez la configuration dans la section 2.1.")

# V√©rification que le dossier existe
raw_data_dir = Path(RAW_DATA_DIR)
if not raw_data_dir.exists():
    raise FileNotFoundError(f"‚ùå Dossier RAW_DATA_DIR introuvable : {raw_data_dir}")

# Chargement du fichier d‚Äô√©tude (data_train.csv)
print("üìÇ Chargement des donn√©es...")
df_study = load_data(
    file_path="data_train.csv",
    require_outcome=True,
    display_info=True,
    raw_data_dir=RAW_DATA_DIR,
    encode_target=True  # Encode outcome en 1 (ad.) et 0 (noad.)
)

# Chargement du fichier d‚Äô√©valuation (data_test.csv)
df_eval = load_data(
    file_path="data_test.csv",
    require_outcome=False,
    display_info=True,
    raw_data_dir=RAW_DATA_DIR,
    encode_target=False  # Pas de outcome dans data_test
)

# Renommer 'outcome' en 'y' pour coh√©rence
if 'outcome' in df_study.columns:
    df_study = df_study.rename(columns={'outcome': 'y'})
    print("\n‚úÖ Colonne 'outcome' renomm√©e en 'y'")
elif 'y' not in df_study.columns:
    raise ValueError("‚ùå Colonne 'y' ou 'outcome' manquante dans df_study")

# V√©rification des dimensions attendues
expected_train_shape = (2459, 1559)  # Selon le challenge
expected_test_shape = (820, 1558)
if df_study.shape != expected_train_shape:
    print(f"‚ö†Ô∏è Dimensions inattendues pour df_study : {df_study.shape} (attendu : {expected_train_shape})")
if df_eval.shape != expected_test_shape:
    print(f"‚ö†Ô∏è Dimensions inattendues pour df_eval : {df_eval.shape} (attendu : {expected_test_shape})")

# V√©rification des valeurs et du type de y
print("\nüîé Valeurs uniques de y :")
print(df_study['y'].unique())
print("\nüîé Type de y :")
print(df_study['y'].dtype)

# R√©sum√© des datasets
print(f"\nüìä R√©sum√© :")
print(f"  - Fichier d‚Äô√©tude : {df_study.shape}")
print(f"  - Fichier d‚Äô√©valuation : {df_eval.shape}")
print("\n‚úÖ Chargement termin√© !")






## 3.2 Inspection des colonnes et types <a id="inspection-des-colonnes-et-types"></a>

import pandas as pd
import numpy as np
from project_config import config  # Import de config (v√©rifier section 2.3)

# Analyse des types de donn√©es dans df_study
print("üîç Analyse des types de donn√©es...")
print("="*60)

# Types de donn√©es dans le fichier d‚Äô√©tude
print("\nüìä Types de donn√©es dans le fichier d‚Äô√©tude :")
type_counts = df_study.dtypes.value_counts()
for dtype, count in type_counts.items():
    print(f"  - {dtype}: {count} colonnes")

# Identification des colonnes par type
continuous_cols = ['X1', 'X2', 'X3']  # Selon le challenge
if 'X4' in df_study.columns and df_study['X4'].dtype == 'float64':
    continuous_cols.append('X4')  # Inclure X4 si float64
binary_cols = df_study.select_dtypes(include=['int64']).columns.tolist()
categorical_cols = df_study.select_dtypes(include=['object']).columns.tolist()

# V√©rifier que y est num√©rique, sinon encoder
if 'y' in df_study.columns and df_study['y'].dtype == 'object':
    if set(df_study['y'].unique()).issubset({'ad.', 'noad.'}):
        df_study['y'] = df_study['y'].map({'ad.': 1, 'noad.': 0})
        print("\n‚úÖ Colonne 'y' encod√©e en num√©rique (ad. -> 1, noad. -> 0)")
    else:
        raise ValueError(f"‚ùå Valeurs inattendues dans 'y': {df_study['y'].unique()}")

print(f"\nüìà Colonnes continues (float64) : {continuous_cols}")
print(f"üî¢ Nombre de colonnes binaires potentielles (int64) : {len(binary_cols)}")
print(f"üìù Colonnes cat√©gorielles (object) : {categorical_cols}")

# V√©rification des colonnes binaires (√©chantillon al√©atoire pour performance)
print("\nüîç V√©rification des valeurs uniques pour confirmer les colonnes binaires...")
np.random.seed(42)
sample_cols = np.random.choice(binary_cols, size=min(50, len(binary_cols)), replace=False)
non_binary_cols = []
for col in sample_cols:
    unique_values = df_study[col].unique()
    if len(unique_values) > 2 or not set(unique_values).issubset({0, 1}):
        non_binary_cols.append(col)

if non_binary_cols:
    print(f"‚ö†Ô∏è Colonnes non binaires d√©tect√©es : {non_binary_cols}")
else:
    print("‚úÖ Les colonnes int64 v√©rifi√©es semblent binaires (0/1)")

# Analyse de la variable cible
print("\nüéØ Variable cible 'y' :")
print(f"  - Type : {df_study['y'].dtype}")
print(f"  - Valeurs uniques : {sorted(df_study['y'].unique())}")
print(f"  - Distribution : \n{df_study['y'].value_counts().sort_index()}")

# Comparaison avec df_eval
print("\nüîÑ Comparaison avec le fichier d‚Äô√©valuation :")
eval_types = df_eval.dtypes.value_counts()
print("Types dans le fichier d‚Äô√©valuation :")
for dtype, count in eval_types.items():
    print(f"  - {dtype}: {count} colonnes")

# V√©rification de la coh√©rence des types
print("\nüîç V√©rification de la coh√©rence des types...")
type_mismatches = []
for col in df_eval.columns:
    if col in df_study.columns and col != 'y':  # Exclure y
        if df_study[col].dtype != df_eval[col].dtype:
            type_mismatches.append({
                'column': col,
                'study_type': df_study[col].dtype,
                'eval_type': df_eval[col].dtype
            })

if type_mismatches:
    print("‚ö†Ô∏è Diff√©rences de types d√©tect√©es :")
    for mismatch in type_mismatches:
        print(f"  - {mismatch['column']}: {mismatch['study_type']} (√©tude) vs {mismatch['eval_type']} (√©valuation)")
else:
    print("‚úÖ Les types de donn√©es sont coh√©rents entre les deux fichiers")

# Mise √† jour de la configuration
print("\nüíæ Mise √† jour de la configuration...")
try:
    config.update("COLUMN_CONFIG.CONTINUOUS_COLS", continuous_cols)
    config.update("COLUMN_CONFIG.BINARY_COLS", binary_cols)
    config.update("COLUMN_CONFIG.CATEGORICAL_COLS", categorical_cols)
    print("‚úÖ Configuration mise √† jour avec succ√®s")
except NameError:
    print("‚ö†Ô∏è Objet 'config' non d√©fini. V√©rifiez la section 2.3.")
except AttributeError:
    print("‚ö†Ô∏è M√©thode 'update' non disponible pour config. V√©rifiez project_config.")

# R√©sum√©
print("\nüìä R√©sum√© de la structure des donn√©es :")
print(f"  - Features continues : {len(continuous_cols)}")
print(f"  - Features binaires : {len(binary_cols) - (1 if 'y' in binary_cols else 0)}  # excluant 'y' si int64")
print(f"  - Features cat√©gorielles : {len(categorical_cols) - (1 if 'y' in categorical_cols else 0)}  # excluant 'y'")
print(f"  - Total features : {df_study.shape[1] - 1}  # excluant 'y'")


# Conversion de la variable cible en format num√©rique
print("üîÑ Conversion de la variable cible en format num√©rique...")
df_study['y'] = df_study['y'].map({'ad.': 1, 'noad.': 0})
print("‚úÖ Conversion termin√©e : 'ad.' ‚Üí 1, 'noad.' ‚Üí 0")
print(f"Nouvelle distribution :\n{df_study['y'].value_counts().sort_index()}")





## 3.3 Distribution de la variable cible <a id="distribution-variable-cible"></a>

print("\nüéØ Analyse de la distribution de la variable cible")
print("="*60)

# Distribution des classes
target_counts = df_study['y'].value_counts().sort_index()
target_pct = df_study['y'].value_counts(normalize=True).sort_index() * 100

print("\nüìä Distribution de la variable cible (y) :")
print(f"  - Classe 0 (noad.) : {target_counts[0]:,} ({target_pct[0]:.1f}%)")
print(f"  - Classe 1 (ad.)   : {target_counts[1]:,} ({target_pct[1]:.1f}%)")

# Ratio de d√©s√©quilibre
imbalance_ratio = target_counts[0] / target_counts[1]
print(f"\nüìà Ratio de d√©s√©quilibre : {imbalance_ratio:.2f}:1")
print(f"   ‚Üí Pour chaque publicit√©, il y a {imbalance_ratio:.1f} non-publicit√©s")

# Visualisation
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# Barplot
target_counts.plot(kind='bar', ax=ax1, color=['#3498db', '#e74c3c'])
ax1.set_title('Distribution des classes', fontsize=14)
ax1.set_xlabel('Classe')
ax1.set_ylabel('Nombre d\'√©chantillons')
ax1.set_xticklabels(['Non-publicit√© (0)', 'Publicit√© (1)'], rotation=0)

# Pie chart
target_pct.plot(kind='pie', ax=ax2, colors=['#3498db', '#e74c3c'],
                autopct='%1.1f%%', startangle=90)
ax2.set_title('Proportion des classes', fontsize=14)
ax2.set_ylabel('')

plt.tight_layout()
plt.savefig(FIGURES_DIR / 'eda' / 'target_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# Impact sur la strat√©gie de mod√©lisation
print("\nüí° Implications pour la mod√©lisation :")
print(f"  - Dataset fortement d√©s√©quilibr√© ({imbalance_ratio:.1f}:1)")
print("  - Strat√©gies recommand√©es :")
print("    ‚Ä¢ Utiliser stratify=True lors du train/test split")
print("    ‚Ä¢ Appliquer SMOTE ou class_weight='balanced'")
print("    ‚Ä¢ Optimiser pour F1-score (m√©trique du challenge)")
print("    ‚Ä¢ Consid√©rer un ensemble de mod√®les")

# Calcul du F1-score baseline
baseline_f1 = 2 * (1 * target_pct[1]/100) / (1 + target_pct[1]/100)
print(f"\nüìä F1-score baseline (pr√©dire toujours 'ad.') : {baseline_f1:.3f}")
print(f"   ‚Üí Nos mod√®les devront d√©passer ce score pour √™tre utiles")





# 4. Analyse exploratoire <a id="analyse-exploratoire"></a>
## 4.1 Analyse des valeurs manquantes <a id="analyse-des-valeurs-manquantes"></a>

print("üîç Analyse des valeurs manquantes")
print("="*60)

# Utilisation de la fonction du module
from data_preprocessing import analyze_missing_values

print("\nüìä Analyse globale des valeurs manquantes :")
missing_stats = analyze_missing_values(df_study)

# Analyse d√©taill√©e pour les colonnes continues
print("\nüìà D√©tail des valeurs manquantes pour les variables continues :")
for col in continuous_cols:
    missing_count = df_study[col].isnull().sum()
    missing_pct = (missing_count / len(df_study)) * 100
    print(f"  - {col}: {missing_count} ({missing_pct:.2f}%)")

# Visualisation des patterns de valeurs manquantes
if missing_stats['total_missing'] > 0:
    # Heatmap des valeurs manquantes pour les colonnes avec des NaN
    cols_with_missing = [col for col in df_study.columns if df_study[col].isnull().sum() > 0]

    if len(cols_with_missing) > 0:
        plt.figure(figsize=(10, 5))

        # Cr√©er une matrice binaire des valeurs manquantes
        missing_matrix = df_study[cols_with_missing].isnull().astype(int)

        # Heatmap
        sns.heatmap(missing_matrix.T, cmap='RdYlBu', cbar_kws={'label': 'Manquant (1) / Pr√©sent (0)'})
        plt.title('Pattern des valeurs manquantes', fontsize=14)
        plt.xlabel('√âchantillons')
        plt.ylabel('Variables')
        plt.tight_layout()
        plt.savefig(FIGURES_DIR / 'eda' / 'missing_values_pattern.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Analyse du pattern MAR vs MCAR
        print("\nüîç Analyse du type de valeurs manquantes (MAR vs MCAR) :")

        # Corr√©lation entre les valeurs manquantes et la cible
        for col in cols_with_missing:
            missing_indicator = df_study[col].isnull().astype(int)
            correlation_with_target = missing_indicator.corr(df_study['y'])
            print(f"  - {col}: corr√©lation avec y = {correlation_with_target:.3f}")

            if abs(correlation_with_target) > 0.1:
                print(f"    ‚Üí Potentiellement MAR (Missing At Random)")
            else:
                print(f"    ‚Üí Potentiellement MCAR (Missing Completely At Random)")
else:
    print("\n‚úÖ Aucune valeur manquante d√©tect√©e dans le dataset !")

# Analyse pour le fichier d'√©valuation aussi
print("\nüìä Analyse des valeurs manquantes dans le fichier d'√©valuation :")
missing_stats_eval = analyze_missing_values(df_eval)

# Comparaison des patterns
if missing_stats['total_missing'] > 0 or missing_stats_eval['total_missing'] > 0:
    print("\nüîÑ Comparaison des patterns de valeurs manquantes :")
    print(f"  - Fichier d'√©tude : {missing_stats['percent_missing']:.2f}% manquant")
    print(f"  - Fichier d'√©valuation : {missing_stats_eval['percent_missing']:.2f}% manquant")

    # Strat√©gie d'imputation recommand√©e
    print("\nüí° Strat√©gie d'imputation recommand√©e :")
    if 'X4' in missing_stats['cols_missing']:
        x4_missing_pct = missing_stats['percent_per_col'].get('X4', 0)
        if x4_missing_pct < 5:
            print(f"  - X4 ({x4_missing_pct:.1f}% manquant) : Imputation par la m√©diane")

    mar_cols = ['X1', 'X2', 'X3']
    mar_missing = any(col in missing_stats['cols_missing'] for col in mar_cols)
    if mar_missing:
        print(f"  - X1, X2, X3 (variables continues) : KNN ou MICE (imputation multivari√©e)")


# Correction du type et imputation de X4
print("\nüîß Correction du type de X4...")
print(f"Valeurs uniques de X4 (avant correction) : {sorted(df_study['X4'].dropna().unique())}")
print(f"Type actuel : {df_study['X4'].dtype}")

# V√©rifier que X4 ne contient que 0 et 1
unique_values = df_study['X4'].dropna().unique()
if set(unique_values).issubset({0.0, 1.0}):
    # Imputer d'abord les valeurs manquantes par la m√©diane
    X4_median = df_study['X4'].median()
    df_study['X4'] = df_study['X4'].fillna(X4_median)
    df_eval['X4'] = df_eval['X4'].fillna(X4_median)

    # Convertir en int
    df_study['X4'] = df_study['X4'].astype(int)
    df_eval['X4'] = df_eval['X4'].astype(int)

    print(f"‚úÖ X4 converti en int64 apr√®s imputation par la m√©diane ({X4_median})")
    print(f"Nouveau type : {df_study['X4'].dtype}")

    # Mettre √† jour la configuration
    config.update("COLUMN_CONFIG.CONTINUOUS_COLS", ['X1', 'X2', 'X3'])
    config.update("COLUMN_CONFIG.BINARY_COLS", ['X4'] + binary_cols)
    continuous_cols = ['X1', 'X2', 'X3']  # Mise √† jour locale
else:
    print("‚ö†Ô∏è X4 contient des valeurs autres que 0 et 1, conservation en float64")

# R√©sum√© final
print("\nüìä R√©sum√© des valeurs manquantes apr√®s traitement de X4 :")
print(f"  - X1, X2, X3 : ~27% manquant ‚Üí √Ä traiter avec KNN/MICE")
print(f"  - X4 : Imput√© et converti en binaire")
print(f"  - Pattern MAR d√©tect√© pour X1, X2, X3 (corr√©lation avec y ‚âà -0.10)")
print(f"  - Les patterns sont coh√©rents entre fichiers d'√©tude et d'√©valuation")





## 4.2 Analyse statistique des variables quantitatives <a id="analyse-statistique-des-variables-quantitatives"></a>

print("üìä Analyse statistique des variables quantitatives")
print("="*60)

# Variables continues √† analyser (X1, X2, X3 uniquement maintenant)
print(f"\nüìà Variables continues √† analyser : {continuous_cols}")

# Statistiques descriptives
print("\nüìä Statistiques descriptives :")
stats_df = df_study[continuous_cols].describe()
display(stats_df)

# Analyse de la distribution
print("\nüìä Analyse de la distribution :")
for col in continuous_cols:
    data = df_study[col].dropna()
    print(f"\n{col}:")
    print(f"  - Skewness (asym√©trie) : {stats.skew(data):.3f}")
    print(f"  - Kurtosis (aplatissement) : {stats.kurtosis(data):.3f}")

    # Test de normalit√©
    stat, p_value = stats.shapiro(data.sample(min(5000, len(data))))  # Limit√© √† 5000 pour Shapiro
    print(f"  - Test de Shapiro-Wilk : p-value = {p_value:.4f}")
    if p_value < 0.01:
        print(f"    ‚Üí Distribution non normale (n√©cessite transformation)")
    else:
        print(f"    ‚Üí Distribution approximativement normale")

# Visualisation des distributions
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i, col in enumerate(continuous_cols):
    data = df_study[col].dropna()

    # Histogramme avec KDE
    ax = axes[i]
    data.hist(bins=50, ax=ax, alpha=0.7, color='skyblue', edgecolor='black')
    ax2 = ax.twinx()
    data.plot(kind='kde', ax=ax2, color='red', linewidth=2)
    ax.set_title(f'Distribution de {col}', fontsize=12)
    ax.set_xlabel(col)
    ax.set_ylabel('Fr√©quence')
    ax2.set_ylabel('Densit√©')

    # Box plot
    ax = axes[i+3]
    df_study.boxplot(column=col, ax=ax)
    ax.set_title(f'Box plot de {col}', fontsize=12)
    ax.set_ylabel('Valeur')

plt.tight_layout()
plt.savefig(FIGURES_DIR / 'eda' / 'continuous_distributions.png', dpi=300, bbox_inches='tight')
plt.show()

# Analyse des outliers
print("\nüîç D√©tection des outliers (m√©thode IQR) :")
outliers_summary = {}
for col in continuous_cols:
    data = df_study[col].dropna()
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data < lower_bound) | (data > upper_bound)]
    outliers_pct = len(outliers) / len(data) * 100

    outliers_summary[col] = {
        'count': len(outliers),
        'percentage': outliers_pct,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound
    }

    print(f"\n{col}:")
    print(f"  - Limites : [{lower_bound:.2f}, {upper_bound:.2f}]")
    print(f"  - Outliers : {len(outliers)} ({outliers_pct:.2f}%)")

# Corr√©lation avec la variable cible
print("\nüéØ Corr√©lation avec la variable cible (y) :")
correlations = {}
for col in continuous_cols:
    # Calculer la corr√©lation seulement pour les valeurs non manquantes
    mask = df_study[col].notna()
    corr = df_study.loc[mask, col].corr(df_study.loc[mask, 'y'])
    correlations[col] = corr
    print(f"  - {col}: {corr:.4f}")

# Matrice de corr√©lation entre variables continues
print("\nüìä Matrice de corr√©lation entre variables continues :")
corr_matrix = df_study[continuous_cols].corr()
display(corr_matrix)

# Heatmap de corr√©lation
plt.figure(figsize=(6, 4))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Matrice de corr√©lation des variables continues', fontsize=14)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'eda' / 'continuous_correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# R√©sum√© et recommandations
print("\nüí° R√©sum√© et recommandations :")
print("  - Les trois variables continues montrent des distributions fortement asym√©triques")
print("  - Transformation Yeo-Johnson recommand√©e pour normaliser les distributions")
print(f"  - Outliers d√©tect√©s : {sum(info['count'] for info in outliers_summary.values())} au total")
print("  - Corr√©lations faibles avec la cible, mais potentiellement utiles apr√®s transformation")








## 4.3 Distribution des variables binaires <a id="distribution-des-variables-binaires"></a>

print("üî¢ Analyse de la distribution des variables binaires")
print("="*60)

# Variables binaires (incluant X4 maintenant)
binary_cols = [col for col in df_study.columns if col.startswith('X') and col not in continuous_cols]
print(f"\nüìä Nombre total de variables binaires : {len(binary_cols)}")

# Calcul du taux de pr√©sence pour chaque variable
presence_rates = {}
for col in binary_cols:
    presence_rate = (df_study[col] == 1).sum() / len(df_study) * 100
    presence_rates[col] = presence_rate

presence_series = pd.Series(presence_rates)

# Statistiques de base
print(f"\nüìä Statistiques des taux de pr√©sence :")
print(f"  - Moyenne : {presence_series.mean():.2f}%")
print(f"  - M√©diane : {presence_series.median():.2f}%")
print(f"  - Min : {presence_series.min():.2f}%")
print(f"  - Max : {presence_series.max():.2f}%")

# Sparsit√© globale
total_values = len(df_study) * len(binary_cols)
total_ones = sum(df_study[col].sum() for col in binary_cols)
sparsity = (1 - total_ones / total_values) * 100
print(f"\nüìä Sparsit√© globale : {sparsity:.2f}% de z√©ros")

# Visualisation simple
plt.figure(figsize=(10, 5))
presence_series.hist(bins=50, color='skyblue', edgecolor='black')
plt.xlabel('Taux de pr√©sence (%)')
plt.ylabel('Nombre de variables')
plt.title('Distribution des taux de pr√©sence des variables binaires')
plt.axvline(presence_series.mean(), color='red', linestyle='--', label=f'Moyenne: {presence_series.mean():.1f}%')
plt.legend()
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'eda' / 'binary_presence_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n‚úÖ Analyse des variables binaires termin√©e")
print("   ‚Üí Dataset tr√®s sparse, adapt√© pour des m√©thodes de s√©lection de features")





## 4.4 Analyse des corr√©lations <a id="analyse-des-correlations"></a>

print("üîó Analyse des corr√©lations")
print("="*60)

# S√©lection d'un √©chantillon de variables pour l'analyse (trop de variables pour tout analyser)
print("\nüìä S√©lection des variables pour l'analyse des corr√©lations...")

# Strat√©gie : prendre les variables avec diff√©rents taux de pr√©sence
presence_series = pd.Series(presence_rates)
quartiles = presence_series.quantile([0.25, 0.5, 0.75])

# S√©lectionner des variables de chaque quartile
vars_q1 = presence_series[presence_series <= quartiles[0.25]].sample(10, random_state=42).index.tolist()
vars_q2 = presence_series[(presence_series > quartiles[0.25]) & (presence_series <= quartiles[0.5])].sample(10, random_state=42).index.tolist()
vars_q3 = presence_series[(presence_series > quartiles[0.5]) & (presence_series <= quartiles[0.75])].sample(10, random_state=42).index.tolist()
vars_q4 = presence_series[presence_series > quartiles[0.75]].sample(10, random_state=42).index.tolist()

selected_vars = continuous_cols + vars_q1 + vars_q2 + vars_q3 + vars_q4
print(f"  - Variables s√©lectionn√©es : {len(selected_vars)} (3 continues + 40 binaires)")

# Calcul de la matrice de corr√©lation
print("\nüìä Calcul de la matrice de corr√©lation...")
corr_matrix = df_study[selected_vars + ['y']].corr()

# Corr√©lations avec la variable cible
target_corr = corr_matrix['y'].drop('y').sort_values(ascending=False)
print("\nüéØ Top 10 corr√©lations avec la variable cible (y) :")
for var, corr in target_corr.head(10).items():
    print(f"  - {var}: {corr:.4f}")

print("\nüéØ Bottom 10 corr√©lations avec la variable cible (y) :")
for var, corr in target_corr.tail(10).items():
    print(f"  - {var}: {corr:.4f}")

# Visualisation de la matrice de corr√©lation
plt.figure(figsize=(8, 6))
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', center=0,
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8},
            vmin=-0.5, vmax=0.5)
plt.title('Matrice de corr√©lation (√©chantillon de variables)', fontsize=14)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'eda' / 'correlation_matrix_sample.png', dpi=300, bbox_inches='tight')
plt.show()

# Analyse des corr√©lations entre features
print("\nüîç Analyse des corr√©lations entre features :")
# Extraire la partie triangulaire sup√©rieure (sans la diagonale)
upper_triangle = corr_matrix.where(np.triu(np.ones_like(corr_matrix, dtype=bool), k=1))
high_corr_pairs = []

for i in range(len(upper_triangle.columns)):
    for j in range(i):
        if abs(upper_triangle.iloc[i, j]) > 0.8:  # Seuil de corr√©lation √©lev√©e
            high_corr_pairs.append({
                'var1': upper_triangle.columns[i],
                'var2': upper_triangle.columns[j],
                'corr': upper_triangle.iloc[i, j]
            })

if high_corr_pairs:
    print(f"\n‚ö†Ô∏è Paires de variables hautement corr√©l√©es (|r| > 0.8) : {len(high_corr_pairs)}")
    for pair in high_corr_pairs[:5]:  # Afficher les 5 premi√®res
        print(f"  - {pair['var1']} vs {pair['var2']}: {pair['corr']:.3f}")
else:
    print("\n‚úÖ Aucune paire de variables avec corr√©lation √©lev√©e (|r| > 0.8)")

# R√©sum√©
print("\nüí° R√©sum√© de l'analyse des corr√©lations :")
print(f"  - Corr√©lations avec la cible g√©n√©ralement faibles (max: {abs(target_corr).max():.3f})")
print(f"  - Variables continues montrent des corr√©lations n√©gatives avec y")
print(f"  - Peu de corr√©lations √©lev√©es entre features ‚Üí pas de multicollin√©arit√© majeure")
print(f"  - Dataset sparse ‚Üí corr√©lations naturellement faibles")





## 4.5 Analyse approfondie des corr√©lations (binaires et continues)
print("\nüîó Analyse approfondie des corr√©lations entre variables...")

# Import de la fonction depuis le module
from eda_module_sta211 import bivariate_analysis

# Pr√©parer les donn√©es avec le bon format de colonne cible
df_bivariate = df_study.copy()
df_bivariate['outcome'] = df_bivariate['y'].map({0: 'noad.', 1: 'ad.'})

# Appeler la fonction bivariate_analysis
print("\nüìä Analyse bivari√©e compl√®te...")
corr_df, high_corr_pairs, binary_corr_pairs = bivariate_analysis(
    data=df_bivariate,
    use_transformed=False,  # Pas de variables transform√©es pour l'instant
    display_correlations=True,
    top_n=20,  # Afficher plus de variables
    show_plot=True
)

# Analyse des r√©sultats pour les variables continues
print(f"\nüìà Variables continues fortement corr√©l√©es entre elles (|r| > 0.9):")
if high_corr_pairs:
    for var1, var2, corr in high_corr_pairs:
        print(f"  - {var1} ‚Üî {var2}: r = {corr:.3f}")
else:
    print("  ‚úÖ Aucune paire de variables continues avec |r| > 0.9")

# Analyse des r√©sultats pour les variables binaires
print(f"\nüî¢ Variables binaires fortement corr√©l√©es entre elles (|r| > 0.95):")
print(f"  - Nombre total de paires: {len(binary_corr_pairs)}")

if binary_corr_pairs:
    # Analyser la distribution des corr√©lations
    corr_values = [abs(corr) for _, _, corr in binary_corr_pairs]
    print(f"  - Corr√©lation moyenne: {np.mean(corr_values):.3f}")
    print(f"  - Corr√©lation maximale: {max(corr_values):.3f}")
    
    # Afficher quelques exemples
    print("\n  Exemples de paires fortement corr√©l√©es:")
    for i, (var1, var2, corr) in enumerate(binary_corr_pairs[:5]):
        print(f"    {i+1}. {var1} ‚Üî {var2}: r = {corr:.3f}")
    
    if len(binary_corr_pairs) > 5:
        print(f"    ... et {len(binary_corr_pairs) - 5} autres paires")
    
    # Visualisation de la distribution des corr√©lations √©lev√©es
    plt.figure(figsize=(10, 5))
    
    plt.subplot(1, 2, 1)
    plt.hist([abs(corr) for _, _, corr in binary_corr_pairs], bins=20, color='coral', edgecolor='black')
    plt.xlabel('|Corr√©lation|')
    plt.ylabel('Nombre de paires')
    plt.title(f'Distribution des {len(binary_corr_pairs)} paires fortement corr√©l√©es')
    plt.axvline(0.95, color='red', linestyle='--', label='Seuil = 0.95')
    plt.legend()
    
    # Identification des variables impliqu√©es dans le plus de paires
    var_counts = {}
    for var1, var2, _ in binary_corr_pairs:
        var_counts[var1] = var_counts.get(var1, 0) + 1
        var_counts[var2] = var_counts.get(var2, 0) + 1
    
    # Top 10 des variables les plus redondantes
    top_redundant = sorted(var_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    
    plt.subplot(1, 2, 2)
    vars_names = [var for var, _ in top_redundant]
    counts = [count for _, count in top_redundant]
    plt.barh(vars_names, counts, color='skyblue')
    plt.xlabel('Nombre de corr√©lations √©lev√©es')
    plt.title('Top 10 variables les plus redondantes')
    
    plt.tight_layout()
    save_fig('binary_correlations_analysis.png', directory=FIGURES_DIR / 'eda', figsize=(10, 5))
    
    # Recommandations
    print("\n‚ö†Ô∏è Recommandations bas√©es sur l'analyse des corr√©lations:")
    print(f"  1. {len(binary_corr_pairs)} paires de variables binaires sont fortement corr√©l√©es (|r| > 0.95)")
    print(f"  2. Environ {len(var_counts)} variables sont impliqu√©es dans ces corr√©lations")
    print("  3. Une r√©duction de dimensionnalit√© est FORTEMENT recommand√©e avant la mod√©lisation")
    print("  4. Options sugg√©r√©es:")
    print("     - S√©lection de features par importance (Random Forest)")
    print("     - Suppression des variables redondantes")
    print("     - PCA ou autre m√©thode de r√©duction")
    
else:
    print("  ‚úÖ Aucune paire de variables binaires avec |r| > 0.95")
    print("     (Ce qui est surprenant avec 1555 variables binaires!)")

# Sauvegarder les r√©sultats importants
print("\nüíæ Sauvegarde des r√©sultats de l'analyse bivari√©e...")
corr_results = {
    'top_correlations_with_target': corr_df.head(50).to_dict(),
    'n_high_corr_continuous': len(high_corr_pairs),
    'n_high_corr_binary': len(binary_corr_pairs),
    'redundant_variables': list(var_counts.keys()) if binary_corr_pairs else []
}

import json
results_path = ROOT_DIR / "results" / "bivariate_analysis_results.json"
results_path.parent.mkdir(exist_ok=True)
with open(results_path, 'w') as f:
    json.dump(corr_results, f, indent=2)
print(f"  ‚úÖ R√©sultats sauvegard√©s dans: {results_path}")





## 4.6 Visualisations exploratoires <a id="visualisations-exploratoires"></a>

print("üìä Visualisations exploratoires")
print("="*60)

# Import des fonctions du module
from eda_module_sta211 import (
    compare_visualization_methods,
    analyze_feature_importance,
    save_fig
)

# 1. Visualisation des variables continues par classe
print("\nüìà Distribution des variables continues par classe...")
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for i, col in enumerate(continuous_cols):
    df_study_clean = df_study[[col, 'y']].dropna()
    df_study_clean['y_label'] = df_study_clean['y'].map({0: 'Non-pub', 1: 'Pub'})

    sns.violinplot(data=df_study_clean, x='y_label', y=col, ax=axes[i])
    axes[i].set_title(f'Distribution de {col} par classe')
    axes[i].set_xlabel('Classe')

plt.tight_layout()
save_fig('continuous_by_class.png', directory=FIGURES_DIR / 'eda', figsize=(15, 5))

# 2. Pr√©paration des donn√©es pour les visualisations multidimensionnelles
print("\nüîç Pr√©paration des donn√©es pour les visualisations...")
# Convertir 'y' en format texte pour le module
df_study_viz = df_study.copy()
df_study_viz['outcome'] = df_study_viz['y'].map({0: 'noad.', 1: 'ad.'})

# S√©lectionner un √©chantillon de features pour la visualisation
# Utiliser les variables continues + les variables binaires les plus corr√©l√©es
important_features = continuous_cols + list(target_corr.abs().nlargest(30).index)
df_sample = df_study_viz[important_features + ['outcome']].dropna()

# 3. Comparaison des m√©thodes de visualisation (UMAP, t-SNE, PCA)
print("\nüìä Comparaison des m√©thodes de r√©duction de dimension...")
try:
    compare_visualization_methods(df_sample, target_col='outcome')
except Exception as e:
    print(f"‚ö†Ô∏è Erreur lors de la comparaison : {e}")
    print("  Installation de packages manquants peut √™tre n√©cessaire:")
    print("  !pip install umap-learn prince")

# 4. Analyse de l'importance des features
print("\nüå≤ Analyse de l'importance des features...")
try:
    # Pr√©parer les donn√©es pour l'analyse
    df_importance = df_study_viz[important_features + ['outcome']].dropna()

    # Analyser l'importance
    importance_results = analyze_feature_importance(
        df_importance,
        target_col='outcome',
        include_enhanced=False  # Pas de features polynomiales pour l'instant
    )

    if importance_results is not None and not importance_results.empty:
        print("\nTop 10 features les plus importantes :")
        print(importance_results.head(10)[['feature', 'Combined_Score']])
except Exception as e:
    print(f"‚ö†Ô∏è Erreur lors de l'analyse d'importance : {e}")

# 5. Matrice de corr√©lation des variables continues avec la cible
print("\nüìä Corr√©lations des variables continues avec la cible...")
fig, ax = plt.subplots(figsize=(8, 6))

cont_corr_data = df_study[continuous_cols + ['y']].corr()['y'][:-1].to_frame()
cont_corr_data.columns = ['Corr√©lation avec y']

sns.heatmap(cont_corr_data, annot=True, cmap='coolwarm', center=0,
            cbar_kws={'label': 'Corr√©lation'}, fmt='.3f', ax=ax)
ax.set_title('Corr√©lations des variables continues avec la cible')
plt.tight_layout()
save_fig('continuous_target_correlation.png', directory=FIGURES_DIR / 'eda', figsize=(8, 6))

# 6. Visualisation de la sparsit√© par blocs
print("\nüìä Visualisation de la sparsit√© des donn√©es...")
# Prendre un √©chantillon de variables binaires
sample_binary_vars = np.random.choice(binary_cols, size=min(100, len(binary_cols)), replace=False)
sample_data = df_study[sample_binary_vars].head(100)

plt.figure(figsize=(12, 8))
plt.imshow(sample_data.values, cmap='binary', aspect='auto')
plt.colorbar(label='Valeur (0 ou 1)')
plt.title('Visualisation de la sparsit√© (100 observations √ó 100 variables)')
plt.xlabel('Variables binaires')
plt.ylabel('Observations')
plt.tight_layout()
save_fig('sparsity_visualization.png', directory=FIGURES_DIR / 'eda', figsize=(12, 8))

# 7. R√©sum√© visuel final
print("\nüìä Cr√©ation du r√©sum√© visuel de l'EDA...")
fig = plt.figure(figsize=(16, 10))

# Subplot 1: Distribution de la cible
ax1 = plt.subplot(2, 3, 1)
target_counts.plot(kind='pie', ax=ax1, colors=['#3498db', '#e74c3c'],
                  autopct='%1.1f%%', startangle=90)
ax1.set_title('Distribution de la variable cible')
ax1.set_ylabel('')

# Subplot 2: Valeurs manquantes
ax2 = plt.subplot(2, 3, 2)
missing_data = pd.Series({
    'X1': 27.41, 'X2': 27.37, 'X3': 27.61, 'X4': 0.0  # X4 d√©j√† imput√©
})
missing_data.plot(kind='bar', ax=ax2, color='coral')
ax2.set_title('Valeurs manquantes (avant traitement)')
ax2.set_ylabel('% manquant')

# Subplot 3: Sparsit√©
ax3 = plt.subplot(2, 3, 3)
sparsity_data = pd.Series({'Z√©ros': sparsity, 'Uns': 100-sparsity})
sparsity_data.plot(kind='pie', ax=ax3, colors=['lightgray', 'darkgray'],
                  autopct='%1.1f%%', startangle=90)
ax3.set_title('Sparsit√© des variables binaires')

# Subplot 4: Top corr√©lations
ax4 = plt.subplot(2, 3, 4)
target_corr_abs = target_corr.abs().nlargest(10)
target_corr_abs.plot(kind='barh', ax=ax4, color='skyblue')
ax4.set_xlabel('|Corr√©lation|')
ax4.set_title('Top 10 corr√©lations avec la cible')

# Subplot 5: Distribution des taux de pr√©sence
ax5 = plt.subplot(2, 3, 5)
presence_series.hist(bins=30, ax=ax5, color='lightgreen', edgecolor='black')
ax5.set_xlabel('Taux de pr√©sence (%)')
ax5.set_ylabel('Nombre de variables')
ax5.set_title('Distribution des taux de pr√©sence')
ax5.axvline(presence_series.mean(), color='red', linestyle='--', label=f'Moy: {presence_series.mean():.1f}%')
ax5.legend()

# Subplot 6: Statistiques cl√©s
ax6 = plt.subplot(2, 3, 6)
ax6.axis('off')
stats_text = f"""
Statistiques cl√©s:
- Observations: {len(df_study):,}
- Variables: {df_study.shape[1]:,}
- D√©s√©quilibre: {imbalance_ratio:.1f}:1
- Sparsit√©: {sparsity:.1f}%
- Corr. max avec y: {abs(target_corr).max():.3f}
- Variables binaires: {len(binary_cols)}
- Variables continues: {len(continuous_cols)}
"""
ax6.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')

plt.suptitle('R√©sum√© de l\'Analyse Exploratoire des Donn√©es', fontsize=16)
plt.tight_layout()
save_fig('eda_summary.png', directory=FIGURES_DIR / 'eda', figsize=(16, 10))

print("\n‚úÖ Visualisations exploratoires termin√©es !")
print("\nüí° Points cl√©s de l'EDA :")
print("  - Dataset fortement d√©s√©quilibr√© (14% vs 86%)")
print("  - ~27% de valeurs manquantes pour X1, X2, X3 (pattern MAR)")
print("  - Dataset extr√™mement sparse (99.19% de z√©ros)")
print("  - X2 montre la plus forte corr√©lation avec la cible (0.573)")
print("  - N√©cessit√© de pr√©traitement : transformation, imputation, r√©√©quilibrage")







































