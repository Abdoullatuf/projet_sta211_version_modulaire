# modules/preprocessing/dataset_generator.py

"""
Module pour la gÃ©nÃ©ration automatisÃ©e des datasets finaux.
GÃ¨re la crÃ©ation de multiples versions de datasets avec diffÃ©rentes 
configurations de preprocessing.
"""

import pandas as pd
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Union, Any
import logging

# Configuration du logging
logger = logging.getLogger(__name__)

class DatasetGenerator:
    """
    GÃ©nÃ©rateur de datasets avec diffÃ©rentes configurations de preprocessing.
    """
    
    def __init__(self, data_processed_dir: Path, models_dir: Path, random_state: int = 42):
        """
        Initialise le gÃ©nÃ©rateur de datasets.
        
        Args:
            data_processed_dir: RÃ©pertoire de sauvegarde des donnÃ©es traitÃ©es
            models_dir: RÃ©pertoire de sauvegarde des modÃ¨les/transformateurs
            random_state: Graine alÃ©atoire pour la reproductibilitÃ©
        """
        self.data_processed_dir = Path(data_processed_dir)
        self.models_dir = Path(models_dir)
        self.random_state = random_state
        
        # CrÃ©ation des rÃ©pertoires si nÃ©cessaire
        self.data_processed_dir.mkdir(parents=True, exist_ok=True)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # Configuration par dÃ©faut
        self.default_common_params = {
            "strategy": "mixed_mar_mcar",
            "mar_cols": ["X1_trans", "X2_trans", "X3_trans"],
            "mcar_cols": ["X4"],
            "correlation_threshold": 0.95,
            "processed_data_dir": self.data_processed_dir,
            "models_dir": self.models_dir,
            "display_info": True
        }
    
    def create_dataset_configs(self, file_path: Path, custom_params: Dict = None) -> Dict:
        """
        CrÃ©e les configurations pour tous les datasets Ã  gÃ©nÃ©rer.
        
        Args:
            file_path: Chemin vers le fichier de donnÃ©es brutes
            custom_params: ParamÃ¨tres personnalisÃ©s Ã  fusionner
            
        Returns:
            Dict: Configurations pour chaque dataset
        """
        
        # Fusion avec les paramÃ¨tres personnalisÃ©s
        common_params = self.default_common_params.copy()
        if custom_params:
            common_params.update(custom_params)
        
        common_params["file_path"] = file_path
        
        # Configurations spÃ©cifiques pour chaque combinaison
        configs = {
            "mice_with_outliers": {
                **common_params,
                "mar_method": "mice",
                "drop_outliers": False,
                "save_transformer": True,  # Premier appel, on sauvegarde
                "output_name": "final_dataset_mice_with_outliers"
            },
            "mice_no_outliers": {
                **common_params,
                "mar_method": "mice", 
                "drop_outliers": True,
                "save_transformer": False,  # Transformateur dÃ©jÃ  sauvegardÃ©
                "output_name": "final_dataset_mice_no_outliers"
            },
            "knn_with_outliers": {
                **common_params,
                "mar_method": "knn",
                "knn_k": 5,  # k optimal par dÃ©faut
                "drop_outliers": False,
                "save_transformer": False,
                "output_name": "final_dataset_knn_with_outliers"
            },
            "knn_no_outliers": {
                **common_params,
                "mar_method": "knn",
                "knn_k": 5,
                "drop_outliers": True, 
                "save_transformer": False,
                "output_name": "final_dataset_knn_no_outliers"
            }
        }
        
        return configs
    
    def generate_single_dataset(self, config_name: str, config: Dict, 
                               save_format: str = 'parquet') -> Dict:
        """
        GÃ©nÃ¨re un seul dataset selon la configuration.
        
        Args:
            config_name: Nom du dataset
            config: Configuration du dataset
            save_format: Format de sauvegarde ('parquet', 'csv', 'both')
            
        Returns:
            Dict: Informations sur le dataset gÃ©nÃ©rÃ©
        """
        
        # Import dynamique pour Ã©viter les dÃ©pendances circulaires
        from preprocessing.final_preprocessing import prepare_final_dataset
        
        logger.info(f"GÃ©nÃ©ration du dataset : {config_name}")
        
        try:
            # Extraction du nom de sortie
            output_name = config.pop('output_name')
            
            # GÃ©nÃ©ration du dataset
            df = prepare_final_dataset(**config)
            
            # Sauvegarde selon le format choisi
            saved_paths = []
            file_sizes = {}
            
            if save_format in ['parquet', 'both']:
                parquet_path = self.data_processed_dir / f"{output_name}.parquet"
                df.to_parquet(parquet_path, index=False)
                saved_paths.append(parquet_path)
                
                size_mb = parquet_path.stat().st_size / (1024 * 1024)
                file_sizes['.parquet'] = f"{size_mb:.2f} MB"
                logger.info(f"Parquet sauvegardÃ© : {parquet_path} ({size_mb:.2f} MB)")
                
            if save_format in ['csv', 'both']:
                csv_path = self.data_processed_dir / f"{output_name}.csv"
                df.to_csv(csv_path, index=False)
                saved_paths.append(csv_path)
                
                size_mb = csv_path.stat().st_size / (1024 * 1024)
                file_sizes['.csv'] = f"{size_mb:.2f} MB"
                logger.info(f"CSV sauvegardÃ© : {csv_path} ({size_mb:.2f} MB)")
            
            # Retour des informations
            return {
                'dataframe': df,
                'paths': saved_paths,
                'primary_path': saved_paths[0],
                'shape': df.shape,
                'missing_values': df.isnull().sum().sum(),
                'file_sizes': file_sizes,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"Erreur lors de la gÃ©nÃ©ration de {config_name} : {e}")
            return {
                'dataframe': None,
                'paths': [],
                'primary_path': None,
                'shape': (0, 0),
                'missing_values': 0,
                'file_sizes': {},
                'status': 'error',
                'error_message': str(e)
            }
    
    def generate_all_datasets(self, file_path: Path, save_format: str = 'parquet',
                             custom_configs: Dict = None) -> Dict:
        """
        GÃ©nÃ¨re tous les datasets selon les configurations.
        
        Args:
            file_path: Chemin vers le fichier de donnÃ©es brutes
            save_format: Format de sauvegarde ('parquet', 'csv', 'both')
            custom_configs: Configurations personnalisÃ©es (optionnel)
            
        Returns:
            Dict: Dictionnaire contenant tous les datasets gÃ©nÃ©rÃ©s
        """
        
        # Validation du format
        valid_formats = ['parquet', 'csv', 'both']
        if save_format not in valid_formats:
            raise ValueError(f"Format non supportÃ© : {save_format}. Utilisez {valid_formats}")
        
        # CrÃ©ation des configurations
        if custom_configs is None:
            configs = self.create_dataset_configs(file_path)
        else:
            configs = custom_configs
        
        logger.info(f"GÃ©nÃ©ration de {len(configs)} datasets en format {save_format}")
        
        datasets = {}
        
        for config_name, config in configs.items():
            print(f"\nğŸ“Š GÃ©nÃ©ration du dataset : {config_name}")
            print("-" * 50)
            
            result = self.generate_single_dataset(config_name, config, save_format)
            
            if result['status'] == 'success':
                print(f"âœ… Dataset gÃ©nÃ©rÃ© avec succÃ¨s")
                print(f"   ğŸ“ Dimensions : {result['shape']}")
                print(f"   ğŸ” Variables manquantes : {result['missing_values']}")
                
                for path in result['paths']:
                    ext = path.suffix
                    size = result['file_sizes'].get(ext, 'N/A')
                    print(f"   ğŸ’¾ {path.name} : {size}")
            else:
                print(f"âŒ Erreur : {result['error_message']}")
            
            datasets[config_name] = result
        
        return datasets
    
    def create_summary_table(self, datasets: Dict) -> pd.DataFrame:
        """
        CrÃ©e un tableau rÃ©capitulatif des datasets gÃ©nÃ©rÃ©s.
        
        Args:
            datasets: Dictionnaire des datasets gÃ©nÃ©rÃ©s
            
        Returns:
            pd.DataFrame: Tableau rÃ©capitulatif
        """
        
        summary_data = []
        
        for name, info in datasets.items():
            if info['status'] == 'success':
                # Gestion des chemins multiples
                if len(info['paths']) > 1:
                    files_info = ' + '.join([p.name for p in info['paths']])
                    sizes_info = ' + '.join([f"{ext}: {size}" for ext, size in info['file_sizes'].items()])
                else:
                    files_info = info['primary_path'].name if info['primary_path'] else 'N/A'
                    sizes_info = list(info['file_sizes'].values())[0] if info['file_sizes'] else "N/A"
                
                summary_data.append({
                    'Dataset': name,
                    'Statut': 'âœ… SuccÃ¨s',
                    'Lignes': info['shape'][0],
                    'Colonnes': info['shape'][1], 
                    'Valeurs manquantes': info['missing_values'],
                    'Fichier(s)': files_info,
                    'Taille': sizes_info
                })
            else:
                summary_data.append({
                    'Dataset': name,
                    'Statut': 'âŒ Erreur',
                    'Lignes': 0,
                    'Colonnes': 0,
                    'Valeurs manquantes': 0,
                    'Fichier(s)': 'N/A',
                    'Taille': 'N/A'
                })
        
        return pd.DataFrame(summary_data)
    
    def validate_datasets_quality(self, datasets: Dict) -> Dict:
        """
        Valide la qualitÃ© des datasets gÃ©nÃ©rÃ©s.
        
        Args:
            datasets: Dictionnaire des datasets gÃ©nÃ©rÃ©s
            
        Returns:
            Dict: Rapport de validation
        """
        
        validation_report = {}
        
        for name, info in datasets.items():
            if info['status'] != 'success':
                validation_report[name] = {'status': 'error', 'checks': {}}
                continue
                
            df = info['dataframe']
            checks = {}
            
            # VÃ©rifications de base
            checks['has_target'] = 'y' in df.columns
            checks['no_missing_values'] = info['missing_values'] == 0
            checks['has_transformed_vars'] = any('_trans' in col for col in df.columns)
            checks['target_distribution'] = dict(df['y'].value_counts()) if 'y' in df.columns else None
            checks['data_types'] = dict(df.dtypes.value_counts())
            
            # Score de qualitÃ© global
            quality_score = sum([
                checks['has_target'],
                checks['no_missing_values'],
                checks['has_transformed_vars']
            ]) / 3
            
            validation_report[name] = {
                'status': 'success',
                'quality_score': quality_score,
                'checks': checks
            }
        
        return validation_report
    
    def save_generation_config(self, datasets: Dict, save_format: str, 
                              output_path: Path = None) -> Path:
        """
        Sauvegarde la configuration de gÃ©nÃ©ration.
        
        Args:
            datasets: Datasets gÃ©nÃ©rÃ©s
            save_format: Format utilisÃ©
            output_path: Chemin de sauvegarde (optionnel)
            
        Returns:
            Path: Chemin du fichier de configuration
        """
        
        if output_path is None:
            output_path = self.data_processed_dir / "dataset_generation_config.json"
        
        config_summary = {
            "generation_date": datetime.now().isoformat(),
            "save_format": save_format,
            "random_state": self.random_state,
            "datasets_generated": list(datasets.keys()),
            "successful_datasets": [
                name for name, info in datasets.items() 
                if info['status'] == 'success'
            ],
            "common_parameters": {
                "strategy": "mixed_mar_mcar",
                "correlation_threshold": 0.95,
                "random_state": self.random_state
            },
            "specific_configs": {
                name: {
                    "method": "mice" if "mice" in name else "knn",
                    "outliers": "with" if "with" in name else "without",
                    "k_value": 5 if "knn" in name else None,
                    "status": info['status']
                }
                for name, info in datasets.items()
            },
            "file_info": {
                name: {
                    "paths": [str(p) for p in info['paths']],
                    "file_sizes": info['file_sizes'],
                    "shape": info['shape']
                }
                for name, info in datasets.items()
                if info['status'] == 'success'
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(config_summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Configuration sauvegardÃ©e : {output_path}")
        return output_path


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def quick_generate_datasets(file_path: Path, data_processed_dir: Path, 
                           models_dir: Path, save_format: str = 'parquet',
                           random_state: int = 42) -> Dict:
    """
    Fonction rapide pour gÃ©nÃ©rer tous les datasets avec les paramÃ¨tres par dÃ©faut.
    
    Args:
        file_path: Chemin vers le fichier de donnÃ©es brutes
        data_processed_dir: RÃ©pertoire de sauvegarde des donnÃ©es
        models_dir: RÃ©pertoire des modÃ¨les
        save_format: Format de sauvegarde
        random_state: Graine alÃ©atoire
        
    Returns:
        Dict: Datasets gÃ©nÃ©rÃ©s
    """
    
    generator = DatasetGenerator(data_processed_dir, models_dir, random_state)
    return generator.generate_all_datasets(file_path, save_format)

def print_generation_summary(datasets: Dict):
    """
    Affiche un rÃ©sumÃ© de la gÃ©nÃ©ration des datasets.
    
    Args:
        datasets: Dictionnaire des datasets gÃ©nÃ©rÃ©s
    """
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ RÃ‰SUMÃ‰ DES DATASETS FINAUX GÃ‰NÃ‰RÃ‰S")
    print("=" * 70)
    
    # Statistiques globales
    successful = sum(1 for info in datasets.values() if info['status'] == 'success')
    failed = len(datasets) - successful
    
    print(f"âœ… Datasets gÃ©nÃ©rÃ©s avec succÃ¨s : {successful}/{len(datasets)}")
    if failed > 0:
        print(f"âŒ Ã‰checs : {failed}")
    
    # Tableau dÃ©taillÃ©
    generator = DatasetGenerator(Path('.'), Path('.'))  # Dummy pour utiliser la mÃ©thode
    summary_df = generator.create_summary_table(datasets)
    print(f"\n{summary_df.to_string(index=False)}")
    
    return summary_df