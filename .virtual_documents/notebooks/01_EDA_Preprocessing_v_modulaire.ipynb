





## D√©finition des m√©tadonn√©es du projet

from datetime import datetime
from pathlib import Path

# M√©tadonn√©es du projet
PROJECT_NAME = "Projet STA 211: Internet Advertisements Classification"
DATASET_NAME = "Internet Advertisements Dataset"
AUTHOR = "Abdoullatuf"
DATE = datetime.now().strftime("%Y-%m-%d")  # Date dynamique
VERSION = "1.0"

# V√©rification des m√©tadonn√©es
metadata = {
    "Projet": PROJECT_NAME,
    "Dataset": DATASET_NAME,
    "Auteur": AUTHOR,
    "Date": DATE,
    "Version": VERSION
}

# Affichage des informations
print("üìã M√©tadonn√©es du projet")
print("="*60)
for key, value in metadata.items():
    if not isinstance(value, str):
        raise TypeError(f"La m√©tadonn√©e '{key}' doit √™tre une cha√Æne de caract√®res, re√ßu {type(value)}")
    print(f"{key}: {value}")

# Sauvegarde des m√©tadonn√©es dans un fichier (optionnel)
METADATA_DIR = Path("metadata")
METADATA_DIR.mkdir(exist_ok=True)
metadata_file = METADATA_DIR / f"metadata_v{VERSION}.txt"
with open(metadata_file, "w", encoding="utf-8") as f:
    for key, value in metadata.items():
        f.write(f"{key}: {value}\n")
print(f"‚úÖ M√©tadonn√©es sauvegard√©es dans : {metadata_file}")








## 2.1 Configuration de l'environnement <a id="configuration-environnement"></a>

# Installation des packages (d√©commenter si n√©cessaire)
# !pip install -q scikit-learn xgboost lightgbm imbalanced-learn umap-learn prince
!pip install -q scikit-learn imbalanced-learn umap-learn prince

import sys
import os
from pathlib import Path
from IPython.display import Markdown, display
import warnings

# Configuration des warnings et de pandas
warnings.filterwarnings('ignore')

import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', '{:.4f}'.format)

# üîç D√©tection de l'environnement
def detect_environment():
    try:
        import google.colab
        return "colab"
    except ImportError:
        return "local"

ENV = detect_environment()
print(f"üîß Environnement d√©tect√© : {ENV}")

# üöó Montage de Google Drive si n√©cessaire
if ENV == "colab":
    from google.colab import drive
    drive.mount('/content/drive', force_remount=True)
    print("‚úÖ Google Drive mont√© avec succ√®s")

# üìÅ D√©finir les chemins selon l‚Äôenvironnement
if ENV == "colab":
    module_path = Path("/content/drive/MyDrive/projet_sta211/modules")
else:
    module_path = Path("G:/Mon Drive/projet_sta211/modules")

if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))
    print(f"‚úÖ Module path ajout√© : {module_path}")

# üì¶ Chargement des chemins
try:
    if ENV == "colab":
        from config.paths_config import setup_project_paths
    else:
        from modules.config.paths_config import setup_project_paths

    paths = setup_project_paths()
    print("‚úÖ Configuration des chemins r√©ussie")
except ImportError as e:
    print(f"‚ùå Erreur : impossible d'importer setup_project_paths")
    raise

# üîß Ajout manuel OUTPUTS_DIR si absent
if "OUTPUTS_DIR" not in paths:
    paths["OUTPUTS_DIR"] = paths["ROOT_DIR"] / "outputs"

# üìã Affichage Markdown des chemins
def display_paths():
    status_icons = {
        key: "‚úÖ" if Path(path).exists() else "‚ùå"
        for key, path in paths.items()
    }
    paths_str = {k: str(v) for k, v in paths.items()}

    md = f"""
### üìÇ **Chemins configur√©s pour le projet**

| Status | Nom du dossier      | Chemin                                        |
|--------|---------------------|-----------------------------------------------|
| {status_icons.get("ROOT_DIR", "?")} | `ROOT_DIR`          | `{paths_str.get("ROOT_DIR", "")}`             |
| {status_icons.get("MODULE_DIR", "?")} | `MODULE_DIR`        | `{paths_str.get("MODULE_DIR", "")}`           |
| {status_icons.get("RAW_DATA_DIR", "?")} | `RAW_DATA_DIR`      | `{paths_str.get("RAW_DATA_DIR", "")}`         |
| {status_icons.get("DATA_PROCESSED", "?")} | `DATA_PROCESSED`    | `{paths_str.get("DATA_PROCESSED", "")}`       |
| {status_icons.get("MODELS_DIR", "?")} | `MODELS_DIR`        | `{paths_str.get("MODELS_DIR", "")}`           |
| {status_icons.get("FIGURES_DIR", "?")} | `FIGURES_DIR`       | `{paths_str.get("FIGURES_DIR", "")}`          |
| {status_icons.get("OUTPUTS_DIR", "?")} | `OUTPUTS_DIR`       | `{paths_str.get("OUTPUTS_DIR", "")}`          |

**L√©gende :** ‚úÖ = Existe | ‚ùå = N'existe pas
"""
    display(Markdown(md))

display_paths()

# üîß Infos syst√®me
print(f"\nüêç Python version : {sys.version.split()[0]}")
print(f"üìç Working directory : {os.getcwd()}")

# üìå Variables globales
ROOT_DIR = paths["ROOT_DIR"]
MODULE_DIR = paths["MODULE_DIR"]
RAW_DATA_DIR = paths["RAW_DATA_DIR"]
DATA_PROCESSED = paths["DATA_PROCESSED"]
MODELS_DIR = paths["MODELS_DIR"]
FIGURES_DIR = paths["FIGURES_DIR"]
OUTPUTS_DIR = paths["OUTPUTS_DIR"]







## 2.2 Import des biblioth√®ques <a id="import-des-bibliotheques"></a>

# üì¶ Modules standards
import sys
import os
import warnings
from pathlib import Path

# üßÆ Manipulation des donn√©es
import numpy as np
import pandas as pd

# üìä Visualisation
import matplotlib.pyplot as plt
import seaborn as sns

# ‚öôÔ∏è Pr√©traitement & Mod√®les
from sklearn.experimental import enable_iterative_imputer  # ‚¨ÖÔ∏è N√©cessaire pour IterativeImputer
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, calinski_harabasz_score, mean_squared_error
from sklearn.model_selection import KFold

from typing import List, Dict, Optional



# üìà Statistiques
from scipy.stats import pointbiserialr

# üîÅ Optionnel : UMAP
try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("‚ö†Ô∏è UMAP non disponible. Certaines visualisations seront d√©sactiv√©es.")

# üé≤ Configuration globale
warnings.filterwarnings("ignore")
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 100)
pd.set_option("display.float_format", '{:.4f}'.format)
sns.set_style("whitegrid")
sns.set_palette("husl")

# D√©finition de RANDOM_STATE
if "RANDOM_STATE" not in globals():
    RANDOM_STATE = 42
    np.random.seed(RANDOM_STATE)

# üîç V√©rification des modules critiques
print("\nüîç V√©rification des modules critiques :")
import sklearn  # Ajout explicite

required_modules = {
    "pandas": pd,
    "numpy": np,
    "scikit-learn": sklearn,
    "matplotlib": plt,
    "seaborn": sns
}

for name, alias in required_modules.items():
    try:
        _ = eval(alias) if isinstance(alias, str) else alias
        print(f"  ‚úÖ {name}")
    except Exception:
        print(f"  ‚ùå {name} - probl√®me lors de l'import")

# ‚öôÔ∏è R√©sum√© de configuration
print(f"\n‚öôÔ∏è Configuration :")
print(f"  - Random State : {RANDOM_STATE}")
print(f"  - Style matplotlib : seaborn-whitegrid")
print(f"  - Palette seaborn : husl")
print(f"  - UMAP disponible : {UMAP_AVAILABLE}")
print(f"  - Python : {sys.version.split()[0]}")






## 2.3 Configuration des param√®tres du projet <a id="configuration-parametres-projet"></a>

# üì¶ Import de la classe ProjectConfig et de la fonction de cr√©ation
try:
    from config.project_config import ProjectConfig, create_config
except ImportError as e:
    print("‚ùå Erreur : impossible d'importer depuis 'config.project_config'")
    raise

# üõ†Ô∏è Cr√©ation de la configuration avec les m√©tadonn√©es + chemins
config = create_config(
    project_name=PROJECT_NAME,
    version=VERSION,
    author=AUTHOR,
    paths=paths
)

# üéØ Ciblage de la m√©trique F1 pour le challenge
config.update("PROJECT_CONFIG.SCORING", "f1")
config.update("PROJECT_CONFIG.SCORING_METRICS", ["f1", "roc_auc", "precision", "recall"])
config.update("PROJECT_CONFIG.PRIMARY_METRIC", "f1")

# üëÅÔ∏è Affichage de la configuration
config.display_config()

# üìå Exemples d'acc√®s √† des valeurs cl√©s
print("\nüìå Exemples d'acc√®s √† la configuration :")
print(f"  - Test size : {config.get('PROJECT_CONFIG.TEST_SIZE')}")
print(f"  - M√©thode d'imputation X4 : {config.get('PROJECT_CONFIG.IMPUTATION_METHODS.X4')}")
print(f"  - Taille des figures : {config.get('VIZ_CONFIG.figure_size')}")

# üíæ Sauvegarde de la configuration dans le dossier 'config'
config_file = Path(paths["ROOT_DIR"]) / "config" / f"project_config_v{VERSION}.json"
config.save_config(config_file)

# üåç Rendre la configuration disponible globalement
PROJECT_CONFIG = config.PROJECT_CONFIG
COLUMN_CONFIG = config.COLUMN_CONFIG
VIZ_CONFIG = config.VIZ_CONFIG
MODEL_CONFIG = config.MODEL_CONFIG
PIPELINE_CONFIG = config.PIPELINE_CONFIG
SAVE_PATHS = config.SAVE_PATHS
F1_OPTIMIZATION = config.F1_OPTIMIZATION

print("\n‚úÖ Configuration charg√©e et disponible globalement")






## 3.1 Chargement des jeux de donn√©es bruts <a id="chargement-des-jeux-de-donnees-bruts"></a>

from preprocessing.data_loader import load_data
#from modules.preprocessing.data_loader import load_data  # en local si jamais ne fonctionne pas, d√©coche ce si.


from pathlib import Path

# üìÅ V√©rification de l‚Äôexistence du dossier RAW_DATA_DIR
if 'RAW_DATA_DIR' not in globals():
    raise NameError("‚ùå RAW_DATA_DIR n‚Äôest pas d√©fini. V√©rifiez la configuration dans la section 2.1.")

raw_data_dir = Path(RAW_DATA_DIR)
if not raw_data_dir.exists():
    raise FileNotFoundError(f"‚ùå Dossier RAW_DATA_DIR introuvable : {raw_data_dir}")

# üìÇ Chargement des fichiers CSV
print("üìÇ Chargement des jeux de donn√©es...")

df_study = load_data(
    file_path="data_train.csv",
    require_outcome=True,
    display_info=True,
    raw_data_dir=RAW_DATA_DIR,
    encode_target=True
)

df_eval = load_data(
    file_path="data_test.csv",
    require_outcome=False,
    display_info=True,
    raw_data_dir=RAW_DATA_DIR,
    encode_target=False
)

# üè∑Ô∏è Renommer 'outcome' en 'y' si n√©cessaire
if 'outcome' in df_study.columns:
    df_study = df_study.rename(columns={'outcome': 'y'})
    print("‚úÖ Colonne 'outcome' renomm√©e en 'y'")
elif 'y' not in df_study.columns:
    raise ValueError("‚ùå Colonne 'y' ou 'outcome' manquante dans df_study")

# üî¢ V√©rification des dimensions attendues
expected_train_shape = (2459, 1559)
expected_test_shape = (820, 1558)

if df_study.shape != expected_train_shape:
    print(f"‚ö†Ô∏è Dimensions inattendues pour df_study : {df_study.shape} (attendu : {expected_train_shape})")
if df_eval.shape != expected_test_shape:
    print(f"‚ö†Ô∏è Dimensions inattendues pour df_eval : {df_eval.shape} (attendu : {expected_test_shape})")

# üîç V√©rification de la variable cible
print("\nüîé Valeurs uniques de y :", df_study['y'].unique())
print("üîé Type de y :", df_study['y'].dtype)

# üìä R√©sum√©
print(f"\nüìä R√©sum√© :")
print(f"  - Fichier d‚Äô√©tude     : {df_study.shape}")
print(f"  - Fichier d‚Äô√©valuation : {df_eval.shape}")
print("\n‚úÖ Chargement termin√© avec succ√®s !")






## 3.2 Inspection des colonnes et types <a id="inspection-des-colonnes-et-types"></a>

print("üîç Inspection des types de donn√©es")
print("=" * 60)

# üîé R√©sum√© des types dans df_study
print("\nüìä Types de donn√©es dans df_study :")
type_counts = df_study.dtypes.value_counts()
for dtype, count in type_counts.items():
    print(f"  - {dtype}: {count} colonnes")

# üìå Identification des colonnes par type
continuous_cols = df_study.select_dtypes(include=['float64']).columns.tolist()
int_cols = df_study.select_dtypes(include=['int64']).columns.tolist()
categorical_cols = df_study.select_dtypes(include=['object']).columns.tolist()

# üîÅ V√©rification binaire r√©elle parmi les colonnes int
binary_cols = []
non_binary_cols = []

for col in int_cols:
    unique_vals = df_study[col].dropna().unique()
    if len(unique_vals) == 2 and set(unique_vals).issubset({0, 1}):
        binary_cols.append(col)
    else:
        non_binary_cols.append(col)

print(f"\nüìà Colonnes continues : {len(continuous_cols)}")
print(f"üî¢ Colonnes binaires : {len(binary_cols)}")
print(f"üì¶ Colonnes cat√©gorielles : {len(categorical_cols)}")

if non_binary_cols:
    print(f"‚ö†Ô∏è Colonnes int64 non binaires d√©tect√©es (extrait) : {non_binary_cols[:5]}")

# üéØ Variable cible 'y'
if 'y' in df_study.columns:
    print("\nüéØ Variable cible 'y' :")
    print(f"  - Type : {df_study['y'].dtype}")
    print(f"  - Valeurs uniques : {sorted(df_study['y'].dropna().unique())}")
    print(f"  - Distribution :\n{df_study['y'].value_counts().sort_index()}")
else:
    print("‚ùå La colonne cible 'y' est manquante")

# üîÑ Comparaison avec df_eval
print("\nüîÑ Comparaison avec df_eval :")
eval_type_counts = df_eval.dtypes.value_counts()
for dtype, count in eval_type_counts.items():
    print(f"  - {dtype}: {count} colonnes")

# ‚öñÔ∏è V√©rification des types entre df_study et df_eval
print("\nüîç V√©rification de la coh√©rence des types entre fichiers :")
type_mismatches = [
    (col, df_study[col].dtype, df_eval[col].dtype)
    for col in df_eval.columns if col in df_study.columns and df_study[col].dtype != df_eval[col].dtype
]

if type_mismatches:
    print("‚ö†Ô∏è Incoh√©rences de type d√©tect√©es :")
    for col, t1, t2 in type_mismatches:
        print(f"  - {col}: {t1} (√©tude) vs {t2} (√©val)")
else:
    print("‚úÖ Types coh√©rents entre df_study et df_eval")

# üíæ Mise √† jour de la configuration
print("\nüíæ Mise √† jour de la configuration...")
config.update("COLUMN_CONFIG.CONTINUOUS_COLS", continuous_cols)
config.update("COLUMN_CONFIG.BINARY_COLS", binary_cols)
config.update("COLUMN_CONFIG.CATEGORICAL_COLS", categorical_cols)

# üìã R√©sum√© final
print("\nüìä R√©sum√© des colonnes (hors 'y') :")
print(f"  - Colonnes continues   : {len(continuous_cols)}")
print(f"  - Colonnes binaires    : {len(binary_cols)}")
print(f"  - Colonnes cat√©gorielles : {len(categorical_cols)}")
print(f"  - Total features (hors cible) : {df_study.shape[1] - 1}")






## 3.3 Distribution de la variable cible <a id="distribution-variable-cible"></a>

print("\nüéØ Analyse de la distribution de la variable cible")
print("=" * 60)

# V√©rification de la pr√©sence de 'y'
if 'y' not in df_study.columns:
    raise ValueError("‚ùå Colonne cible 'y' introuvable dans df_study")

# Distribution brute et en %
target_counts = df_study['y'].value_counts().sort_index()
target_pct = df_study['y'].value_counts(normalize=True).sort_index() * 100

# Affichage des proportions
print("\nüìä Distribution de la variable cible (y) :")
for label in target_counts.index:
    label_str = "Classe 1 (ad.)" if label == 1 else "Classe 0 (noad.)"
    print(f"  - {label_str:<20}: {target_counts[label]:,} ({target_pct[label]:.1f}%)")

# Ratio de d√©s√©quilibre
imbalance_ratio = target_counts[0] / target_counts[1]
print(f"\nüìà Ratio de d√©s√©quilibre : {imbalance_ratio:.2f}:1")
print(f"   ‚Üí Pour chaque publicit√©, il y a {imbalance_ratio:.1f} non-publicit√©s")

# Visualisation
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# Bar plot
target_counts.plot(kind='bar', ax=ax1, color=['#3498db', '#e74c3c'])
ax1.set_title('Distribution des classes', fontsize=14)
ax1.set_xlabel('Classe')
ax1.set_ylabel('Nombre d\'√©chantillons')
ax1.set_xticklabels(['Non-publicit√© (0)', 'Publicit√© (1)'], rotation=0)

# Pie chart
target_pct.plot(kind='pie', ax=ax2, colors=['#3498db', '#e74c3c'],
                autopct='%1.1f%%', startangle=90)
ax2.set_title('Proportion des classes', fontsize=14)
ax2.set_ylabel('')

plt.tight_layout()

# Sauvegarde s√©curis√©e dans un sous-dossier
eda_dir = FIGURES_DIR / 'eda'
eda_dir.mkdir(parents=True, exist_ok=True)
plt.savefig(eda_dir / 'target_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# Impact pour la mod√©lisation
print("\nüí° Implications pour la mod√©lisation :")
print(f"  - Dataset fortement d√©s√©quilibr√© ({imbalance_ratio:.1f}:1)")
print("  - Strat√©gies recommand√©es :")
print("    ‚Ä¢ Utiliser stratify=True lors du train/test split")
print("    ‚Ä¢ Appliquer SMOTE ou class_weight='balanced'")
print("    ‚Ä¢ Optimiser pour F1-score (m√©trique du challenge)")
print("    ‚Ä¢ Envisager un stacking ou un mod√®le robuste aux d√©s√©quilibres")

# Calcul du F1-score baseline
p = target_pct[1] / 100  # Pr√©cision et recall identiques si on pr√©dit toujours 1
baseline_f1 = 2 * p / (1 + p)
print(f"\nüìä F1-score baseline (pr√©dire toujours 'ad.') : {baseline_f1:.3f}")
print("   ‚Üí Les mod√®les devront d√©passer ce seuil pour √™tre utiles")






# 4. Analyse exploratoire <a id="analyse-exploratoire"></a>
## 4.1 Analyse des valeurs manquantes <a id="analyse-des-valeurs-manquantes"></a>

print("üîç Analyse des valeurs manquantes")
print("="*60)

# Utilisation de la fonction du module
from preprocessing.missing_values import (
    analyze_missing_values,
    handle_missing_values,
    find_optimal_k
)



print("\nüìä Analyse globale des valeurs manquantes :")
missing_stats = analyze_missing_values(df_study)

# Analyse d√©taill√©e pour les colonnes continues
print("\nüìà D√©tail des valeurs manquantes pour les variables continues :")
for col in continuous_cols:
    missing_count = df_study[col].isnull().sum()
    missing_pct = (missing_count / len(df_study)) * 100
    print(f"  - {col}: {missing_count} ({missing_pct:.2f}%)")

# Visualisation des patterns de valeurs manquantes
if missing_stats['total_missing'] > 0:
    # Heatmap des valeurs manquantes pour les colonnes avec des NaN
    cols_with_missing = [col for col in df_study.columns if df_study[col].isnull().sum() > 0]

    if len(cols_with_missing) > 0:
        plt.figure(figsize=(10, 5))

        # Cr√©er une matrice binaire des valeurs manquantes
        missing_matrix = df_study[cols_with_missing].isnull().astype(int)

        # Heatmap
        sns.heatmap(missing_matrix.T, cmap='RdYlBu', cbar_kws={'label': 'Manquant (1) / Pr√©sent (0)'})
        plt.title('Pattern des valeurs manquantes', fontsize=14)
        plt.xlabel('√âchantillons')
        plt.ylabel('Variables')
        plt.tight_layout()
        plt.savefig(FIGURES_DIR / 'eda' / 'missing_values_pattern.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Analyse du pattern MAR vs MCAR
        print("\nüîç Analyse du type de valeurs manquantes (MAR vs MCAR) :")

        # Corr√©lation entre les valeurs manquantes et la cible
        for col in cols_with_missing:
            missing_indicator = df_study[col].isnull().astype(int)
            correlation_with_target = missing_indicator.corr(df_study['y'])
            print(f"  - {col}: corr√©lation avec y = {correlation_with_target:.3f}")

            if abs(correlation_with_target) > 0.1:
                print(f"    ‚Üí Potentiellement MAR (Missing At Random)")
            else:
                print(f"    ‚Üí Potentiellement MCAR (Missing Completely At Random)")
else:
    print("\n‚úÖ Aucune valeur manquante d√©tect√©e dans le dataset !")

# Analyse pour le fichier d'√©valuation aussi
print("\nüìä Analyse des valeurs manquantes dans le fichier d'√©valuation :")
missing_stats_eval = analyze_missing_values(df_eval)

# Comparaison des patterns
if missing_stats['total_missing'] > 0 or missing_stats_eval['total_missing'] > 0:
    print("\nüîÑ Comparaison des patterns de valeurs manquantes :")
    print(f"  - Fichier d'√©tude : {missing_stats['percent_missing']:.2f}% manquant")
    print(f"  - Fichier d'√©valuation : {missing_stats_eval['percent_missing']:.2f}% manquant")

    # Strat√©gie d'imputation recommand√©e
    print("\nüí° Strat√©gie d'imputation recommand√©e :")
    if 'X4' in missing_stats['cols_missing']:
        x4_missing_pct = missing_stats['percent_per_col'].get('X4', 0)
        if x4_missing_pct < 5:
            print(f"  - X4 ({x4_missing_pct:.1f}% manquant) : Imputation par la m√©diane")

    mar_cols = ['X1', 'X2', 'X3']
    mar_missing = any(col in missing_stats['cols_missing'] for col in mar_cols)
    if mar_missing:
        print(f"  - X1, X2, X3 (variables continues) : KNN ou MICE (imputation multivari√©e)")


# Correction du type et imputation de X4
print("\nüîß Correction du type de X4...")
print(f"Valeurs uniques de X4 (avant correction) : {sorted(df_study['X4'].dropna().unique())}")
print(f"Type actuel : {df_study['X4'].dtype}")

# V√©rifier que X4 ne contient que 0 et 1
unique_values = df_study['X4'].dropna().unique()
if set(unique_values).issubset({0.0, 1.0}):
    # Imputer d'abord les valeurs manquantes par la m√©diane
    X4_median = df_study['X4'].median()
    df_study['X4'] = df_study['X4'].fillna(X4_median)
    df_eval['X4'] = df_eval['X4'].fillna(X4_median)

    # Convertir en int
    df_study['X4'] = df_study['X4'].astype(int)
    df_eval['X4'] = df_eval['X4'].astype(int)

    print(f"‚úÖ X4 converti en int64 apr√®s imputation par la m√©diane ({X4_median})")
    print(f"Nouveau type : {df_study['X4'].dtype}")

    # Mettre √† jour la configuration
    config.update("COLUMN_CONFIG.CONTINUOUS_COLS", ['X1', 'X2', 'X3'])
    config.update("COLUMN_CONFIG.BINARY_COLS", ['X4'] + binary_cols)
    continuous_cols = ['X1', 'X2', 'X3']  # Mise √† jour locale
else:
    print("‚ö†Ô∏è X4 contient des valeurs autres que 0 et 1, conservation en float64")

# R√©sum√© final
print("\nüìä R√©sum√© des valeurs manquantes apr√®s traitement de X4 :")
print(f"  - X1, X2, X3 : ~27% manquant ‚Üí √Ä traiter avec KNN/MICE")
print(f"  - X4 : Imput√© et converti en binaire")
print(f"  - Pattern MAR d√©tect√© pour X1, X2, X3 (corr√©lation avec y ‚âà -0.10)")
print(f"  - Les patterns sont coh√©rents entre fichiers d'√©tude et d'√©valuation")






## 4.2 Analyse statistique des variables quantitatives <a id="analyse-statistique-des-variables-quantitatives"></a>

print("üìä Analyse statistique des variables quantitatives")
print("="*60)

from exploration.statistics import analyze_continuous_variables
#from modules.exploration.statistics import analyze_continuous_variables


# Lancement de l‚Äôanalyse compl√®te
results_stats = analyze_continuous_variables(
    df=df_study,
    continuous_cols=continuous_cols,
    target_col='y',
    save_figures_path=str(FIGURES_DIR / "eda")  # Assure-toi que ce dossier existe
)







## 4.2.1 Visualisation des distributions et des boxplots <a id="distributions-et-boxplots"></a>

print("üìä Visualisation des distributions et des boxplots")
print("="*60)

from exploration.visualization import visualize_distributions_and_boxplots
# Appel de la fonction
visualize_distributions_and_boxplots(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / "eda"
)








## 4.3 Distribution des variables binaires <a id="distribution-des-variables-binaires"></a>

print("üî¢ Analyse de la distribution des variables binaires")
print("="*60)

from exploration.visualization import save_fig

# Variables binaires (exclut les variables continues)
binary_cols = [col for col in df_study.columns if col.startswith('X') and col not in continuous_cols]
print(f"\nüìä Nombre total de variables binaires : {len(binary_cols)}")

# Taux de pr√©sence (valeurs √† 1)
presence_rates = {
    col: (df_study[col] == 1).sum() / len(df_study) * 100 for col in binary_cols
}
presence_series = pd.Series(presence_rates)

# Statistiques globales
print(f"\nüìä Statistiques des taux de pr√©sence :")
print(f"  - Moyenne : {presence_series.mean():.2f}%")
print(f"  - M√©diane : {presence_series.median():.2f}%")
print(f"  - Min : {presence_series.min():.2f}%")
print(f"  - Max : {presence_series.max():.2f}%")

# Sparsit√© globale
total_values = len(df_study) * len(binary_cols)
total_ones = df_study[binary_cols].sum().sum()
sparsity = (1 - total_ones / total_values) * 100
print(f"\nüìä Sparsit√© globale : {sparsity:.2f}% de z√©ros")

# Visualisation
plt.figure(figsize=(8, 4))
presence_series.hist(bins=50, color='skyblue', edgecolor='black')
plt.axvline(presence_series.mean(), color='red', linestyle='--', label=f'Moyenne: {presence_series.mean():.1f}%')
plt.xlabel('Taux de pr√©sence (%)')
plt.ylabel('Nombre de variables')
plt.title('Distribution des taux de pr√©sence des variables binaires')
plt.legend()
plt.tight_layout()

save_fig("binary_presence_distribution.png", directory=FIGURES_DIR / "eda", dpi=300, show=True)

print("\n‚úÖ Analyse des variables binaires termin√©e")
print("   ‚Üí Dataset tr√®s sparse, adapt√© pour des m√©thodes de s√©lection de features")






## 4.4 Analyse des corr√©lations combin√©es <a id="analyse-correlations-combinees"></a>

print("üîó Lancement de l'analyse combin√©e des corr√©lations (features ‚Üî cible, features ‚Üî features)...")
print("=" * 80)

from exploration.eda_analysis import full_correlation_analysis

# Appel avec param√®tres personnalis√©s
full_correlation_analysis(
    df_study=df_study,
    continuous_cols=continuous_cols,
    presence_rates=presence_rates,
    FIGURES_DIR=FIGURES_DIR,
    ROOT_DIR=ROOT_DIR,
    figsize_corr_matrix=(7, 5),
    figsize_binary=(8, 4)
)








## 4.6 Visualisations globales de l'EDA <a id="visualisation-globale"></a>

print("üìä Visualisations exploratoires")
print("=" * 60)

# Imports des fonctions refactoris√©es
from exploration.visualization import (
    compare_visualization_methods,
    plot_continuous_by_class,
    plot_binary_sparsity,
    plot_continuous_target_corr,
    plot_eda_summary,
    save_fig
)
from exploration.statistics import optimized_feature_importance  # ‚úÖ Nouvelle fonction import√©e

# 1. Distribution des variables continues par classe
print("\nüìà Distribution des variables continues par classe...")
plot_continuous_by_class(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / 'eda'
)

# 2. Visualisation de la sparsit√©
print("\nüìâ Visualisation de la sparsit√© des donn√©es binaires...")
plot_binary_sparsity(
    df=df_study,
    binary_cols=binary_cols,
    output_dir=FIGURES_DIR / 'eda'
)




# 3. Corr√©lations des variables continues avec la cible
print("\nüîó Corr√©lations des variables continues avec la cible...")
plot_continuous_target_corr(
    df=df_study,
    continuous_cols=continuous_cols,
    output_dir=FIGURES_DIR / 'eda'
)

# 4. R√©duction de dimension avec UMAP / t-SNE / PCA
print("\nüìä Visualisation multidimensionnelle (PCA / t-SNE / UMAP)...")

df_study_viz = df_study.copy()
df_study_viz['outcome'] = df_study_viz['y'].map({0: 'noad.', 1: 'ad.'})  # ‚úÖ temporaire

# üîÅ Recalcul des corr√©lations si besoin
target_corr = df_study[continuous_cols + ['y']].corr()['y'].drop('y')

important_features = continuous_cols + list(target_corr.abs().nlargest(30).index)
df_sample = df_study_viz[important_features + ['outcome']].dropna()





# 5. Importance des variables
print("\nüå≤ Analyse de l‚Äôimportance des features...")
try:
    df_importance = df_sample.copy()  # contient outcome d√©j√† transform√©e
    importance_results = optimized_feature_importance(
        df=df_importance,
        target_col='outcome',
        method='all',
        top_n=10,
        figsize=(8, 4),
        save_path=FIGURES_DIR / 'eda' / 'feature_importance.png',
        show=True
    )
    if not importance_results.empty:
        print("\nTop 10 features les plus importantes :")
        print(importance_results[['feature', 'Combined_Score']].head(10))
except Exception as e:
    print(f"‚ö†Ô∏è Erreur lors de l‚Äôanalyse d‚Äôimportance des features : {e}")

# 6. R√©sum√© visuel global
print("\nüìä Cr√©ation du r√©sum√© visuel de l‚ÄôEDA...")
plot_eda_summary(
    df=df_study,
    continuous_cols=continuous_cols,
    binary_cols=binary_cols,
    target_corr=target_corr,
    sparsity=sparsity,
    imbalance_ratio=imbalance_ratio,
    output_dir=FIGURES_DIR / 'eda',
    presence_series=presence_series
)

print("\n‚úÖ Visualisations exploratoires termin√©es avec succ√®s !")








from preprocessing.final_preprocessing import apply_yeojohnson

# Appliquer Yeo-Johnson aux variables continues
df_study = apply_yeojohnson(
    df=df_study,
    columns=["X1", "X2", "X3"],
    standardize=False,
    save_model=True,
    model_path=MODELS_DIR / "yeojohnson_transformer.pkl"
)

# V√©rification visuelle rapide des variables transform√©es
df_study[["X1_trans", "X2_trans", "X3_trans"]].describe()





import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# ‚úÖ Liste des variables transform√©es
transformed_vars = ["X1_trans", "X2_trans", "X3_trans"]

# üìÅ Dossier de sauvegarde
output_dir = FIGURES_DIR / 'preprocessing'
output_dir.mkdir(parents=True, exist_ok=True)

# üîÅ G√©n√©ration et sauvegarde des figures
for col in transformed_vars:
    fig, ax = plt.subplots(1, 2, figsize=(6, 2))

    # Histogramme + KDE
    sns.histplot(df_study[col], bins=30, kde=True, ax=ax[0], color="mediumseagreen")
    ax[0].set_title(f"{col} - Histogramme")
    ax[0].set_xlabel(col)

    # Boxplot
    sns.boxplot(x=df_study[col], ax=ax[1], color="salmon")
    ax[1].set_title(f"{col} - Boxplot")

    plt.tight_layout()

    # üíæ Sauvegarde
    fig_path = output_dir / f"{col}_distribution_boxplot.png"
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"‚úÖ Figure sauvegard√©e : {fig_path}")









## 5.2 D√©tection et suppression des outliers <a id="detection-et-suppression-des-outliers"></a>

print("üîç D√©tection et suppression des outliers (m√©thode IQR)")
print("=" * 60)

from preprocessing.outliers import detect_and_remove_outliers

# ‚úÖ Variables √† traiter (transform√©es)
transformed_cols = ["X1_trans", "X2_trans", "X3_trans"]

# ‚úÖ Sauvegarde de la version avant suppression
df_with_outliers = df_study.copy()

# ‚úÖ Chemin de sauvegarde apr√®s nettoyage
output_path = OUTPUTS_DIR / "data" / "df_after_outliers.csv"

# ‚úÖ Suppression des outliers avec export CSV
df_study = detect_and_remove_outliers(
    df=df_study,
    columns=transformed_cols,
    method='iqr',
    iqr_multiplier=1.5,
    verbose=True,
    save_path=output_path
)

# ‚úÖ Aper√ßu statistique post-nettoyage
print("\nüìä Statistiques descriptives apr√®s suppression des outliers :")
display(df_study[transformed_cols].describe())







## üìä Visualisation comparative avant/apr√®s suppression des outliers

from exploration.visualization import plot_outlier_comparison

# ‚úÖ Comparaison visuelle avant / apr√®s (X1_trans, X2_trans, X3_trans)
plot_outlier_comparison(
    df_before=df_with_outliers,
    df_after=df_study,
    cols=transformed_cols,
    output_dir=FIGURES_DIR / "eda",
    show=True
)


















## 5.3.2 Pr√©paration pour l'imputation multivari√©e <a id="preparation-imputation-multivariee"></a>

print("üîß Pr√©paration √† l'imputation multiple (KNN / MICE)")
print("=" * 60)

from preprocessing.missing_values import analyze_missing_values

cols_to_check = ["X1_trans", "X2_trans", "X3_trans"]

# üìÅ Analyse sur les donn√©es AVEC outliers
print("\nüìä Analyse (donn√©es avec outliers)")
analyze_missing_values(df=df_with_outliers, columns=cols_to_check, plot=True)

# ‚úÖ Colonnes √† imputer (si moins de 30 % de valeurs manquantes)
cols_impute_with_outliers = [col for col in cols_to_check
                             if df_with_outliers[col].isna().mean() < 0.30]

print("\nüìå Colonnes retenues (avec outliers) :")
print(cols_impute_with_outliers)


# üìÅ Analyse sur les donn√©es SANS outliers
print("\nüìä Analyse (donn√©es sans outliers)")
analyze_missing_values(df=df_study, columns=cols_to_check, plot=True)

cols_impute_no_outliers = [col for col in cols_to_check
                           if df_study[col].isna().mean() < 0.30]

print("\nüìå Colonnes retenues (sans outliers) :")
print(cols_impute_no_outliers)





























