#modules/evaluation/avaluate_predictions


import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix

def evaluate_predictions(y_true, y_pred, label=""):
    """
    Affiche les m√©triques principales, le rapport de classification et une jolie matrice de confusion.
    
    Param√®tres :
    ------------
    y_true : array-like
        Vraies √©tiquettes.
    y_pred : array-like
        Pr√©dictions binaires.
    label : str
        Titre personnalis√© pour l'affichage (ex: "VAL KNN", "TEST MICE").
    """
    # Calcul des m√©triques
    f1 = f1_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)

    print(f"\n=== √âVALUATION : {label.upper()} ===")
    print(f"F1-score   : {f1:.4f}")
    print(f"Pr√©cision  : {precision:.4f}")
    print(f"Rappel     : {recall:.4f}")
    print("\n--- Rapport d√©taill√© ---")
    print(classification_report(y_true, y_pred, digits=4))

    # Matrice de confusion
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(3.5, 3))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=["Pr√©dit: No-AD", "Pr√©dit: AD"],
                yticklabels=["R√©el: No-AD", "R√©el: AD"])
    plt.title(f"Matrice de Confusion ‚Äì {label.upper()}", fontsize=11)
    plt.xlabel("Valeurs Pr√©dites")
    plt.ylabel("Valeurs R√©elles")
    plt.tight_layout()
    plt.grid(False)
    plt.show()


def evaluate_from_probabilities(y_true, y_proba, threshold=0.5, label=""):
    """
    √âvalue les performances √† partir des probabilit√©s pr√©dites avec un seuil donn√©.
    
    Param√®tres :
    ------------
    y_true : array-like
        Vraies √©tiquettes.
    y_proba : array-like
        Probabilit√©s pr√©dites pour la classe positive.
    threshold : float
        Seuil de d√©cision √† appliquer sur y_proba.
    label : str
        Titre de l‚Äô√©valuation (ex: "TEST KNN", "VALIDATION MICE").
    """
    y_pred = (np.array(y_proba) >= threshold).astype(int)

    # üìä M√©triques
    f1 = f1_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)

    print(f"\n=== √âVALUATION : {label.upper()} ===")
    print(f"Seuil appliqu√© : {threshold:.3f}")
    print(f"F1-score       : {f1:.4f}")
    print(f"Pr√©cision      : {precision:.4f}")
    print(f"Rappel         : {recall:.4f}")
    print("\n--- Rapport d√©taill√© ---")
    print(classification_report(y_true, y_pred, digits=4))

    # üîç Matrice de confusion
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(3.5, 3))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=["Pr√©dit: No-AD", "Pr√©dit: AD"],
                yticklabels=["R√©el: No-AD", "R√©el: AD"])
    plt.title(f"Matrice de Confusion ‚Äì {label.upper()}", fontsize=11)
    plt.xlabel("Valeurs Pr√©dites")
    plt.ylabel("Valeurs R√©elles")
    plt.tight_layout()
    plt.grid(False)
    plt.show()

    # Optionnel : retourner les scores pour logs
    return {
        "threshold": threshold,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }


def analyze_model_performance(result_dict, X_test, y_test, method_name):
    """
    Analyse les performances d'un mod√®le √† partir du dictionnaire de r√©sultats.
    """
    if result_dict is None:
        print(f"‚ùå Aucun r√©sultat disponible pour {method_name}.")
        return

    model = result_dict['model']
    threshold = result_dict['threshold']
    
    if model is None:
        print(f"‚ùå Aucun mod√®le disponible dans les r√©sultats pour {method_name}.")
        return

    # a. Pr√©dictions
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)

    # b. Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    print(f"\n--- Matrice de Confusion - {method_name} ---")
    print(cm)

    # c. Classification Report
    print(f"\n--- Rapport de Classification - {method_name} ---")
    print(classification_report(y_test, y_pred))

    # d. Visualisation de la matrice de confusion (optionnel)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                xticklabels=["Non-admis", "Admis"], 
                yticklabels=["Non-admis (R√©el)", "Admis (R√©el)"])
    plt.title(f'Matrice de Confusion - {method_name}\n(Seuil: {threshold:.3f}, F1-test: {result_dict["metrics"]["f1_score_test"]:.4f})')
    plt.xlabel('Pr√©dit')
    plt.ylabel('R√©el')
    plt.tight_layout()
    plt.show()

    # e. Afficher les m√©triques d√©j√† calcul√©es
    print(f"\n--- M√©triques Test (Seuil Optimal {threshold:.3f}) - {method_name} ---")
    metrics = result_dict['metrics']
    print(f"F1-score (test): {metrics['f1_score_test']:.4f}")
    print(f"Pr√©cision (test): {metrics['precision_test']:.4f}")
    print(f"Rappel (test): {metrics['recall_test']:.4f}")